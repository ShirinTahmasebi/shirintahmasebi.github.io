<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Attention Head Pruning | Shirin Tahmasebi </title> <meta name="author" content="Shirin Tahmasebi"> <meta name="description" content="Pruning the Attention Heads in Layer-wise Way to Make LLMs Smaller "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?ce48ee9bc248ee6b18f3aeefc03ed2f9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shirintahmasebi.github.io/compression/2021/attention_head_pruning/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Shirin Tahmasebi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">CV </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">Publications</a> <a class="dropdown-item " href="/background/">Background</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Notes </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/bias/">Bias</a> <a class="dropdown-item " href="/tokenization/">Tokenization</a> <a class="dropdown-item " href="/llm_judge/">LLM as Evaluator</a> <a class="dropdown-item " href="/prompt/">Prompt Engineering</a> <a class="dropdown-item " href="/compression/">LLM Compression</a> <a class="dropdown-item " href="/agents/">LLM Agents</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/recsys/">Recommendation Systems</a> <a class="dropdown-item " href="/other/">Others</a> </div> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Attention Head Pruning</h1> <p class="post-meta"> October 07, 2021 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2021   ·   <i class="fa-solid fa-hashtag fa-sm"></i> compression   <i class="fa-solid fa-hashtag fa-sm"></i> pruning   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#layer-wise-pruning-of-transformer-attention-heads-for-efficient-language-modeling">Layer-wise Pruning of Transformer Attention Heads for Efficient Language Modeling</a> <ul> <li class="toc-entry toc-h3"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h3"><a href="#research-gap">Research Gap</a></li> <li class="toc-entry toc-h3"> <a href="#methodology">Methodology</a> <ul> <li class="toc-entry toc-h4"><a href="#step-1-attach-a-trainable-switch-to-each-head">Step 1: Attach a Trainable “Switch” to Each Head</a></li> <li class="toc-entry toc-h4"><a href="#step-2-train-the-model-with-gating-mechanism">Step 2: Train the Model with Gating Mechanism</a></li> <li class="toc-entry toc-h4"><a href="#step-3-permanently-remove-weak-heads">Step 3: Permanently Remove Weak Heads</a></li> <li class="toc-entry toc-h4"><a href="#step-4-fine-tune-the-pruned-model">Step 4: Fine-Tune the Pruned Model</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#conclusion">Conclusion</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="layer-wise-pruning-of-transformer-attention-heads-for-efficient-language-modeling"><a href="https://arxiv.org/pdf/2110.03252" rel="external nofollow noopener" target="_blank">Layer-wise Pruning of Transformer Attention Heads for Efficient Language Modeling</a></h2> <h3 id="introduction">Introduction</h3> <p>Attention head pruning is an effective method for reducing the computational cost of Transformer models without significantly impacting their performance. This paper introduces a trainable, layer-wise attention head pruning approach, where the model learns which heads to remove during training using a gating mechanism and L₀ sparsity loss. By gradually eliminating less important heads, the method achieves efficient model compression while maintaining accuracy.</p> <p>This pruning method can be categorized based on:</p> <ol> <li> <strong>Granularity</strong>: The approach operates at the attention head level, making it a structured pruning technique that removes entire heads rather than individual weights or neurons. This ensures efficient execution on modern hardware.</li> <li> <strong>Pruning Criteria</strong>: The decision to prune an attention head is based on a trainable gating mechanism that evaluates its contribution to the model’s output. A special L₀ sparsity loss encourages the removal of heads that minimally impact performance, making the process data-driven rather than heuristic.</li> <li> <strong>Pruning Ratio</strong>: The pruning ratio is learned automatically during training rather than being manually predefined. The model determines the optimal number of heads to remove based on performance preservation and sparsity constraints, ensuring a balance between efficiency and accuracy.</li> <li> <strong>Need for Retraining</strong>: Unlike traditional pruning methods that require full retraining after pruning, this approach does not need retraining from scratch. Instead, it includes a short fine-tuning phase after pruning to allow the remaining heads to adapt, leading to a more stable and practical pruning framework.</li> </ol> <h3 id="research-gap">Research Gap</h3> <p>While attention head pruning has been explored in prior work, most existing methods focus on removing heads after pretraining using heuristic importance measures or sensitivity analysis. However, these approaches often require additional retraining steps and do not fully optimize the pruning process during model training. Furthermore, traditional attention head pruning does not proportionally reduce the feedforward module’s computational cost, limiting overall efficiency gains. This paper addresses these gaps by introducing layer-wise, trainable attention head pruning, where the model learns which heads to remove during training, guided by a trainable gating mechanism and L₀ sparsity loss.</p> <h3 id="methodology">Methodology</h3> <p>This paper proposes a method to <strong>prune unnecessary attention heads</strong> from Transformer models to <strong>reduce computation and model size</strong> while maintaining accuracy.</p> <p>So, let us see how it works step by step. Each attention head is assigned a <strong>trainable gating parameter</strong> that learns whether the head is important or can be removed.</p> <h4 id="step-1-attach-a-trainable-switch-to-each-head">Step 1: Attach a Trainable “Switch” to Each Head</h4> <ul> <li>Every attention head gets a <strong>gating parameter</strong> (a trainable value).</li> <li>This parameter <strong>decides whether a head stays active or gets pruned</strong>.</li> </ul> <h4 id="step-2-train-the-model-with-gating-mechanism">Step 2: Train the Model with Gating Mechanism</h4> <ul> <li>The model is trained <strong>as usual</strong>, but now it also <strong>learns which heads are useful</strong>.</li> <li>A special <strong>L₀ loss function</strong> helps decide which heads to remove.</li> <li>The loss is based on <strong>the difference between the layer’s output with and without a specific head</strong>.</li> <li>If removing a head <strong>doesn’t significantly change the output</strong>, its gating value decreases, making it more likely to be pruned.</li> </ul> <h4 id="step-3-permanently-remove-weak-heads">Step 3: Permanently Remove Weak Heads</h4> <ul> <li>Once training is done, heads with <strong>low gating parameters are fully deleted</strong> from the model.</li> <li>This reduces both <strong>model size</strong> and <strong>computational cost</strong>.</li> </ul> <h4 id="step-4-fine-tune-the-pruned-model">Step 4: Fine-Tune the Pruned Model</h4> <ul> <li>Since removing heads slightly changes the model structure, a short <strong>fine-tuning phase</strong> is needed.</li> <li>This helps the remaining heads <strong>adjust and recover any lost performance</strong>.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>This paper presents a layer-wise, trainable attention head pruning approach that enables Transformer models to become more efficient while maintaining strong performance. By introducing trainable gating parameters and an L₀ sparsity loss, the model learns which attention heads are unimportant and gradually removes them during training. Unlike traditional pruning techniques that rely on heuristic importance measures or require full retraining, this method ensures automatic, structured pruning with only a short fine-tuning phase. Additionally, by leveraging the All-Attention Transformer, the approach effectively reduces both computation and parameter count, making it more scalable for real-world deployment. Experimental results demonstrate that this pruning method outperforms conventional head pruning techniques in parameter efficiency and inference speed, highlighting its potential for optimizing Transformer-based language models.</p> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Shirin Tahmasebi. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>