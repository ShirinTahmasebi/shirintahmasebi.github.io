<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Gemini | Shirin Tahmasebi </title> <meta name="author" content="Shirin Tahmasebi"> <meta name="description" content="A Family of Highly Capable Multimodal Models "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?ce48ee9bc248ee6b18f3aeefc03ed2f9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shirintahmasebi.github.io/other/2023/gemini/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Shirin Tahmasebi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">CV </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">Publications</a> <a class="dropdown-item " href="/background/">Background</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Notes </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/bias/">Bias</a> <a class="dropdown-item " href="/tokenization/">Tokenization</a> <a class="dropdown-item " href="/llm_judge/">LLM as Evaluator</a> <a class="dropdown-item " href="/prompt/">Prompt Engineering</a> <a class="dropdown-item " href="/compression/">LLM Compression</a> <a class="dropdown-item " href="/agents/">LLM Agents</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/recsys/">Recommendation Systems</a> <a class="dropdown-item " href="/other/">Others</a> </div> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Gemini</h1> <p class="post-meta"> December 24, 2023 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2023   ·   <i class="fa-solid fa-hashtag fa-sm"></i> llm   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#gemini-a-family-of-highly-capable-multimodal-models">Gemini: A Family of Highly Capable Multimodal Models</a> <ul> <li class="toc-entry toc-h3"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h3"><a href="#core-architecture">Core Architecture</a></li> <li class="toc-entry toc-h3"><a href="#variants-of-gemini">Variants of Gemini</a></li> <li class="toc-entry toc-h3"> <a href="#techniques">Techniques</a> <ul> <li class="toc-entry toc-h4"><a href="#instruction-tuning-training-phase">Instruction Tuning (Training Phase)</a></li> <li class="toc-entry toc-h4"><a href="#multi-query-attention-architectural">Multi-query Attention (Architectural)</a></li> <li class="toc-entry toc-h4"><a href="#uncertainty-routed-chain-of-thought-cot-inference-phase">Uncertainty-routed Chain-of-Thought (CoT) (Inference Phase)</a></li> <li class="toc-entry toc-h4"><a href="#model-distillation">Model Distillation</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#experiments">Experiments</a></li> <li class="toc-entry toc-h3"><a href="#conclusion">Conclusion</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="gemini-a-family-of-highly-capable-multimodal-models"><a href="https://arxiv.org/pdf/2312.11805.pdf" rel="external nofollow noopener" target="_blank">Gemini: A Family of Highly Capable Multimodal Models</a></h2> <h3 id="introduction">Introduction</h3> <p>The Gemini project marks a significant advancement in artificial intelligence, developed by Google to seamlessly integrate and process information across textual, visual, audio, and video modalities. Presented in three distinct configurations—Ultra, Pro, and Nano—the Gemini models are designed to address a broad spectrum of computational demands and application scenarios.</p> <h3 id="core-architecture">Core Architecture</h3> <p>Gemini models are decoder-only transformer-based models, supporting the following features to enhance their performance across a wide range of tasks, including language, image, audio, and video understanding:</p> <ul> <li> <strong>32k Context Length:</strong> Gemini models are trained to support a 32,000 token context length. This is a significant increase over many existing models and allows Gemini to handle very long documents, conversations, or sequences of data without truncation.</li> <li> <strong>Efficient Attention Mechanisms:</strong> The core challenge in supporting a 32k context length stems from the computational cost associated with the attention mechanism in Transformer models. The standard self-attention mechanism has a quadratic computational complexity with respect to the sequence length, making it prohibitively expensive for very long sequences. The Gemini model addresses this scalability issue by employing an efficient attention mechanism known as multi-query attention, which is a key innovation allowing it to handle longer contexts more efficiently.</li> <li> <strong>Adaptations for Multimodality:</strong> To support the variety of modalities, Gemini employs specialized encoding strategies. For instance, video understanding is achieved by encoding video as a sequence of frames within the large context window, allowing video frames or images to be interleaved naturally with text or audio as part of the model input. This requires adjustments to how data is represented and processed within the model, which is what distinguishes it from purely text-based decoder models.</li> <li> <strong>Training Innovations:</strong> The training of Gemini models incorporates innovations, including custom training algorithms, custom datasets, and infrastructure adaptations that enable stable training at scale and optimized inference on Google’s TPUs.</li> <li> <strong>Application-Specific Model Variants:</strong> Gemini introduces three main sizes (Ultra, Pro, Nano) tailored for different computational limits and application requirements. This variety allows the deployment of state-of-the-art performance models across a range of environments, from cloud-based applications requiring the Ultra model’s capabilities to on-device applications suitable for the Nano models.</li> </ul> <h3 id="variants-of-gemini">Variants of Gemini</h3> <p>Gemini has three variants:</p> <ul> <li> <strong>Ultra:</strong> This variant is the most capable, designed for highly complex tasks. It likely has the highest number of decoder layers, the largest dimensionality, and the most attention heads. It’s optimized for performance but requires significant computational resources.</li> <li> <strong>Pro:</strong> The Pro variant reduces the number of layers, the dimensionality, and the number of attention heads compared to the Ultra variant. It’s designed to offer strong performance across a wide range of tasks with more manageable computational needs.</li> <li> <strong>Nano:</strong> Nano significantly scales down the number of decoder layers, dimensionality, and attention heads. These models focus on efficiency, with the architecture optimized for tasks that can be run on devices with lower processing capabilities. The process of scaling transformer-based models across these variants involves a trade-off between computational cost and the model’s capacity to handle complex tasks. Larger models (like Ultra) are better at dealing with more complex tasks but are also more expensive to train and run, requiring more memory and processing power. Smaller models (like Nano) are much more efficient but may not perform as well on tasks that require deep understanding or complex reasoning.</li> </ul> <h3 id="techniques">Techniques</h3> <h4 id="instruction-tuning-training-phase">Instruction Tuning (Training Phase)</h4> <p>Instruction tuning, involving supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), is employed to enhance the model’s performance on specific tasks. This technique allows Gemini models to improve their ability to follow instructions, generate more helpful and relevant responses, and reduce hallucinations or inaccuracies in outputs.</p> <h4 id="multi-query-attention-architectural">Multi-query Attention (Architectural)</h4> <p>Multi-query attention modifies the standard self-attention mechanism to reduce its computational complexity, especially for long sequences. Instead of calculating attention scores for each token independently, multi-query attention generates multiple query vectors for a group of tokens at once. This can significantly reduce the number of attention score calculations. By grouping tokens or processing multiple queries together, multi-query attention can lower the effective computation needed to calculate the attention scores. The specifics of how queries are grouped or processed can vary but the goal is to minimize redundancy in attention score calculation across the sequence.</p> <h4 id="uncertainty-routed-chain-of-thought-cot-inference-phase">Uncertainty-routed Chain-of-Thought (CoT) (Inference Phase)</h4> <p>The uncertainty-routed CoT technique introduced in this paper represents a new approach to enhancing the decision-making process of LLMs, particularly when handling complex reasoning tasks. This technique builds on the conventional CoT reasoning, which involves generating intermediate reasoning steps leading to a final answer, thereby making the model’s thought process more transparent and understandable.</p> <p>Here’s a breakdown of how the uncertainty-routed CoT works:</p> <ul> <li> <strong>Uncertainty Routing:</strong> The uncertainty-routed aspect of this technique involves generating multiple CoT samples (i.e., different sequences of reasoning steps) for a given problem and then assessing the model’s confidence across these samples. The model produces \(k\) CoT samples for each question.</li> <li> <strong>Majority Voting and Thresholding:</strong> After generating multiple CoT samples, the model evaluates the consensus among these samples. If a clear majority agreement is reached above a certain confidence threshold, the model selects the majority answer. This threshold is optimized based on performance on a validation set. The idea is that if the model consistently arrives at the same conclusion through different reasoning paths, that answer is likely more reliable.</li> <li> <strong>Greedy Sampling:</strong> In cases where no clear consensus is reached (i.e., the model’s confidence does not exceed the predetermined threshold), the model defaults to a greedy sampling method. This involves selecting the most likely answer based on the model’s initial, direct estimation without relying on the CoT reasoning process.</li> </ul> <p>This method aims to leverage the benefits of CoT reasoning while mitigating its drawbacks. By only relying on CoT when the model shows a high degree of internal consistency and consensus, it attempts to strike a balance between the enhanced reasoning capabilities provided by CoT and the need for reliable, confident answers. This approach allows the model to significantly improve its performance on challenging reasoning tasks, like the MMLU benchmark, by effectively managing uncertainty and leveraging the strengths of CoT reasoning.</p> <p>However, there are several potential drawbacks to the uncertainty-routed CoT, especially as it relates to the parameter \(k\), which represents the number of CoT samples generated for each question. Here are some of the key drawbacks and challenges associated with this approach:</p> <ul> <li> <p><strong>Time and Computational Resources:</strong> Generating multiple CoT samples (\(k\) samples) for each question significantly increases the computational load and processing time. This can lead to increased latency in obtaining an answer, making the method less practical for applications requiring real-time responses or when operating under tight computational constraints.</p> </li> <li> <p><strong>Optimization of Confidence Threshold:</strong> Finding the optimal confidence threshold that determines when to rely on the majority vote and when to default to greedy sampling can be challenging. This threshold is crucial for balancing the benefits of CoT reasoning with the need for reliability and confidence in the answers. However, it might vary significantly across different tasks, domains, or even specific types of questions, requiring extensive validation and potentially limiting the technique’s generalizability.</p> </li> </ul> <h4 id="model-distillation">Model Distillation</h4> <p>Model distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, more efficient model (student) without significantly compromising performance. This process is particularly important for deploying sophisticated AI capabilities on devices with limited computational resources, such as mobile phones or embedded systems. In the Gemini family, the Nano variants of the Gemini models use advanced distillation techniques to achieve this goal. Here’s an elaboration on how the distillation approach might work for these Nano models:</p> <ul> <li> <strong>Training the Teacher Model:</strong> Initially, a large and highly capable model (such as Gemini Ultra) is trained on a comprehensive dataset covering the target tasks. This model serves as the teacher.</li> <li> <strong>Generating Soft Labels:</strong> The teacher model processes the training data and generates outputs (soft labels) that capture its predictions and the soundness of its decision-making process. Unlike hard labels (the actual, discrete labels in the dataset), soft labels can provide more information about the probability distribution of the outputs, revealing the teacher model’s confidence across different possible answers.</li> <li> <strong>Training the Student Model:</strong> The smaller, Nano model is then trained not just on the original training data, but also to mimic the soft labels produced by the teacher model. This process helps the student model to learn both the correct answers and the teacher model’s reasoning patterns and confidence levels.</li> </ul> <p>For using the model distillation technique, several critical tricks are leveraged:</p> <ul> <li> <strong>Custom Data Generation:</strong> For Gemini Nano models, custom data generation might be used to create more effective training examples that highlight the teacher model’s knowledge and reasoning capabilities, focusing on areas where distillation can most improve the student model’s performance.</li> <li> <strong>4-bit Quantization:</strong> Distillation for the Nano models includes quantization steps, reducing the precision of the model’s parameters to 4 bits. This drastic reduction in parameter size decreases the computational load and memory requirements, making the model suitable for on-device applications. Despite the reduced precision, the knowledge distilled from the teacher model helps maintain performance.</li> </ul> <h3 id="experiments">Experiments</h3> <p>The Gemini models were evaluated across a diverse set of benchmarks and datasets to assess their capabilities in language understanding, generation, reasoning, and multimodal tasks. There are more than 50 benchmarks to evaluate the Gemini models’ capabilities in different areas, including text understanding and generation, factuality, long context, math/science, reasoning, summarization, and multilinguality.</p> <h3 id="conclusion">Conclusion</h3> <p>As we have explored throughout this blog post, the Gemini models mark a pivotal advancement in the field of artificial intelligence, particularly in the realm of multimodal interaction and understanding. By addressing the challenges of processing extensive sequences and integrating diverse forms of data, Gemini not only achieves state-of-the-art performance across a wide range of benchmarks but also opens the door to a lot of of practical applications that were previously unattainable.</p> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Shirin Tahmasebi. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>