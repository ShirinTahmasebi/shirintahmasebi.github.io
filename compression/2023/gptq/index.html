<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> GPTQ | Shirin Tahmasebi </title> <meta name="author" content="Shirin Tahmasebi"> <meta name="description" content="Accurate Post-Training Quantization for Generative Pre-Trained Transformers "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?ce48ee9bc248ee6b18f3aeefc03ed2f9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shirintahmasebi.github.io/compression/2023/gptq/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Shirin Tahmasebi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">CV </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">Publications</a> <a class="dropdown-item " href="/background/">Background</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Notes </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/bias/">Bias</a> <a class="dropdown-item " href="/tokenization/">Tokenization</a> <a class="dropdown-item " href="/llm_judge/">LLM as Evaluator</a> <a class="dropdown-item " href="/prompt/">Prompt Engineering</a> <a class="dropdown-item " href="/compression/">LLM Compression</a> <a class="dropdown-item " href="/agents/">LLM Agents</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/recsys/">Recommendation Systems</a> <a class="dropdown-item " href="/other/">Others</a> </div> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">GPTQ</h1> <p class="post-meta"> March 22, 2023 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2023   ·   <i class="fa-solid fa-hashtag fa-sm"></i> compression   <i class="fa-solid fa-hashtag fa-sm"></i> quantization   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#gptq-accurate-post-training-quantization-for-generative-pre-trained-transformers">GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers</a> <ul> <li class="toc-entry toc-h3"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h3"><a href="#research-gap">Research Gap</a></li> <li class="toc-entry toc-h3"> <a href="#solution-gptq">Solution: GPTQ</a> <ul> <li class="toc-entry toc-h4"><a href="#how-to-approximate-hessian-matrix">How to Approximate Hessian Matrix?</a></li> <li class="toc-entry toc-h4"><a href="#how-to-use-the-hessian-in-gptq">How to Use the Hessian in GPTQ</a></li> <li class="toc-entry toc-h4"><a href="#final-takeawayhow-the-hessian-helps">Final Takeaway:How the Hessian Helps</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#conclusion">Conclusion</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="gptq-accurate-post-training-quantization-for-generative-pre-trained-transformers"><a href="https://arxiv.org/pdf/2210.17323" rel="external nofollow noopener" target="_blank">GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers</a></h2> <h3 id="introduction">Introduction</h3> <p>Large-scale Transformer models, such as GPT-3 and BLOOM, have demonstrated remarkable performance in various NLP tasks. However, their enormous size leads to high computational and storage costs, making efficient deployment a significant challenge. Model quantization, which reduces the bitwidth of weights, is a common technique to alleviate these costs, but traditional Post-Training Quantization (PTQ) methods struggle to maintain accuracy when reducing weights below 8-bit precision. To address this, GPTQ introduces a highly efficient one-shot quantization method that leverages second-order information to compress models to 3–4 bits while preserving near-original accuracy.</p> <h3 id="research-gap">Research Gap</h3> <p>While prior quantization techniques exist, they suffer from major limitations when applied to extremely large language models. Most previous PTQ methods, such as Round-To-Nearest (RTN) and basic adaptive quantization, either fail to maintain accuracy at very low bitwidths or require extensive retraining, which is computationally infeasible for billion-parameter models. Existing approaches also do not effectively compensate for quantization errors at scale, as they lack mechanisms to exploit the structure of weight correlations. GPTQ fills this gap by introducing a Hessian-based weight adjustment mechanism, allowing for accurate quantization of models as large as 175 billion parameters in just a few GPU hours, significantly outperforming prior methods in efficiency and accuracy.</p> <h3 id="solution-gptq">Solution: GPTQ</h3> <p>At a high level, the Hessian matrix (or an approximation of it) tells us how sensitive the model’s outputs are to changes in its weights. This matrix plays a key role in guiding error compensation during weight quantization in GPTQ. The <strong>true Hessian matrix</strong> in optimization is typically defined as:</p> \[H = \nabla^2 L(W)\] <p>where \(H\) is the matrix of second-order partial derivatives of the <strong>loss function</strong> \(L(W)\) with respect to the model’s weights \(W\). In other words, it tells us how the loss <strong>curves</strong> in different directions, which is useful for optimization. But, computing the true Hessian for deep networks is extremely expensive because:</p> <ul> <li>It requires taking second derivatives for <strong>all</strong> model weights.</li> <li>It results in a <strong>massive</strong> matrix (billions of parameters squared).</li> <li>Inverting it would be infeasible for large models.</li> </ul> <h4 id="how-to-approximate-hessian-matrix">How to Approximate Hessian Matrix?</h4> <p>So, GPTQ does <strong>not</strong> compute the true second-order Hessian (which would be expensive). Instead, it <strong>approximates</strong> it as:</p> \[H \approx 2 X X^\top + \lambda I\] <p>Where:</p> <ul> <li>\(X\) is the <strong>layer input matrix</strong> of shape \(\text{[input_dim]} \times m\), where: <ul> <li>Each <strong>column</strong> represents the input activation for one calibration sample.</li> <li>Each <strong>row</strong> represents how a specific input dimension varies across calibration samples.</li> </ul> </li> <li>\(X X^\top\) is a \(\text{[input_dim]} \times \text{[input_dim]}\) <strong>correlation-like matrix</strong>, measuring how different input dimensions interact.</li> <li>\(\lambda I\) is a <strong>small damping term</strong> (stabilization), ensuring numerical stability when inverting the matrix.</li> </ul> <p>Each entry \(H_{i,j}\) of the Hessian approximation \(H\) tells us:</p> <ol> <li> <strong>Diagonal entries (\(H_{i,i}\)):</strong> Sensitivity of individual input dimensions <ul> <li>Measures how much input dimension \(i\) affects the output.</li> <li>Large diagonal values mean that small changes in weight \(W_i\) could cause large output changes, so we should quantize more carefully.</li> <li>Small diagonal values mean that quantization error for weight \(W_i\) has less impact, so we can round it more aggressively.</li> </ul> </li> <li> <strong>Off-diagonal entries (\(H_{i,j}\), \(i \neq j\)):</strong> Correlation between input dimensions <ul> <li>Measures how much dimension \(i\) and dimension \(j\) co-activate across the calibration data.</li> <li> <strong>Large \(H_{i,j}\)</strong> (strong correlation): <ul> <li>If you introduce an error by rounding weight \(W_i\), you might be able to adjust weight \(W_j\) to compensate.</li> <li>This means one weight can be quantized aggressively, while the other should be adjusted more carefully to balance the error.</li> </ul> </li> <li> <strong>Small or zero \(H_{i,j}\)</strong> (weak or no correlation): <ul> <li>Quantization errors in \(W_i\) cannot be corrected by adjusting \(W_j\), so each should be handled independently.</li> </ul> </li> </ul> </li> </ol> <h4 id="how-to-use-the-hessian-in-gptq">How to Use the Hessian in GPTQ</h4> <ol> <li>Prioritizing Which Weights to Quantize First <ul> <li>Weights corresponding to <strong>small diagonal values</strong> (\(H_{i,i}\)) can be quantized more aggressively.</li> <li>Weights with <strong>large diagonal values</strong> should be quantized more carefully since they have a higher impact on outputs.</li> </ul> </li> <li>Compensating Quantization Errors Using Correlation (Off-Diagonal Entries) <ul> <li>When a weight is quantized, its rounding introduces an error.</li> <li>The Hessian’s off-diagonal terms tell us which remaining (unquantized) weights can be adjusted to reduce the total error.</li> <li>If \(H_{i,j}\) is large, adjusting \(W_j\) can help offset the error introduced when quantizing \(W_i\).</li> </ul> </li> <li>Batch Quantization for Efficiency <ul> <li>Instead of adjusting weights one at a time, GPTQ quantizes in blocks of columns (e.g., 128 at a time).</li> <li>Uses Hessian-based compensation across the block before moving to the next batch.</li> </ul> </li> </ol> <h4 id="final-takeawayhow-the-hessian-helps">Final Takeaway:How the Hessian Helps</h4> <ul> <li> <strong>Diagonal elements</strong> tell us which individual dimensions need careful quantization vs. which ones can be rounded more freely.</li> <li> <strong>Off-diagonal elements</strong> reveal which weights are “linked,” allowing error correction when one is rounded.</li> <li>This <strong>trade-off between aggressive rounding and compensation</strong> enables GPTQ to quantize large models to 3-bit or 4-bit precision while preserving accuracy.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>GPTQ presents a breakthrough in post-training quantization by making it possible to efficiently compress massive Transformer models while maintaining their performance. By leveraging an approximate second-order Hessian matrix, it quantizes weights while minimizing accuracy loss through targeted error compensation. This allows for single-GPU execution of models previously requiring multi-GPU setups and enables significant inference speedups. The study demonstrates that ultra-low-bitwidth quantization (as low as 3 bits) is feasible for state-of-the-art language models, paving the way for more accessible and cost-effective deployment of large-scale systems.</p> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Shirin Tahmasebi. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>