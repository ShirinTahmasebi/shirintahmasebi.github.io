---
layout: post
title: AWQ
thumbnail: /assets/img/cover/awq.jpg
categories: []
tags:
  - compression
  - quantization
description: >
    Activation-aware Weight Quantization
toc:
  beginning: true
---

## [AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration][awqPaper]


### Introduction

Large Language Models (LLMs) have revolutionized natural language processing by enabling near-human performance in tasks like text generation, code completion, and conversational AI. However, the hardware and memory overhead of running these massive models on small devices (e.g., mobile phones, edge servers) remain prohibitively high. To address this challenge, recent efforts have turned to quantization to achieve efficient on-device inference. Among these approaches, weight-only quantization strikes a strong balance between feasibility and performance, lowering overall storage without adding much computational overhead. This paper introduces Activation-aware Qunatization (AWQ), which is a weight-only Post-Training Quantization (PTQ) method that converts full-precision LLM weights to 4-bit integers without requiring any fine-tuning or backpropagation.


### Research Gap

Despite the advantages of weight-only quantization, straightforward “round-to-nearest” methods can cause significant drops in accuracy, especially under low-bit settings such as 4-bit integer precision. Additionally, many advanced techniques rely on either partial retraining (which can be expensive) or heavy data requirements for calibrating weights, both of which can diminish the appeal of a truly lightweight, post-training solution. The paper addresses this gap by introducing AWQ, which specifically protects the small fraction of “salient” input neurons that have a disproportionately large effect on the model's output—thereby greatly reducing quantization error without requiring any fine-tuning or large-scale data.


### Solution: AWQ

 AWQ is a weight-only Post-Training Quantization (PTQ) method that converts full-precision LLM weights to 4-bit integers without requiring any fine-tuning or backpropagation. It simply measures which input neurons (dimensions) get the largest activations and pre-scales the corresponding weights to mitigate rounding errors. Everything is done after the model is trained, using only a small calibration set to gather activation statistics.

The core insight is that not all weights matter equally—specifically, the weights connected to input neurons with large activations have a bigger impact on the model's output. AWQ identifies those “salient” neurons based on their average activation magnitude (collected from a small calibration set) and scales up their weights before rounding them to 4 bits. This scaling step reduces the relative rounding error for the crucial (high-activation) neurons, preserving model quality without needing any full-blown re-training. The end result is a hardware-friendly approach that achieves significant memory reduction and speedups on GPUs and other edge devices, all while maintaining strong performance across language, coding, and even multi-modal tasks.

Here's a concise summary of how AWQ works:

1. **Collect Activation Statistics**  
   - For each linear layer in an LLM, gather the **average activation** seen by each *input neuron* on a small calibration dataset.  
   - An *input neuron* here means a single dimension of the layer's input vector.

2. **Identify Salient Input Neurons**  
   - Based on these average activation values, a small subset (e.g., ~0.1–1%) of *input neurons* will stand out because their average activations are significantly higher. These are termed “salient,” since a small quantization error there can cause a large error in the layer's output.

3. **Apply Input-Neuron-Specific Scaling**  
   - Before quantizing the layer's weights to 4 bits, multiply the weights corresponding to each *input neuron* by a scaling factor. 
   - Crucially, the factor depends on the neuron's average activation value: 

     $$
       \text{Scaling Factor}[i] \;=\; \bigl(\text{avg_act}[i]\bigr)^{\alpha},
     $$

     where $$\alpha$$ is chosen via a quick grid search between [0,1]. This ensures that *salient* neurons (the ones with larger activations) get a bigger boost.

4. **Quantize All Weights**  
   - After scaling is applied to each neuron's weights, the entire layer is quantized to a unified 4-bit format. 
   - Because the weights for *salient* neurons were scaled up in advance, their *relative* rounding error is smaller, improving overall accuracy.

5. **Preserve the Model's Computation**  
   - To keep the final numerical outputs consistent, the input activations can be proportionally adjusted (scaled down) during inference. This is effectively the same operation as the original layer multiplication, but now it has better resilience to 4-bit rounding errors where it matters most (the salient neurons).

In short, AWQ detects which *input neurons* handle large activation values, scales up their associated weights before 4-bit rounding, and thereby protects the most important pathways in the network without resorting to slow or memory-heavy mixed-precision.


### Conclusion

In essence, AWQ bridges a critical need in practical LLM deployment by facilitating efficient model compression while preserving strong performance. By identifying and scaling the most impactful weight columns based on activation statistics, it cuts down memory and latency costs without compromising the robustness or accuracy of the model. This allows large-scale language models to be more readily deployed on resource-constrained devices, democratizing access to cutting-edge NLP capabilities in real-world settings—from mobile assistants to embedded IoT applications.


[awqPaper]: https://arxiv.org/pdf/2306.00978