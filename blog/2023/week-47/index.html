<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Paper Review - Week 47 | Shirin Tahmasebi </title> <meta name="author" content="Shirin Tahmasebi"> <meta name="description" content="Top NLP Papers Published from November 20 to November 26 "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?ce48ee9bc248ee6b18f3aeefc03ed2f9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shirintahmasebi.github.io/blog/2023/week-47/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Shirin Tahmasebi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/background/">Background </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Paper Review - Week 47</h1> <p class="post-meta"> November 26, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/papers"> <i class="fa-solid fa-tag fa-sm"></i> papers</a>   <a href="/blog/category/weekly-review"> <i class="fa-solid fa-tag fa-sm"></i> weekly-review</a>   <a href="/blog/category/nlp"> <i class="fa-solid fa-tag fa-sm"></i> nlp</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#freshllms-refreshing-large-language-models-with-search-engine-augmentation">FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation</a> <ul> <li class="toc-entry toc-h3"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h3"><a href="#research-gap">Research Gap</a></li> <li class="toc-entry toc-h3"> <a href="#solution-freshllm">Solution: FreshLLM</a> <ul> <li class="toc-entry toc-h4"><a href="#freshqa">FRESHQA</a></li> <li class="toc-entry toc-h4"><a href="#freshprompt">FRESHPROMPT</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#experiments">Experiments</a></li> <li class="toc-entry toc-h3"><a href="#conclusion">Conclusion</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <p>Here is the list of the most interesting papers published in this week:</p> <ul> <li><a href="/blog/2023/week-47/#freshllms-refreshing-large-language-models-with-search-engine-augmentation">FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation</a></li> </ul> <hr> <h2 id="freshllms-refreshing-large-language-models-with-search-engine-augmentation"><a href="https://arxiv.org/pdf/2305.14283.pdf" rel="external nofollow noopener" target="_blank">FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation</a></h2> <h3 id="introduction">Introduction</h3> <p>This paper introduces FRESHQA, a dynamic question-answering benchmark designed to evaluate the factuality of large language models (LLMs) in the context of changing world knowledge. The paper explores the limitations of existing LLMs, including their struggle with questions involving fast-changing knowledge and false premises. To address these challenges, the authors introduce FRESHPROMPT, a few-shot prompting method that incorporates relevant and up-to-date information from a search engine into the LLM’s prompt.</p> <h3 id="research-gap">Research Gap</h3> <p>The FreshLLMs paper addresses several research gaps in LLMs and their ability to handle dynamically changing information. Here are some key research gaps that the paper aims to fill:</p> <ul> <li> <strong>Static Nature of LLMs:</strong> Many existing LLMs are trained once and lack mechanisms for dynamic updates. This results in a static representation of knowledge, which can become outdated in a rapidly changing world.</li> <li> <strong>Factuality and Trustworthiness:</strong> LLMs, despite their impressive capabilities, are known to generate responses that may be factually incorrect or outdated. This affects the trustworthiness of their answers, especially in real-world scenarios where accurate and up-to-date information is crucial.</li> <li> <strong>Adaptation to Fast-Changing Knowledge:</strong> The paper identifies a gap in the ability of LLMs to adapt to fast-changing knowledge, such as current events or recent developments. Existing models may struggle to provide accurate answers to questions that require knowledge updates beyond their training data.</li> <li> <strong>Handling False Premises:</strong> LLMs often face challenges in answering questions with false premises. The paper highlights the need to assess how well LLMs can identify and correct false information in the questions they are presented with.</li> </ul> <h3 id="solution-freshllm">Solution: FreshLLM</h3> <p>This paper introduces a novel dataset called FRESHQA along with a prompting approach named FRESHPROMPT.</p> <h4 id="freshqa">FRESHQA</h4> <p>The authors addresses a crucial gap in the field by introducing FRESHQA, a dynamic question-answering dataset designed to evaluate language models’ performance on real-time information tasks. FRESHQA encompasses questions of varying difficulty levels, from <em>one-hop</em> to <em>multi-hop</em>, and considers the dynamicity of information, distinguishing between <em>never-changing</em>, <em>slow-changing</em>, <em>fast-changing</em>, and <em>false-premise</em> scenarios. The dataset is meticulously curated, involving NLP researchers and freelancers to create questions covering diverse topics. Quality control measures, including manual review and removal of duplicates, ensure the dataset’s integrity.</p> <p>The following image represents several example questions in the dataset:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/freshllm_sample_questions.png" alt="The Architecture" width="700" height="500"></p> <h4 id="freshprompt">FRESHPROMPT</h4> <p>In parallel, they propose FRESHPROMPT, a novel prompting approach tailored to facilitate the answering process for large language models. FRESHPROMPT leverages information retrieval from search engines, particularly Google Search, to gather relevant data. Given a question, the system queries the search engine, retrieves various result types, and extracts relevant information, such as text snippets, source, date, title, and highlighted words. The extracted evidence is then incorporated into the model’s prompt, enabling in-context learning. Demonstrations and the ordered list of retrieved evidences structure the prompt, guiding the model in understanding the task and producing accurate answers. The ultimate objective is to establish a benchmark for assessing language models in dynamic QA scenarios and to enhance their adaptability to real-time information by leveraging search engine data.</p> <h3 id="experiments">Experiments</h3> <p>The experiments conducted in the study involve benchmarking various language models on the FRESHQA dataset to assess their performance in dynamic question-answering scenarios. The models under evaluation span a range of sizes, from 770M to 540B parameters, and include well-known pre-trained models like T5, PALM, FLAN-T5, GPT-3.5, GPT-4, CODEX, and CHATGPT, among others. Two distinct evaluation modes, RELAXED and STRICT, are employed to measure correctness and hallucination in the models’ responses. The RELAXED evaluation focused on measuring the correctness of the primary answer, allowing for some flexibility in accepting ill-formed responses. In contrast, the STRICT evaluation scrutinized correctness but also whether any hallucination was present in the response.</p> <p>The following image shows several evaluations:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/freshllm_sample_evaluations.png" alt="The Architecture" width="700" height="250"></p> <p>The main findings highlight the challenges that existing large language models face in handling questions involving fast-changing knowledge and false premises. Regardless of model size, all models exhibit limitations in these scenarios, emphasizing the need for improvement. The study also reveals that increasing model size does not consistently lead to improved performance on questions with rapidly changing information.</p> <p>Motivated by these challenges, the researchers introduce FRESHPROMPT, an in-context learning method designed to enhance language models’ factuality. FRESHPROMPT leverages search engine outputs to provide up-to-date information, resulting in substantial accuracy improvements. The experiments demonstrate that FRESHPROMPT outperforms other search engine-augmented prompting methods and commercial systems.</p> <h3 id="conclusion">Conclusion</h3> <p>In conclusion, the study delves into the challenges faced by LLMs in handling dynamically changing information and questions with false premises. The introduced FRESHQA dataset serves as a valuable benchmark to assess the factuality of language models, revealing their limitations and the pressing need for improvement. The proposed FRESHPROMPT method showcases promising results, significantly boosting model performance by incorporating real-time information from search engines. This underscores the potential of in-context learning approaches to enhance language models’ adaptability to evolving knowledge.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <p class="mb-2">Here are some more articles relevant to this one:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-46/">Paper Review - Week 46</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/week-8/">Paper Review - Week 8</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-25/">Paper Review - Week 25</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/week-12/">Paper Review - Week 12</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-9/">Paper Review - Week 9</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Shirin Tahmasebi. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>