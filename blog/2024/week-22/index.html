<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Paper Review - Week 22 | Shirin Tahmasebi </title> <meta name="author" content="Shirin Tahmasebi"> <meta name="description" content="Top NLP Papers Published from May 27 to June 02 "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?ce48ee9bc248ee6b18f3aeefc03ed2f9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shirintahmasebi.github.io/blog/2024/week-22/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Shirin Tahmasebi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/background/">Background </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Paper Review - Week 22</h1> <p class="post-meta"> June 02, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/category/papers"> <i class="fa-solid fa-tag fa-sm"></i> papers</a>   <a href="/blog/category/weekly-review"> <i class="fa-solid fa-tag fa-sm"></i> weekly-review</a>   <a href="/blog/category/nlp"> <i class="fa-solid fa-tag fa-sm"></i> nlp</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#reflexion-language-agents-with-verbal-reinforcement-learning">Reflexion: Language Agents with Verbal Reinforcement Learning</a> <ul> <li class="toc-entry toc-h3"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h3"><a href="#research-gap">Research Gap</a></li> <li class="toc-entry toc-h3"><a href="#solution-reflexion">Solution: Reflexion</a></li> <li class="toc-entry toc-h3"><a href="#reflexion-process">Reflexion Process:</a></li> <li class="toc-entry toc-h3"><a href="#advantages">Advantages:</a></li> <li class="toc-entry toc-h3"> <a href="#experiments">Experiments</a> <ul> <li class="toc-entry toc-h4"><a href="#sequential-decision-making-alfworld">Sequential Decision-Making (ALFWorld)</a></li> <li class="toc-entry toc-h4"><a href="#reasoning-hotpotqa">Reasoning (HotpotQA)</a></li> <li class="toc-entry toc-h4"><a href="#programming">Programming</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#conclusion">Conclusion</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <p>Here is the list of the most interesting papers published in this week:</p> <ul> <li><a href="/blog/2024/week-22/#reflexion-language-agents-with-verbal-reinforcement-learning">Reflexion: Language Agents with Verbal Reinforcement Learning</a></li> </ul> <hr> <h2 id="reflexion-language-agents-with-verbal-reinforcement-learning"><a href="https://dl.acm.org/doi/10.5555/3666122.3666499" rel="external nofollow noopener" target="_blank">Reflexion: Language Agents with Verbal Reinforcement Learning</a></h2> <h3 id="introduction">Introduction</h3> <p>The Reflexion framework presents a novel approach to enhancing the performance of Large Language Models (LLMs) by addressing the limitations of traditional Reinforcement Learning (RL) methods. Traditional RL typically involves computationally intensive weight updates and relies on scalar rewards that often lack the nuance needed for effective learning. Reflexion diverges from this paradigm by reinforcing LLMs through verbal feedback instead of weight updates. This feedback, stored in <strong>an episodic memory buffer</strong>, consists of reflective summaries that provide detailed insights and suggestions for improvement. By leveraging the LLM’s natural language understanding capabilities, Reflexion enables the model to learn iteratively from past experiences, thus enhancing its decision-making, reasoning, and coding capabilities without the need for extensive computational resources.</p> <h3 id="research-gap">Research Gap</h3> <p>The Reflexion framework addresses the limitations of traditional RL methods, which involve computationally intensive weight updates and often lack nuanced feedback. Traditional RL typically relies on scalar rewards and extensive model fine-tuning, which can be inefficient and time-consuming. Reflexion introduces a novel approach by reinforcing LLMs through verbal feedback rather than weight updates. This feedback, stored in an episodic memory buffer, consists of reflective summaries that provide detailed insights and suggestions for improvement. This method leverages the LLM’s natural language understanding capabilities, enabling it to learn from past experiences without the need for expensive computational resources.</p> <p>Compared to existing methods, Reflexion stands out by incorporating memory-based learning and flexible feedback mechanisms, allowing it to be applied across diverse tasks such as sequential decision-making, coding, and language reasoning. Reflexion achieves state-of-the-art performance on several benchmarks, demonstrating significant improvements over baseline agents. By mimicking human reflective learning processes and leveraging the LLM’s capabilities, Reflexion represents a significant advancement in LLM training and reinforcement learning.</p> <h3 id="solution-reflexion">Solution: Reflexion</h3> <p>While Reflexion diverges from traditional RL in how it updates the agent (LLM), it still employs RL concepts but in a unique way. Here’s how RL concepts map onto the Reflexion framework:</p> <ol> <li> <strong>Agent</strong>: In traditional RL, the agent is typically a neural network that learns a policy by adjusting its weights based on rewards. In Reflexion, the agent is a LLM that generates actions (text) based on a fixed policy.</li> <li> <strong>Environment</strong>: In traditional RL, the environment provides states, receives actions from the agent, and returns rewards and new states. In Reflexion, the environment interacts with the LLM, providing feedback (rewards) based on the LLM’s actions.</li> <li> <strong>Policy</strong>: In traditional RL, the policy determines the agent’s actions and is updated based on rewards to improve performance. In Reflexion, the policy is implicitly defined by the LLM’s capabilities and its use of contextual information from memory.</li> <li> <strong>Reward</strong>: In traditional RL, rewards are used to update the agent’s policy through mechanisms like gradient descent. In Reflexion, rewards are used to generate verbal feedback, which is stored in memory and used to guide future actions.</li> <li> <strong>Learning Process</strong>: In traditional RL, learning involves updating the weights of the agent’s neural network. In Reflexion, learning involves updating the agent’s memory with reflective summaries, which influences future decisions.</li> <li> <strong>Exploration vs. Exploitation</strong>: In traditional RL, agents explore the environment to find better policies and exploit known good actions. In Reflexion, the LLM explores different actions based on feedback stored in memory and exploits successful patterns discovered through reflection.</li> </ol> <p>In Reflexion, the LLM (Actor) generates text or actions based on the current state and context from memory, aligning with the policy in RL. The Evaluator \(M_e\) evaluates the output of the Actor and provides a reward score, corresponding to the reward function in RL. The Self-Reflection model (\(M_{sr}\)) generates reflective feedback based on the reward and trajectory, which is stored in memory. This acts as an indirect way of updating the policy by changing the context the LLM uses to make decisions.</p> <h3 id="reflexion-process">Reflexion Process:</h3> <ol> <li> <strong>Generation of Initial Trajectory</strong>: The LLM (Actor) interacts with the environment to generate an initial trajectory \(\tau\).</li> <li> <strong>Evaluation</strong>: The Evaluator assesses the trajectory and produces a reward \(r\).</li> <li> <strong>Self-Reflection</strong>: The Self-Reflection model uses the trajectory \(\tau\) and reward \(r\) to generate a verbal summary \(sr\). This summary captures the agent’s reflections on its performance, highlighting what went wrong and suggesting improvements.</li> <li> <strong>Updating Memory</strong>: The reflective summary \(sr\) is stored in the agent’s memory. In subsequent trials, this memory provides context and guidance to the LLM, helping it to avoid past mistakes and make better decisions.</li> <li> <strong>Iterative Improvement</strong>: The process of generating trajectories, evaluating them, creating self-reflections, and updating memory continues iteratively. The agent leverages the accumulated memory to improve its performance over time.</li> </ol> <p>The whole process along with the psudo-code of the alogrithm is as follows:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/reflexion_solution.png" alt="Solution" width="800" height="400"></p> <h3 id="advantages">Advantages:</h3> <ul> <li> <strong>Lightweight</strong>: The approach doesn’t require computationally expensive weight updates and fine-tuning.</li> <li> <strong>Nuanced Feedback</strong>: Verbal feedback can be more detailed and specific compared to scalar rewards.</li> <li> <strong>Explicit Memory</strong>: The agent has an interpretable memory of past experiences that can guide future actions.</li> <li> <strong>Scalability</strong>: As LLM capabilities improve, this method can become even more effective.</li> </ul> <p>In a programming task, the LLM might generate a piece of code. The Evaluator checks the code against test cases and provides a reward based on correctness. Instead of updating the LLM’s weights, the Self-Reflection model generates feedback like, “The function failed because it didn’t handle edge cases correctly. Consider adding checks for empty input.” This feedback is stored in memory and used in the next coding attempt.</p> <p>The Reflexion framework cleverly leverages the existing capabilities of LLMs and augments them with a memory system driven by verbal feedback, enabling continuous improvement without the need for traditional weight updates in RL.</p> <h3 id="experiments">Experiments</h3> <p>The primary aspects of the experimental setups are as follows:</p> <h4 id="sequential-decision-making-alfworld">Sequential Decision-Making (ALFWorld)</h4> <p><strong>Task</strong>: The task involves solving multi-step problems in text-based environments, such as finding and manipulating objects.</p> <p><strong>Dataset</strong>: ALFWorld, a suite of text-based environments derived from TextWorld, encompassing 134 different tasks related to finding, moving, and manipulating objects.</p> <p><strong>Baselines</strong>:</p> <ul> <li> <strong>ReAct</strong>: An action generation model that uses intermediate thoughts to solve tasks.</li> </ul> <p><strong>Evaluation Metrics</strong>:</p> <ul> <li> <strong>Success Rate</strong>: The proportion of tasks completed successfully.</li> <li> <strong>Error Classification</strong>: Identifying types of errors such as hallucinations or inefficient planning.</li> </ul> <p><strong>Results</strong>:</p> <ul> <li> <strong>Performance</strong>: Reflexion agents significantly outperformed the ReAct baseline, completing 130 out of 134 tasks.</li> <li> <strong>Error Reduction</strong>: The use of reflective feedback reduced error rates due to hallucinations and inefficient planning.</li> </ul> <h4 id="reasoning-hotpotqa">Reasoning (HotpotQA)</h4> <p><strong>Task</strong>: The task tests the model’s ability to answer complex questions by reasoning over multiple supporting documents.</p> <p><strong>Dataset</strong>: HotpotQA, a large dataset with 113k question-answer pairs requiring reasoning over multiple Wikipedia articles.</p> <p><strong>Baselines</strong>:</p> <ul> <li> <strong>Chain-of-Thought (CoT)</strong>: A prompting method generating intermediate reasoning steps to arrive at a final answer.</li> <li> <strong>ReAct + CoT</strong>: Combines reasoning and action generation for more complex tasks.</li> </ul> <p><strong>Evaluation Metrics</strong>:</p> <ul> <li> <strong>Exact Match (EM)</strong>: The percentage of answers that match the ground truth exactly.</li> <li> <strong>Improvement over Trials</strong>: Measuring how the success rate changes over multiple trials.</li> </ul> <p><strong>Results</strong>:</p> <ul> <li> <strong>Performance</strong>: Reflexion improved performance by 20% on HotpotQA reasoning tasks compared to baseline approaches.</li> <li> <strong>Iterative Improvement</strong>: Reflexion agents demonstrated the ability to iteratively improve accuracy through self-reflection.</li> </ul> <h4 id="programming">Programming</h4> <p><strong>Task</strong>: The task involves writing correct code based on problem descriptions, including generating and debugging code in Python and Rust.</p> <p><strong>Datasets</strong>:</p> <ul> <li> <strong>HumanEval</strong>: A benchmark for evaluating the correctness of function implementations based on given problem descriptions.</li> <li> <strong>MBPP (ManyBabies Programming Problems)</strong>: A set of programming challenges in Python.</li> <li> <strong>LeetcodeHardGym</strong>: A newly introduced dataset consisting of 40 hard-rated Leetcode questions in 19 programming languages.</li> </ul> <p><strong>Baselines</strong>:</p> <ul> <li> <strong>AlphaCode</strong>: A code generation model evaluating generations on hidden test cases.</li> <li> <strong>CodeT</strong>: Uses self-generated unit tests to score function implementations.</li> <li> <strong>Self-Debugging</strong>: Employs debugging components to improve existing implementations.</li> <li> <strong>CodeRL</strong>: A reinforcement learning framework for code generation and debugging.</li> </ul> <p><strong>Evaluation Metrics</strong>:</p> <ul> <li> <strong>Pass@1</strong>: The accuracy of the first code generation sample passing all test cases.</li> <li> <strong>False Positive Rate</strong>: The proportion of incorrect solutions that pass self-generated unit tests.</li> <li> <strong>Accuracy</strong>: The overall correctness of the code implementations.</li> </ul> <p><strong>Results</strong>:</p> <ul> <li> <strong>HumanEval</strong>: Reflexion achieved a Pass@1 accuracy of 91% in Python, surpassing the previous state-of-the-art GPT-4 at 80%.</li> <li> <strong>MBPP</strong>: Reflexion maintained high performance in Python and Rust, demonstrating its language-agnostic capabilities.</li> <li> <strong>LeetcodeHardGym</strong>: Reflexion outperformed GPT-4 by achieving a higher pass rate on challenging Leetcode problems.</li> </ul> <p>In summary, the Reflexion framework demonstrated significant improvements across various tasks by leveraging reflective feedback stored in memory, highlighting its effectiveness and potential for further enhancement as LLM capabilities evolve.</p> <p>An example scenario for three of the main tasks is as follows:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/reflexion_example.png" alt="Solution" width="1000" height="400"></p> <h3 id="conclusion">Conclusion</h3> <p>The Reflexion framework significantly advances the field of LLM training by introducing memory-based learning and verbal reinforcement, which allows models to improve iteratively without the need for computationally expensive fine-tuning. Through the use of reflective feedback stored in memory, Reflexion achieves state-of-the-art performance across various tasks, including sequential decision-making, coding, and language reasoning. It surpasses existing methods, such as GPT-4, by demonstrating substantial improvements in benchmarks like the HumanEval coding test. By mimicking human reflective learning processes and leveraging the inherent strengths of LLMs, Reflexion provides a flexible and efficient alternative to traditional RL methods, representing a significant leap forward in the development of intelligent language agents.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <p class="mb-2">Here are some more articles relevant to this one:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-9/">Paper Review - Week 9</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-21/">Paper Review - Week 21</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/week-8/">Paper Review - Week 8</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/week-12/">Paper Review - Week 12</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-27/">Paper Review - Week 27</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Shirin Tahmasebi. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>