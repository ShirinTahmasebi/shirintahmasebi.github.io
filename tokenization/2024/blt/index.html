<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Byte Latent Transformer | Shirin Tahmasebi </title> <meta name="author" content="Shirin Tahmasebi"> <meta name="description" content="From Tokens to Patches "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?ce48ee9bc248ee6b18f3aeefc03ed2f9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shirintahmasebi.github.io/tokenization/2024/blt/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Shirin Tahmasebi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">CV </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">Publications</a> <a class="dropdown-item " href="/background/">Background</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Notes </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/bias/">Bias</a> <a class="dropdown-item " href="/tokenization/">Tokenization</a> <a class="dropdown-item " href="/llm_judge/">LLM as Evaluator</a> <a class="dropdown-item " href="/prompt/">Prompt Engineering</a> <a class="dropdown-item " href="/compression/">LLM Compression</a> <a class="dropdown-item " href="/agents/">LLM Agents</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/recsys/">Recommendation Systems</a> <a class="dropdown-item " href="/other/">Others</a> </div> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Byte Latent Transformer</h1> <p class="post-meta"> December 15, 2024 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2024   ·   <i class="fa-solid fa-hashtag fa-sm"></i> tokenization   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#byte-latent-transformer-patches-scale-better-than-tokens">Byte Latent Transformer: Patches Scale Better Than Tokens</a> <ul> <li class="toc-entry toc-h3"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h3"><a href="#research-gap">Research Gap</a></li> <li class="toc-entry toc-h3"> <a href="#solution-blt">Solution: BLT</a> <ul> <li class="toc-entry toc-h4"> <a href="#patch-extraction">Patch Extraction</a> <ul> <li class="toc-entry toc-h5"><a href="#what-are-patches">What Are Patches?</a></li> <li class="toc-entry toc-h5"><a href="#entropy-and-its-role">Entropy and Its Role</a></li> <li class="toc-entry toc-h5"><a href="#why-use-entropy">Why Use Entropy?</a></li> <li class="toc-entry toc-h5"><a href="#illustrative-example">Illustrative Example</a></li> </ul> </li> <li class="toc-entry toc-h4"> <a href="#patch-representation">Patch Representation</a> <ul> <li class="toc-entry toc-h5"><a href="#byte-embeddings">Byte Embeddings</a></li> <li class="toc-entry toc-h5"><a href="#adding-context-with-hash-n-gram-embeddings">Adding Context with Hash n-Gram Embeddings</a></li> <li class="toc-entry toc-h5"><a href="#training-vs-inference">Training vs. Inference</a></li> </ul> </li> <li class="toc-entry toc-h4"><a href="#encoder">Encoder</a></li> <li class="toc-entry toc-h4"><a href="#latent-transformer">Latent Transformer</a></li> <li class="toc-entry toc-h4"><a href="#decoder">Decoder</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#conclusion">Conclusion</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="byte-latent-transformer-patches-scale-better-than-tokens"><a href="https://arxiv.org/pdf/2412.09871" rel="external nofollow noopener" target="_blank">Byte Latent Transformer: Patches Scale Better Than Tokens</a></h2> <h3 id="introduction">Introduction</h3> <p>The Byte Latent Transformer (BLT) is a novel architecture that eliminates the need for traditional tokenization by directly processing raw bytes. This tokenizer-free approach allows the model to dynamically adapt to the complexity of the input data, making it robust across diverse domains and modalities. By leveraging entropy-based patching, adaptive compute allocation, and a combination of local and global attention mechanisms, BLT introduces a highly efficient and effective solution for sequence modeling tasks.</p> <h3 id="research-gap">Research Gap</h3> <p>The research gaps addressed by this paper are as follows:</p> <ol> <li>Dependency on Tokenizers: Traditional token-based LLMs rely heavily on a pre-defined tokenizer, which determines how raw text is split into tokens. This introduces several issues: <ul> <li>Domain Sensitivity: Tokenizers might not perform well across different domains or modalities because they are typically trained on a specific dataset.</li> <li>Input Noise Sensitivity: Tokenizers can be brittle to noisy inputs, like typos or uncommon character combinations.</li> <li>Multilingual Inequity: Fixed token vocabularies can be biased toward high-resource languages, disadvantaging low-resource or morphologically rich languages.</li> </ul> </li> <li>Not Truly End-to-End: Tokenization adds a heuristic, pre-processing step that prevents LLMs from being purely end-to-end systems. This limits their flexibility and robustness.</li> <li>Compute Allocation Inefficiency: Token-based models allocate equal computational resources (attention and processing) to every token, even though not all tokens are equally complex or important. This rigid allocation can lead to inefficiencies, especially for simpler, predictable patterns in the data. <ul> <li>For example: Predicting the first character of a new word or sentence often requires more computation than completing a common word (e.g., “M” in “Mozart” is harder to predict than “ozart”).</li> </ul> </li> </ol> <h3 id="solution-blt">Solution: BLT</h3> <p>First, let us have a quick overview of the main steps in the architecture, which is depicted in the figure below:</p> <ol> <li> <strong>Patch Extraction</strong>: Raw text is split into patches using an entropy model.</li> <li> <strong>Patch Representation</strong>: Byte embeddings and hash n-gram embeddings are combined to form enriched representations for each patch.</li> <li> <strong>Encoder</strong>: Processes patch representations using multi-head self-attention and feed-forward layers.</li> <li> <strong>Latent Transformer</strong>: Captures global dependencies across patches using block-causal attention.</li> <li> <strong>Decoder</strong>: Autoregressively generates the output sequence, integrating context from the latent transformer and previously generated bytes.</li> </ol> <p>Now, let us walk through each of these steps in detail.</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/blt_architecture_overview.png" alt="Overview of the BLT Approach" width="750" height="500"></p> <p style="text-align:center;"><img src="/assets/img/weekly-review/blt_architecture_detailed.png" alt="More Details of the BLT Approach" width="750" height="300"></p> <h4 id="patch-extraction">Patch Extraction</h4> <p>The core idea revolves around patches instead of tokens, and their dynamic creation based on entropy is central to the Byte Latent Transformer (BLT) approach.</p> <h5 id="what-are-patches">What Are Patches?</h5> <p>Patches are groups of consecutive bytes (e.g., characters) created dynamically instead of relying on pre-defined tokens. These patches serve as computation units, replacing traditional tokens.</p> <h5 id="entropy-and-its-role">Entropy and Its Role</h5> <p>Entropy measures the uncertainty or complexity of predicting the next byte in a sequence. High entropy indicates more unpredictable or complex data, while low entropy suggests predictable data. BLT uses entropy to decide where to split the sequence into patches:</p> <ul> <li> <strong>Global Threshold</strong>: If the entropy of the next byte exceeds a pre-set threshold, it marks the beginning of a new patch.</li> <li> <strong>Increasing Trend</strong>: If the entropy shows a sharp increase compared to the previous bytes, it signals a context shift and starts a new patch.</li> </ul> <h5 id="why-use-entropy">Why Use Entropy?</h5> <p>This approach ensures that compute is dynamically allocated:</p> <ul> <li> <strong>High-Entropy Regions</strong>: Complex parts (e.g., new words, context shifts) get more attention and computational resources.</li> <li> <strong>Low-Entropy Regions</strong>: Predictable data (e.g., spaces, repetitive patterns) is grouped together into longer patches, reducing computational overhead.</li> </ul> <p>By avoiding rigid tokenization, BLT adapts its structure based on the data’s complexity.</p> <h5 id="illustrative-example">Illustrative Example</h5> <p>For a sentence like <code class="language-plaintext highlighter-rouge">I go to work.</code>, here’s how patch extraction works step by step:</p> <ol> <li> <strong>Input: Raw Bytes</strong> The sentence is represented as a sequence of <strong>bytes</strong> (ASCII/UTF-8 encoded): <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>"I go to work." → [73, 32, 103, 111, 32, 116, 111, 32, 119, 111, 114, 107, 46]
</code></pre></div> </div> </li> <li> <strong>Modeling Byte Probabilities</strong> A <strong>small byte-level language model</strong> estimates the probability of each byte given the preceding bytes. <ul> <li>Probability of <code class="language-plaintext highlighter-rouge">73 (I)</code> given no prior context.</li> <li>Probability of <code class="language-plaintext highlighter-rouge">32 (space)</code> given <code class="language-plaintext highlighter-rouge">I</code>.</li> <li>Probability of <code class="language-plaintext highlighter-rouge">103 (g)</code> given <code class="language-plaintext highlighter-rouge">I</code> and <code class="language-plaintext highlighter-rouge">space</code>, and so on.</li> </ul> </li> <li> <strong>Entropy Calculation</strong> Entropy measures the uncertainty in predicting each byte. For a byte \(x_i\), the entropy \(H(x_i)\) is calculated as: \(H(x_i) = - \sum_{v \in V} p(v \vert x_{&lt;i}) \log p(v \vert x_{&lt;i})\), where: <ul> <li>\(V\): The byte vocabulary (all possible byte values, e.g., 0–255 for UTF-8).</li> <li>\(p(v \vert x_{&lt;I})\): The probability of byte \(v\) given the previous bytes \(x_{&lt;I}\).</li> </ul> </li> <li> <strong>Detecting Patch Boundaries</strong> Using the entropy values \(H(x_i)\), the model identifies boundaries for patches based on: <ul> <li>A <strong>global threshold</strong> (e.g., entropy &gt; threshold marks a new patch).</li> <li>A <strong>trend check</strong>, where entropy sharply increases compared to previous values.</li> </ul> </li> </ol> <p>Resulting Patches for <code class="language-plaintext highlighter-rouge">I go to work.</code>:</p> <ul> <li>Patch 1: <code class="language-plaintext highlighter-rouge">I go</code>.</li> <li>Patch 2: <code class="language-plaintext highlighter-rouge">​ to</code>.</li> <li>Patch 3: <code class="language-plaintext highlighter-rouge">​ work.</code>.</li> </ul> <h4 id="patch-representation">Patch Representation</h4> <p>Once patches are created, they are converted into patch representations through the following steps:</p> <h5 id="byte-embeddings">Byte Embeddings</h5> <p>Each byte in the patch is mapped to a high-dimensional vector using a learned embedding matrix. For example, in Patch 1 (<code class="language-plaintext highlighter-rouge">I go</code>):</p> <ul> <li>Bytes: <code class="language-plaintext highlighter-rouge">I</code>, space (<code class="language-plaintext highlighter-rouge">​ </code>), <code class="language-plaintext highlighter-rouge">g</code>, <code class="language-plaintext highlighter-rouge">o</code> </li> <li>Byte embeddings: Each byte is independently mapped to a vector.</li> </ul> <h5 id="adding-context-with-hash-n-gram-embeddings">Adding Context with Hash n-Gram Embeddings</h5> <p>To enrich local context, n-grams (sequences of consecutive bytes) are used. For example, in Patch 1 (<code class="language-plaintext highlighter-rouge">I go</code>):</p> <ul> <li>2-grams: [<code class="language-plaintext highlighter-rouge">I​ </code>, <code class="language-plaintext highlighter-rouge">​ g</code>, <code class="language-plaintext highlighter-rouge">go</code>]</li> <li>3-grams: [<code class="language-plaintext highlighter-rouge">I g</code>, <code class="language-plaintext highlighter-rouge">​ go</code>]</li> </ul> <p>Each n-gram is hashed into a fixed-size hash table, and the corresponding embedding is retrieved and added to the byte embedding. This provides local context for each byte.</p> <h5 id="training-vs-inference">Training vs. Inference</h5> <ul> <li> <strong>Training</strong>: The hash table is randomly initialized and updated during training using backpropagation.</li> <li> <strong>Inference</strong>: The trained hash table is used to retrieve embeddings for new input on-the-fly.</li> </ul> <h4 id="encoder">Encoder</h4> <p>The patch representations are then fed into the encoder, which processes them further to capture dependencies across patches. This is how it works:</p> <ul> <li> <strong>Multi-Head Self-Attention</strong>: Similar to BERT, the encoder uses key, query, and value matrices to compute attention. However, instead of tokens, BLT’s encoder operates on patch representations.</li> <li> <strong>Feed-Forward Layers</strong>: Stacked layers of feed-forward networks refine the patch representations.</li> </ul> <p>This allows the encoder to model relationships between patches rather than individual tokens.</p> <h4 id="latent-transformer">Latent Transformer</h4> <p>The latent transformer is a global component in BLT that processes the patch representations to model long-range dependencies across patches. The key features are:</p> <ul> <li> <strong>Block-Causal Attention</strong>: Ensures autoregressive behavior, where each patch can only attend to itself and preceding patches (not future ones). For example: <ul> <li>Patch 1 (<code class="language-plaintext highlighter-rouge">I go</code>) attends only to itself.</li> <li>Patch 2 (<code class="language-plaintext highlighter-rouge"> to</code>) attends to Patch 1 and itself.</li> </ul> </li> <li> <strong>Purpose</strong>: Captures global context across the sequence and allocates compute dynamically based on patch complexity.</li> </ul> <h4 id="decoder">Decoder</h4> <p>The decoder generates the final sequence of bytes autoregressively, based on the latent representations from the latent transformer. This is how it works:</p> <ul> <li> <strong>Cross-Attention</strong>: Combines information from the latent transformer (patch-level context) and previously generated bytes. <ul> <li>Query: Byte representations from the decoder.</li> <li>Key/Value: Patch representations from the latent transformer.</li> </ul> </li> <li> <strong>Byte-by-Byte Generation</strong>: The decoder predicts one byte at a time until the entire sequence is generated.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>So, by the proposed solution, the research gaps mentioned earlier are addressed as follows:</p> <ul> <li>Dependency on Tokenizers: BLT processes raw bytes directly, eliminating the need for a tokenizer and the associated domain, noise, and multilingual biases.</li> <li>Not Truly End-to-End: BLT integrates the entire learning process into the model, removing the pre-processing step of tokenization, making the system fully end-to-end.</li> <li>Compute Allocation Inefficiency: BLT dynamically groups bytes into patches based on entropy, allocating more compute to complex regions and less to predictable ones, improving efficiency and performance.</li> </ul> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Shirin Tahmasebi. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>