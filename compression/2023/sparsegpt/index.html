<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> SparseGPT | Shirin Tahmasebi </title> <meta name="author" content="Shirin Tahmasebi"> <meta name="description" content="Making Smaller LLMs in One-shot "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?ce48ee9bc248ee6b18f3aeefc03ed2f9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shirintahmasebi.github.io/compression/2023/sparsegpt/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Shirin Tahmasebi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">CV </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">Publications</a> <a class="dropdown-item " href="/background/">Background</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Notes </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/bias/">Bias</a> <a class="dropdown-item " href="/tokenization/">Tokenization</a> <a class="dropdown-item " href="/llm_judge/">LLM as Evaluator</a> <a class="dropdown-item " href="/prompt/">Prompt Engineering</a> <a class="dropdown-item " href="/compression/">LLM Compression</a> <a class="dropdown-item " href="/agents/">LLM Agents</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/recsys/">Recommendation Systems</a> <a class="dropdown-item " href="/other/">Others</a> </div> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">SparseGPT</h1> <p class="post-meta"> March 22, 2023 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2023   ·   <i class="fa-solid fa-hashtag fa-sm"></i> compression   <i class="fa-solid fa-hashtag fa-sm"></i> pruning   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#sparsegpt-massive-llms-can-be-accurately-pruned-in-one-shot">SparseGPT: Massive LLMs Can be Accurately Pruned in One-Shot</a> <ul> <li class="toc-entry toc-h3"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h3"><a href="#research-gap">Research Gap</a></li> <li class="toc-entry toc-h3"><a href="#sparsegpt">SparseGPT</a></li> <li class="toc-entry toc-h3"><a href="#conclusion">Conclusion</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="sparsegpt-massive-llms-can-be-accurately-pruned-in-one-shot"><a href="https://arxiv.org/pdf/2301.00774" rel="external nofollow noopener" target="_blank">SparseGPT: Massive LLMs Can be Accurately Pruned in One-Shot</a></h2> <h3 id="introduction">Introduction</h3> <p>SparseGPT is a <strong>one-shot pruning method</strong> that enables <strong>massive language models</strong> (e.g., GPT-3-scale) to be pruned <strong>without retraining or fine-tuning</strong>. Unlike previous pruning techniques, which require multiple training steps to recover accuracy, SparseGPT efficiently removes <strong>50-60% of weights</strong> while maintaining <strong>low accuracy loss</strong>.</p> <p>This pruning method can be categorized based on:</p> <ol> <li> <strong>Granularity</strong>: SparseGPT performs <strong>unstructured pruning</strong> (removes individual weights), but it also supports <strong>semi-structured pruning</strong> (e.g., <strong>2:4, 4:8 sparsity</strong>, which is optimized for GPU acceleration).</li> <li> <strong>Pruning Criteria</strong>: Instead of simply removing small-magnitude weights, SparseGPT <strong>estimates weight importance using the Hessian inverse</strong> and removes the <strong>least important</strong> ones.</li> <li> <strong>Pruning Ratio</strong>: It achieves <strong>50-60% sparsity</strong> with minimal impact on model accuracy.</li> <li> <strong>Need for Retraining</strong>: <strong>No retraining or fine-tuning is required</strong>. It works in a <strong>one-shot</strong> manner, pruning <strong>each layer individually</strong> while adjusting the remaining weights.</li> </ol> <h3 id="research-gap">Research Gap</h3> <p>Prior to SparseGPT, <strong>pruning large-scale language models</strong> (100B+ parameters) was <strong>an unsolved problem</strong> due to two major challenges: <strong>scalability and accuracy recovery</strong>. Most pruning methods required <strong>iterative fine-tuning</strong> to regain accuracy after removing weights, making them computationally infeasible for <strong>GPT-scale models</strong>. Even existing <strong>one-shot pruning methods</strong> were limited in effectiveness because they either relied on <strong>simple magnitude pruning</strong> (which severely degrades accuracy at high sparsity levels) or were <strong>too expensive to apply</strong> to models with billions of parameters.</p> <p>SparseGPT <strong>fills this gap</strong> by introducing a <strong>scalable one-shot pruning method</strong> that can <strong>prune extremely large models</strong> without fine-tuning while preserving performance. The method is designed to handle <strong>models with 100+ billion parameters</strong> efficiently, achieving <strong>high sparsity (50-60%)</strong> without requiring costly retraining. This work is the <strong>first to demonstrate</strong> that GPT-scale models can be <strong>accurately pruned in one shot</strong>, making pruning <strong>practical for real-world deployment</strong>.</p> <h3 id="sparsegpt">SparseGPT</h3> <ol> <li>Layer-wise Pruning: <ul> <li>It <strong>prunes one layer at a time</strong> rather than the whole network simultaneously.</li> </ul> </li> <li>Selecting Which Weights to Prune (Using Hessian Inverse): <ul> <li>The <strong>Hessian matrix</strong> captures how each weight <strong>affects the loss function</strong>.</li> <li>SparseGPT <strong>approximates the inverse of this Hessian</strong>, which helps measure the <strong>sensitivity of the loss function</strong> to each weight by calculating <strong>Hessian matrix</strong>.</li> <li>It <strong>removes the least important weights</strong>, meaning those that have the lowest impact on the layer’s output.</li> </ul> </li> <li>Reconstructing the Remaining Weights (Using Sparse Regression): <ul> <li>After pruning, the layer’s output <strong>would normally change</strong>, which could hurt accuracy.</li> <li>To fix this, SparseGPT <strong>solves a sparse regression problem</strong>: <ul> <li> <strong>Sparse regression</strong> is a technique that <strong>finds the best values for the remaining weights</strong> while minimizing the squared error between the <strong>original layer output and the pruned layer output</strong>.</li> </ul> </li> <li>This adjustment ensures that the <strong>pruned layer behaves similarly to the original</strong>.</li> </ul> </li> <li>One-Shot Execution: <ul> <li>This process is done <strong>once per layer</strong>, moving through the network <strong>without retraining</strong>.</li> <li>The entire model can be pruned in just a few hours on a single GPU, even for 100+ billion parameter models.</li> </ul> </li> </ol> <h3 id="conclusion">Conclusion</h3> <p>SparseGPT is important because it enables highly efficient pruning of massive GPT models while avoiding expensive retraining. It is particularly valuable for large-scale models like OPT-175B and BLOOM-176B, where standard pruning approaches either fail or require enormous computational resources. By allowing 50-60% sparsity while maintaining low accuracy loss, SparseGPT makes large models more efficient and easier to deploy.</p> <p>SparseGPT achieves this by measuring weight importance using a Hessian inverse approximation to determine sensitivity to loss, removing the least important weights, and recalculating the remaining ones using sparse regression to minimize accuracy loss. Since pruning happens layer by layer in one-shot, it does not require fine-tuning or multiple training iterations.</p> <p>Additionally, SparseGPT supports hardware-friendly sparsity patterns (such as 2:4 and 4:8 sparsity) that make it compatible with modern GPU acceleration techniques. Furthermore, it can be combined with quantization to further reduce memory and computation costs.</p> <p>By making large language models faster, smaller, and more efficient, SparseGPT is a practical and scalable solution for LLM compression, enabling cost-effective deployment without sacrificing accuracy.</p> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Shirin Tahmasebi. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>