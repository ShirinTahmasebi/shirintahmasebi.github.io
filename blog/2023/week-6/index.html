<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Paper Review - Week 6 | Shirin Tahmasebi </title> <meta name="author" content="Shirin Tahmasebi"> <meta name="description" content="Top NLP Papers Published from February 6 to February 12 - Toolformer and Offsite-Tuning "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?ce48ee9bc248ee6b18f3aeefc03ed2f9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shirintahmasebi.github.io/blog/2023/week-6/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Shirin Tahmasebi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/background/">Background </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Paper Review - Week 6</h1> <p class="post-meta"> February 12, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/papers"> <i class="fa-solid fa-tag fa-sm"></i> papers</a>   <a href="/blog/category/weekly-review"> <i class="fa-solid fa-tag fa-sm"></i> weekly-review</a>   <a href="/blog/category/nlp"> <i class="fa-solid fa-tag fa-sm"></i> nlp</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#toolformer">Toolformer</a></li> <li class="toc-entry toc-h2"><a href="#offsite-tuning">Offsite-Tuning</a></li> </ul> </div> <hr> <div id="markdown-content"> <p>Several interesting papers are published in this week in NLP. Here is a list of them:</p> <ul> <li><a href="/blog/2023/week-6/#toolformer">Toolformer: Language Models Can Teach Themselves to Use Tools</a></li> <li><a href="/blog/2023/week-6/#offsite-tuning">Offsite-Tuning: Transfer Learning without Full Model</a></li> </ul> <hr> <h2 id="toolformer"><a href="https://arxiv.org/pdf/2302.04761.pdf" rel="external nofollow noopener" target="_blank">Toolformer</a></h2> <p>This paper is really interesting, because it shows that language models, such as GPT-3, can be trained to perform arithmetic operations, such as addition, subtraction, multiplication, and division, with a high level of accuracy. While probabilistic methods are often used to generate answers to questions, some questions have a definite answer that should be calculated rather than generated randomly. In such cases, the use of language models that can generate exact answers can be beneficial.</p> <p>The idea in <a href="https://arxiv.org/pdf/2302.04761.pdf" rel="external nofollow noopener" target="_blank">Toolformer</a> is to leverage some external tools (e.g., a calculator, Q&amp;A systems, search engines, translators, a calendar) to enhance the model’s ability for doing some basic tasks, such as arithmetic operations and factual lookups. For training a language model for this purpose, a self-supervised approach is followed. First, given a corpus, it is necessary to create the train dataset for training the model. For doing so, each sentence of the corpus is modified and augmented by injecting the API calls. Among all the available positions of a sentence, the API calls are added where the probabiltiy of having the API calls is the most, based on the predictions of a langugae model such as GPT-J. Then, the response of the API call is added right after that. But, how to make sure that this sentence is good enough to be added to the train dataset? To ensure that the sentence is good enough, they calculate two loss functions for the augmented sentence. But, what are the two loss functions?</p> <ol> <li> <p>The augmented sentence is first passed to GPT-J to generate the output. Then, the cross-entropy of the generated sentence (by GPT-J) and the original sentence is calcuated. This cross-entropy shows how much the generated sentence is close to its original version. The more similar the better. We use \(loss_1\) to refer to this loss value.</p> </li> <li> <p>The original sentence (without any API calls) and the augmented sentence without the response of the API calls are passed to the GPT-J model. The cross-entropy of the both generated sentences againt the original sentence is calculated. Then, the minumum of the two calculated cross-entropy is considered as the seond loss value. We use \(loss_2\) to refer to this loss value.</p> </li> </ol> <p>After calculating \(loss_1\) and \(loss_2\), if (\(loss_2\) - \(loss_1\)) is more than a specific threshold, then it means that adding the API call can help the model to generate a sentence similar to the original one. So, in this case, the augmented sentence is added to the train dataset.</p> <h2 id="offsite-tuning"><a href="https://arxiv.org/pdf/2302.04761.pdf" rel="external nofollow noopener" target="_blank">Offsite-Tuning</a></h2> <p>This problem focuses on a very interesting problem. Recently, Large Language Models (LLMs) are being increasingly popular because of their fantastic performance for different task. However, since these models are very large and also significantly-high computation power is required for training or even fine-tuning them, users cannot download the model weights on their systems to adapt them based on their data and task. Apart from that, in some cases the model owners are also very reluctant to share their models publicly–such as GPT-3 proposed by OpenAI. The approach that is now being used by both model owners and data owner is that the data owner sends the data to the model owner data. Then, the model owner fine-tune the model based on the data and provides the access to the fine-tuned model to the data owner <a href="https://platform.openai.com/docs/guides/fine-tuning" rel="external nofollow noopener" target="_blank">Link</a>. This process violates the privacy of data owners, because they need to send their data directly to the model owners.</p> <p>The offsite-tuning provides a solution to this problem. First of all, let us make a list of the requirements that such solution needs to satisfy:</p> <ol> <li>The solution should allow the model owner not to share the model weights.</li> <li>The solution should allow the data owner not to share their data directly with the model owners.</li> <li>The solution should result in a better performance compared to using the zero-shot reasoning capabiltiy of the model.</li> </ol> <p>Keeping these requirements in mind, the approach in offsite-tuning can be summarized as follows:</p> <ul> <li>The LM can be separated into two part, the concatenation of which makes the whole language model: <ol> <li>A freezed part: In this part, all the wieght are fixed. No matter what the data is or what the task is, the weights in this part are always freezed.</li> <li>An adaptable part: This part should be fine-tuned based on the data. So, this part can vary based on the data for each data owner.</li> </ol> </li> <li>According to this separation, the model owner sends two things to data owners: <ol> <li>A compressed version of the freezed part, named <strong>emulator</strong>.</li> <li>The initialized parameters of the adaptable part, named <strong>adapter</strong>.</li> </ol> </li> <li>Data owners can use the concatenation of emulator and adapter to train the adapter part based on their data.</li> <li>After training the adapter, the data owners can send the trained adapter to the model owner. So, the model owner can create the fine-tuned model by concatenating the freezed part and the trained adapter. And then, the model owner provides the access of this newly-created model to the data owner.</li> </ul> <p>Accordingly, the three requirements metioned above can be formulated as follows:</p> <ol> <li>The emulator’s performance should be significantly less than the main model’s performance. In this way, we can ensure that the model owner would not be concerned of sharing their emulator with users.</li> <li>The emulator and the adapter should be light-weight enough for being fine-tuned by data owners.</li> <li>Using the newly-created model (provided by the model owner) should result in better performance compared to when the original model is used in its few-shot mode.</li> </ol> <p>In general, this is a very novel approach for addressing the violation of data privacy problem in fine-tuning LLMs.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <p class="mb-2">Here are some more articles relevant to this one:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-27/">Paper Review - Week 27</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-25/">Paper Review - Week 25</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-22/">Paper Review - Week 22</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-9/">Paper Review - Week 9</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-36/">Paper Review - Week 36</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Shirin Tahmasebi. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>