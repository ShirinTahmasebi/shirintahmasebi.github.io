---
layout: post
title: Paper Review - Week 11
image: 
  path: /assets/img/weekly-review/nlp_cover_week_11.jpg
categories: [papers, weekly-review, nlp]
description: >
    Top NLP Papers Published from March 13 to March 19 - CoLT5
sitemap: false
hide_last_modified: true
---


Several interesting papers are published in this week in NLP. Here is a list of them:

* [CoLT5: Faster Long-range Transformers with Conditional Computation][colt5]
* [Memorizing Transformers][memorizingTransformers]

## CoLT5: Faster Long-range Transformers with Conditional Computation

This paper focuses on the scalability problem of computing attentions in transformer-based architectures. Such scalability problem especially appears while processing long documents with a transformer-based models; the reason is that: 
1. A main component of the transformer architecture is the attention module. However, the attention computational cost scales quadratically with input length.
1. The feedforward network and the projection layer in the transformer architecture should be applied on all of the input tokens.

CoLT5 is an extension to LongT5, which tries to solve the mentioned challenge. The main objective of CoLT5 is to reduce the computation burden in both the training phase and inference phase in the transformer-based architectures. The main idea of CoLT5 is to divide the transformer components into two branches: light branch--which is applied to all of the tokens in the input, and the heavy-branch--which is only applied to a selected list of input tokens.

The architecture of CoLT5 is depicted in the following figure:

<p style="text-align:center;"><img src="/assets/img/weekly-review/CoLT5/architecture.png" alt="CoLT5 Architecture" width="450" height="500"></p>

As is shown, one of the components of CoLT5 is the **Routing** component. Routing component is composed of two parts:
1. a learnable vector which has the same dimentionality as the input. This vector is multiplied by all the input tokens, leading to having a score value for each of the tokens. So, it the input has _n_ tokens, at this step, _n_ score values are calculated. 
1. a function which recieves the score of all input tokens and select the top _k_ input tokens with the highest score. 


After the first routing component, input tokens are divided into two groups: the important and the non-important ones! The important ones (the ones with the highest score) will be passed to the **heavy attention** component. In this component, the  full attention is applied to all of these tokens. Then, the less important ones are passed to the **light attention** component. In this component, for each token, the attention is applied only to a specific number of tokens surrounding it--its local context window. In other words, the difference between the light and heavy attention is that:
* The light attention has fewer head and attends only yo a local context window.
* The heavy attention has more heads and attends to all the important (highly-scored) tokens.

The idea behid leveraging this approach is that, for the more important tokens the long-range dependencies are worth to be counted. However, for the less important ones, only the short-range dependencies are worth counting. 

After passing the tokens through the heavy and light attention components, the output of the attention components are used to calculate the new representations. For example, if $$s_i$$ is the importance score of token $$i$$ (which is set to 0 if token $$i$$ is not among the top-$$k$$ important tokens), then the new representation of token $$i$$ is represented by $$X_i$$, which is calculated as:

$$X_i = X_i + Att_{Light}(X_i) + s_i * Att_{Heavy}(X_i)$$


Then, the newly-calculated representations are passed to another routing component to select the top-$$k$$ important tokens. The most important ones are passed to a **heavy feedforward network** and the less important ones are passed to a **light feedforward network**. The difference between the light and heavy feedforward networks is only in their number of hidden layers. So, to calculate the final representation of the input tokens, considering $$s_i$$ as the importance score of token $$i$$, the following formula can be used:

$$X_i = X_i + FFN_{Light}(X_i) + s_i * FFN_{Heavy}(X_i)$$


They used FLOP as the metric for measuring the computational cost of their approach. Their experiments show that they could achieve less computational cost compared to their baseline--LongT5.

[memorizingTransformers]: https://arxiv.org/abs/2203.08913
[colt5]: https://arxiv.org/pdf/2303.09752.pdf