<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Paper Review - Week 9 | Shirin Tahmasebi </title> <meta name="author" content="Shirin Tahmasebi"> <meta name="description" content="Top NLP Papers Published from February 27 to March 05 "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?ce48ee9bc248ee6b18f3aeefc03ed2f9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shirintahmasebi.github.io/blog/2023/week-9/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Shirin Tahmasebi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Paper Review - Week 9</h1> <p class="post-meta"> March 05, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/papers"> <i class="fa-solid fa-tag fa-sm"></i> papers</a>   <a href="/blog/category/weekly-review"> <i class="fa-solid fa-tag fa-sm"></i> weekly-review</a>   <a href="/blog/category/nlp"> <i class="fa-solid fa-tag fa-sm"></i> nlp</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#mathprompter">MathPrompter</a></li> <li class="toc-entry toc-h2"> <a href="#llama-open-and-efficient-foundation-language-models">LLaMA: Open and Efficient Foundation Language Models</a> <ul> <li class="toc-entry toc-h3"><a href="#training-datasets">Training Datasets</a></li> <li class="toc-entry toc-h3"><a href="#architecture">Architecture</a></li> <li class="toc-entry toc-h3"><a href="#performance">Performance</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <p>To me, the most interesting papers among those published in this week are:</p> <ul> <li><a href="/blog/2023/week-9/#mathprompter">MathPrompter: Mathematical Reasoning Using Large Langugage Models</a></li> <li><a href="/blog/2023/week-9/#llama-open-and-efficient-foundation-language-models">LLaMA: Open and Efficient Foundation Language Models</a></li> </ul> <hr> <h2 id="mathprompter"><a href="https://arxiv.org/pdf/2303.05398.pdf" rel="external nofollow noopener" target="_blank">MathPrompter</a></h2> <p>This paper focuses on using LLMs for mathematical reasoning. Some of the challenges in this work are as follows:</p> <ul> <li>Mathematical operations usually have a single correct answer.</li> <li>Not only the final output is important, the procedure and the intermidate steps that resulted into that final output should also be correct.</li> <li>It is difficult to ensure that the generated output and the intermediate steps are all correct. In other words, it is better to have a metric to show that how much the model is confident about the generated output!</li> </ul> <p>Accordingly, MathPrompter tries to propose a solution to increase the validity and reliability of mathematical reasoning using LLMs. Their solution is to simulate the usual approach that studnets usually take for solving a math problem. So, the steps in MathPrompter is as follows:</p> <ol> <li>Receiving the definition of a mathematical problem as text. For example: <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>At a restaurant, each adult meal costs $5 and kids eat free. If a group of 15 people came in and 8 were kids, how much would it cost for the group to eat? 
</code></pre></div> </div> </li> <li>Generating a template for the input question and creating a list of values for solving the template: <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>Template = At a restaurant, each adult meal costs A and kids eat free. if a group of B people came in and C were kids, how much would it cost for the group to eat?
Mapping = {A:5, B:15, C:8}
</code></pre></div> </div> </li> <li>Creaing proper prompts for creating different solutions. For example, one solution can be a python function and the other can be a algebraic equation. So, by providing these prompts to the LLM: <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>Algebraic prompt: Write a mathematical equation and generate the answer format starting with ‘Answer =’
Python prompt: Write a Python function that returns the answer.
</code></pre></div> </div> <p>then, the LLM is supposed to generated the two following outputs:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Algebraic expression output
</span><span class="n">Answer</span> <span class="o">=</span> <span class="n">A</span> <span class="o">*</span> <span class="p">(</span><span class="n">B</span> <span class="o">-</span> <span class="n">C</span><span class="p">)</span>
<span class="c1"># Python expression output
</span><span class="k">def</span> <span class="nf">total_price</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">A</span> <span class="o">*</span> <span class="p">(</span><span class="n">B</span> <span class="o">-</span> <span class="n">C</span><span class="p">)</span>
</code></pre></div> </div> <p>These outputs can help the user to have an insight about how the LLM is reasoning.</p> </li> <li>Then, MathPrompter uses different randomized values for \(A\), \(B\), and \(C\). Then, it passes all the randomized values to the both outpus. If, for more than 5 times, the result of the two outputs (algebraic and python expression) are the same, then both of them are accepted as the correct solutions for the mathematical problem.</li> </ol> <p>This is definitely a very interesting and novel idea. However, I personally have some concerns about it:</p> <ol> <li>First of all, MathPrompter is not making it easier for LLMs to find a solution. The only thing that it does is to ask LLMs to generate multiple outputs, such as algebraic and python expression.</li> <li>MathPrompter uses several randomized values (at least 5 values) to validate if the LLM outputs result in the same number or not. However, it is not clear that how these randomized values are generated. If users are supposed to provide these values, then I think it somehow makes the LLM useless here! Because users need to solve the problem themselves to be able to provide such values. So, in this case, they don’t need LLMs anymore! :)</li> <li>If for the specific values in the input mathematical problem, algebraic and python expressions result in different values, then what can be reported to the user?</li> </ol> <h2 id="llama-open-and-efficient-foundation-language-models"><a href="https://arxiv.org/pdf/2302.13971.pdf" rel="external nofollow noopener" target="_blank">LLaMA: Open and Efficient Foundation Language Models</a></h2> <p>Recently, the trend of novel language models is that they are becomming larger and larger. The main idea behind this trend is that, probabely, by increasing the number of parameters of a language model, it can have a better performance. In other words, the prevalent idea is that, the larger the model, the better the performance.</p> <p>However, a paper, published in 2022, tried to challange this idea. They hypothesised that it is not only the size of the language models which make their quality better. There is another very important aspect: <strong>the size of the training data</strong>. This hypothesis, which is also known as the <strong>Hoffmann’s Scaling Law</strong>, has been the foundation of one of the recenly-proposed language models, named <strong>Chinchilla</strong>. Chinchilla has only 70B parameter (compared to GPT-3 with about 175B parameter) and can still get comparable performance to GPT-3.</p> <p>LLaMa is another novel language model which uses the Hoffmann’s scaling law as its foundation. It has several variants, each of which with different number of parameter–ranging from 7B to 65B parameter. Although the size of the model is significantly smaller than GPT-3, it has comparable results to GPT-3 for most downstream task; and even for some of the downstream tasks, it can outperform GPT-3.</p> <p>Now, to investigate the specific charactristics of LLaMa, we look at its three aspects:</p> <ol> <li>Training Datasets</li> <li>Architecture</li> <li>Performance</li> </ol> <h3 id="training-datasets">Training Datasets</h3> <p>As mentioned previously, LLaMa is supposed to have between 7B and 65B parameters. To this end, according to the Hoffmann’s scaling law, the training dataset consist of 14 Trillion tokens. To collect this 14 Trillion tokens, the following datasets are used:</p> <ul> <li>English CommonCrawl [67%]: Public crawled web pages from 2017 to 2020</li> <li>C4 [15%]: Preprocessed CommonCrawl!</li> <li>Github [4.5%]</li> <li>Wikipedia [4.5%]: Covering 20 languages from June 2022 to August 2022</li> <li>Gutenberg and Books3 [4.5%]: Two book corpora from public domain</li> <li>ArXiv [2.5%]</li> <li>Stack Exchange [2%]: From 28 topics</li> </ul> <h3 id="architecture">Architecture</h3> <p>LLaMa is a transformer-based model. However, instead of using the main transformer building block as it is, they apply several minor changes to it. These changes are mainly for reducing the training time. The changes are as follows:</p> <ul> <li>Pre-normalization instead of post-normalization</li> <li>Activation function from ReLU to SwiGLU</li> <li>Positional encoding from absolute to Rotary</li> </ul> <p>The following image represents how the architecture looks like after applying the above changes. The image on the left shows the usual transformer building block and the one on the right illustrates the building block after applying the changes.</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/llama_architecture_building_block.png" alt="Candidate Generation Approaches" width="800" height="400"></p> <h3 id="performance">Performance</h3> <p>They have evaluated LLaMa on different tasks and compared it with the two of the recent extremely large language models. The results are as the following:</p> <ul> <li>Common Sense Reading <ul> <li>Evaluated in zero-shot setting.</li> <li> <strong>LLaMa (65B)</strong> <em>outperforms</em> <strong>Chinchilla (70B)</strong>.</li> <li> <strong>LLaMa (65B)</strong> <em>outperforms</em> <strong>PaLM (540B)</strong> and <strong>GPT-3 (175B)</strong> on most datasets.</li> </ul> </li> <li>Closed-book Question Answering <ul> <li>Evaluated in zero-shot and few-shot settings.</li> <li> <strong>LLaMa (13B)</strong> is <em>competitive</em> with <strong>GPT-3 (175B)</strong> and <strong>Chinchilla (70B)</strong>.</li> <li> <strong>LLaMa (65B)</strong> <em>outperforms</em> <strong>PaLM (540B)</strong>, <strong>GPT-3 (175B)</strong>, and <strong>Chinchilla (70B)</strong>.</li> </ul> </li> <li>Reading Comprehension <ul> <li>Evaluated in zero-shot setting.</li> <li> <strong>LLaMa (65B)</strong> is <em>competitive</em> with <strong>PaLM (540B)</strong>.</li> <li> <strong>LLaMa (13B)</strong> <em>outperforms</em> <strong>GPT-3 (175B)</strong>.</li> </ul> </li> <li>Mathematical Reasoning <ul> <li> <strong>LLaMa (65B)</strong> <em>outperforms</em> <strong>PaLM (540B)</strong>.</li> </ul> </li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <p class="mb-2">Here are some more articles relevant to this one:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-27/">Paper Review - Week 27</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-25/">Paper Review - Week 25</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-36/">Paper Review - Week 36</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-10/">Paper Review - Week 10</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-6/">Paper Review - Week 6</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Shirin Tahmasebi. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>