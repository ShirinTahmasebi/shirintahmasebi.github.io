<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Paper Review - Week 41 | Shirin Tahmasebi </title> <meta name="author" content="Shirin Tahmasebi"> <meta name="description" content="Top NLP Papers Published from October 09 to October 15 "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?ce48ee9bc248ee6b18f3aeefc03ed2f9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shirintahmasebi.github.io/blog/2023/week-41/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Shirin Tahmasebi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Paper Review - Week 41</h1> <p class="post-meta"> October 15, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/papers"> <i class="fa-solid fa-tag fa-sm"></i> papers</a>   <a href="/blog/category/weekly-review"> <i class="fa-solid fa-tag fa-sm"></i> weekly-review</a>   <a href="/blog/category/nlp"> <i class="fa-solid fa-tag fa-sm"></i> nlp</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#prompt-distillation-for-efficient-llm-based-recommendation">Prompt Distillation for Efficient LLM-based Recommendation</a> <ul> <li class="toc-entry toc-h3"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h3"><a href="#research-gap">Research Gap</a></li> <li class="toc-entry toc-h3"><a href="#solution-pod">Solution: POD</a></li> <li class="toc-entry toc-h3"><a href="#experiments">Experiments</a></li> <li class="toc-entry toc-h3"><a href="#conclusion">Conclusion</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <p>Here is the list of the most interesting papers published in this week:</p> <ul> <li><a href="/blog/2023/week-41/#prompt-distillation-for-efficient-llm-based-recommendation">Prompt Distillation for Efficient LLM-based Recommendation</a></li> </ul> <hr> <h2 id="prompt-distillation-for-efficient-llm-based-recommendation"><a href="https://lileipisces.github.io/files/CIKM23-POD-paper.pdf" rel="external nofollow noopener" target="_blank">Prompt Distillation for Efficient LLM-based Recommendation</a></h2> <h3 id="introduction">Introduction</h3> <p>The paper presents an novel approach to multi-task recommendation systems, aiming to train a single model that handles various recommendation tasks, including sequence recommendation, top-N recommendation, and explanation generation. Leveraging a novel prompt distillation technique, the study formulates these recommendation tasks as text generation challenges, effectively transforming them into a unified framework. The proposed method entails the extraction of continuous prompts from discrete prompts, which are both task and sample-specific, enabling the integration of tailored prompts into a single LLM. They conducted comprehensive experiments and compared their system (POD) with state-of-the-art baselines on three Amazon datasets. The study shows the effectiveness of the proposed method in enhancing recommendation accuracy and generating coherent explanations.</p> <h3 id="research-gap">Research Gap</h3> <p>The research gap that the authors are addressing revolves around the limitations of integrating LLMs into recommendation systems, particularly with respect to handling user and item IDs in the context of the textual data processed by LLMs. They specifically focus on the following challenges:</p> <ul> <li> <p><strong>Efficiency in Processing Long Texts:</strong> The authors emphasize that the input to LLM-based recommendation models is often long and may contain noisy information, making the processing time-consuming and inefficient, especially for tasks that require immediate responses.</p> </li> <li> <p><strong>Fine-tuning and Bridging the Gap:</strong> LLMs typically require extensive fine-tuning to bridge the gap between user/item IDs and the template words, hindering their full potential for recommendation tasks.</p> </li> <li> <p><strong>Inference Efficiency:</strong> While they successfully improve the training efficiency, the authors note that enhancing the inference efficiency of LLM-based recommendation models remains a challenging issue.</p> </li> </ul> <p>The proposed PrOmpt Distillation (POD) approach aims to address these issues by distilling discrete prompts into continuous prompt vectors, which can reduce the inference time and improve the overall efficiency of LLM-based recommendation models. By focusing on this prompt distillation technique and proposing a task-alternated training strategy, the authors seek to enhance the performance of LLMs in the context of various recommendation tasks.</p> <h3 id="solution-pod">Solution: POD</h3> <p>The solution is based on training a unified model for diverse recommendation tasks by reframing these tasks as text generation problems. It covers a range of tasks including sequence recommendation, top-N recommendation, and explanation tasks.</p> <p>The key technique employed is <strong>prompt distillation</strong>, which aims to shorten lengthy prompts without compromising the performance of the underlying language model on various test tasks. This process results in the generation of concise prompts, either in the form of free text or vectors.</p> <p>In this paper, prompt distillation involves the identification of effective continuous prompts from discrete prompts. These continuous prompts are fine-tuned for each task but remain consistent across samples within the same task. On the other hand, discrete prompts are specific to each sample and are created by combining user IDs and item IDs.</p> <p>The continuous prompts are appended to the input sequence before the discrete prompts and are treated as trainable components that are part of the model parameters. The entire model is updated using the Negative Log Likelihood loss function, which is well-suited for generation tasks.</p> <p>The following image represents the input sequence for a specific task during the training time.</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/pod_finetuning_architecture.png" alt="The Architecture" width="700" height="500"></p> <p>As it is illustrated, <em>P1</em> is the task-specific continuous prompt. After the training phase, when the continuous prompt is tuned, the entire prompt “Generate an explanation for …. about item ….” is substituted with the tuned continuous prompts—as is depicted in the following figure:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/pod_inference.png" alt="The Architecture" width="450" height="300"></p> <p>So, generally, the input sequence format in training and inference time can be represented as:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/pod_training_inference.png" alt="The Architecture" width="350" height="400"></p> <h3 id="experiments">Experiments</h3> <p>The experimental setup involved the use of three popular datasets sourced from Amazon, namely Sports &amp; Outdoors, Beauty, and Toys &amp; Games, each containing specific user IDs, item IDs, ratings, textual reviews, and timestamps. The datasets were divided into training, validation, and testing sets.</p> <p>For the sequential recommendation task, the experiment employed a range of baselines such as CASER, HGN, GRU4Rec, BERT4Rec, FDSA, SASRec, and S3-Rec. Additionally, for the top-N recommendation task, baselines included MF, MLP, and P5. Lastly, for the explanation generation task, models like Att2Seq, NRT, and PETER were used for comparison.</p> <p>The evaluation metrics utilized were Hit Ratio (HR), Normalized Discounted Cumulative Gain (NDCG) for sequential and top-N recommendation, and BLEU and ROUGE for generated explanations. The study was conducted using the T5-small model as the backbone, employing the AdamW optimizer, with specific hyperparameters and checkpoints set for the training and evaluation phases. Beam search with specific parameters was used for inference.</p> <h3 id="conclusion">Conclusion</h3> <p>In conclusion, the paper introduces a novel framework for multi-task recommendation systems, demonstrating the efficacy of prompt distillation in training a single model for various recommendation tasks. By leveraging task-specific and sample-specific prompts, the proposed method achieves improved recommendation accuracy and generates coherent explanations, as evidenced by extensive experiments on diverse Amazon datasets. The study’s findings highlight the potential of leveraging text generation approaches within the recommendation domain and pave the way for further advancements in unified recommendation systems.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <p class="mb-2">Here are some more articles relevant to this one:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-27/">Paper Review - Week 27</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-25/">Paper Review - Week 25</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-35/">Paper Review - Week 35</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-23/">Paper Review - Week 23</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-36/">Paper Review - Week 36</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Shirin Tahmasebi. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>