<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://shirintahmasebi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shirintahmasebi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-19T06:42:16+00:00</updated><id>https://shirintahmasebi.github.io/feed.xml</id><title type="html">Shirin Tahmasebi</title><subtitle>Personal Webpage </subtitle><entry><title type="html">Paper Review - Week 22</title><link href="https://shirintahmasebi.github.io/blog/2024/week-22/" rel="alternate" type="text/html" title="Paper Review - Week 22"/><published>2024-06-02T00:00:00+00:00</published><updated>2024-06-02T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2024/week-22</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2024/week-22/"><![CDATA[<p>Here is the list of the most interesting papers published in this week:</p> <ul> <li><a href="/blog/2024/week-22/#reflexion-language-agents-with-verbal-reinforcement-learning">Reflexion: Language Agents with Verbal Reinforcement Learning</a></li> </ul> <hr/> <h2 id="reflexion-language-agents-with-verbal-reinforcement-learning"><a href="https://dl.acm.org/doi/10.5555/3666122.3666499">Reflexion: Language Agents with Verbal Reinforcement Learning</a></h2> <h3 id="introduction">Introduction</h3> <p>The Reflexion framework presents a novel approach to enhancing the performance of Large Language Models (LLMs) by addressing the limitations of traditional Reinforcement Learning (RL) methods. Traditional RL typically involves computationally intensive weight updates and relies on scalar rewards that often lack the nuance needed for effective learning. Reflexion diverges from this paradigm by reinforcing LLMs through verbal feedback instead of weight updates. This feedback, stored in <strong>an episodic memory buffer</strong>, consists of reflective summaries that provide detailed insights and suggestions for improvement. By leveraging the LLM’s natural language understanding capabilities, Reflexion enables the model to learn iteratively from past experiences, thus enhancing its decision-making, reasoning, and coding capabilities without the need for extensive computational resources.</p> <h3 id="research-gap">Research Gap</h3> <p>The Reflexion framework addresses the limitations of traditional RL methods, which involve computationally intensive weight updates and often lack nuanced feedback. Traditional RL typically relies on scalar rewards and extensive model fine-tuning, which can be inefficient and time-consuming. Reflexion introduces a novel approach by reinforcing LLMs through verbal feedback rather than weight updates. This feedback, stored in an episodic memory buffer, consists of reflective summaries that provide detailed insights and suggestions for improvement. This method leverages the LLM’s natural language understanding capabilities, enabling it to learn from past experiences without the need for expensive computational resources.</p> <p>Compared to existing methods, Reflexion stands out by incorporating memory-based learning and flexible feedback mechanisms, allowing it to be applied across diverse tasks such as sequential decision-making, coding, and language reasoning. Reflexion achieves state-of-the-art performance on several benchmarks, demonstrating significant improvements over baseline agents. By mimicking human reflective learning processes and leveraging the LLM’s capabilities, Reflexion represents a significant advancement in LLM training and reinforcement learning.</p> <h3 id="solution-reflexion">Solution: Reflexion</h3> <p>While Reflexion diverges from traditional RL in how it updates the agent (LLM), it still employs RL concepts but in a unique way. Here’s how RL concepts map onto the Reflexion framework:</p> <ol> <li><strong>Agent</strong>: In traditional RL, the agent is typically a neural network that learns a policy by adjusting its weights based on rewards. In Reflexion, the agent is a LLM that generates actions (text) based on a fixed policy.</li> <li><strong>Environment</strong>: In traditional RL, the environment provides states, receives actions from the agent, and returns rewards and new states. In Reflexion, the environment interacts with the LLM, providing feedback (rewards) based on the LLM’s actions.</li> <li><strong>Policy</strong>: In traditional RL, the policy determines the agent’s actions and is updated based on rewards to improve performance. In Reflexion, the policy is implicitly defined by the LLM’s capabilities and its use of contextual information from memory.</li> <li><strong>Reward</strong>: In traditional RL, rewards are used to update the agent’s policy through mechanisms like gradient descent. In Reflexion, rewards are used to generate verbal feedback, which is stored in memory and used to guide future actions.</li> <li><strong>Learning Process</strong>: In traditional RL, learning involves updating the weights of the agent’s neural network. In Reflexion, learning involves updating the agent’s memory with reflective summaries, which influences future decisions.</li> <li><strong>Exploration vs. Exploitation</strong>: In traditional RL, agents explore the environment to find better policies and exploit known good actions. In Reflexion, the LLM explores different actions based on feedback stored in memory and exploits successful patterns discovered through reflection.</li> </ol> <p>In Reflexion, the LLM (Actor) generates text or actions based on the current state and context from memory, aligning with the policy in RL. The Evaluator \(M_e\) evaluates the output of the Actor and provides a reward score, corresponding to the reward function in RL. The Self-Reflection model (\(M_{sr}\)) generates reflective feedback based on the reward and trajectory, which is stored in memory. This acts as an indirect way of updating the policy by changing the context the LLM uses to make decisions.</p> <h3 id="reflexion-process">Reflexion Process:</h3> <ol> <li><strong>Generation of Initial Trajectory</strong>: The LLM (Actor) interacts with the environment to generate an initial trajectory \(\tau\).</li> <li><strong>Evaluation</strong>: The Evaluator assesses the trajectory and produces a reward \(r\).</li> <li><strong>Self-Reflection</strong>: The Self-Reflection model uses the trajectory \(\tau\) and reward \(r\) to generate a verbal summary \(sr\). This summary captures the agent’s reflections on its performance, highlighting what went wrong and suggesting improvements.</li> <li><strong>Updating Memory</strong>: The reflective summary \(sr\) is stored in the agent’s memory. In subsequent trials, this memory provides context and guidance to the LLM, helping it to avoid past mistakes and make better decisions.</li> <li><strong>Iterative Improvement</strong>: The process of generating trajectories, evaluating them, creating self-reflections, and updating memory continues iteratively. The agent leverages the accumulated memory to improve its performance over time.</li> </ol> <p>The whole process along with the psudo-code of the alogrithm is as follows:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/reflexion_solution.png" alt="Solution" width="800" height="400"/></p> <h3 id="advantages">Advantages:</h3> <ul> <li><strong>Lightweight</strong>: The approach doesn’t require computationally expensive weight updates and fine-tuning.</li> <li><strong>Nuanced Feedback</strong>: Verbal feedback can be more detailed and specific compared to scalar rewards.</li> <li><strong>Explicit Memory</strong>: The agent has an interpretable memory of past experiences that can guide future actions.</li> <li><strong>Scalability</strong>: As LLM capabilities improve, this method can become even more effective.</li> </ul> <p>In a programming task, the LLM might generate a piece of code. The Evaluator checks the code against test cases and provides a reward based on correctness. Instead of updating the LLM’s weights, the Self-Reflection model generates feedback like, “The function failed because it didn’t handle edge cases correctly. Consider adding checks for empty input.” This feedback is stored in memory and used in the next coding attempt.</p> <p>The Reflexion framework cleverly leverages the existing capabilities of LLMs and augments them with a memory system driven by verbal feedback, enabling continuous improvement without the need for traditional weight updates in RL.</p> <h3 id="experiments">Experiments</h3> <p>The primary aspects of the experimental setups are as follows:</p> <h4 id="sequential-decision-making-alfworld">Sequential Decision-Making (ALFWorld)</h4> <p><strong>Task</strong>: The task involves solving multi-step problems in text-based environments, such as finding and manipulating objects.</p> <p><strong>Dataset</strong>: ALFWorld, a suite of text-based environments derived from TextWorld, encompassing 134 different tasks related to finding, moving, and manipulating objects.</p> <p><strong>Baselines</strong>:</p> <ul> <li><strong>ReAct</strong>: An action generation model that uses intermediate thoughts to solve tasks.</li> </ul> <p><strong>Evaluation Metrics</strong>:</p> <ul> <li><strong>Success Rate</strong>: The proportion of tasks completed successfully.</li> <li><strong>Error Classification</strong>: Identifying types of errors such as hallucinations or inefficient planning.</li> </ul> <p><strong>Results</strong>:</p> <ul> <li><strong>Performance</strong>: Reflexion agents significantly outperformed the ReAct baseline, completing 130 out of 134 tasks.</li> <li><strong>Error Reduction</strong>: The use of reflective feedback reduced error rates due to hallucinations and inefficient planning.</li> </ul> <h4 id="reasoning-hotpotqa">Reasoning (HotpotQA)</h4> <p><strong>Task</strong>: The task tests the model’s ability to answer complex questions by reasoning over multiple supporting documents.</p> <p><strong>Dataset</strong>: HotpotQA, a large dataset with 113k question-answer pairs requiring reasoning over multiple Wikipedia articles.</p> <p><strong>Baselines</strong>:</p> <ul> <li><strong>Chain-of-Thought (CoT)</strong>: A prompting method generating intermediate reasoning steps to arrive at a final answer.</li> <li><strong>ReAct + CoT</strong>: Combines reasoning and action generation for more complex tasks.</li> </ul> <p><strong>Evaluation Metrics</strong>:</p> <ul> <li><strong>Exact Match (EM)</strong>: The percentage of answers that match the ground truth exactly.</li> <li><strong>Improvement over Trials</strong>: Measuring how the success rate changes over multiple trials.</li> </ul> <p><strong>Results</strong>:</p> <ul> <li><strong>Performance</strong>: Reflexion improved performance by 20% on HotpotQA reasoning tasks compared to baseline approaches.</li> <li><strong>Iterative Improvement</strong>: Reflexion agents demonstrated the ability to iteratively improve accuracy through self-reflection.</li> </ul> <h4 id="programming">Programming</h4> <p><strong>Task</strong>: The task involves writing correct code based on problem descriptions, including generating and debugging code in Python and Rust.</p> <p><strong>Datasets</strong>:</p> <ul> <li><strong>HumanEval</strong>: A benchmark for evaluating the correctness of function implementations based on given problem descriptions.</li> <li><strong>MBPP (ManyBabies Programming Problems)</strong>: A set of programming challenges in Python.</li> <li><strong>LeetcodeHardGym</strong>: A newly introduced dataset consisting of 40 hard-rated Leetcode questions in 19 programming languages.</li> </ul> <p><strong>Baselines</strong>:</p> <ul> <li><strong>AlphaCode</strong>: A code generation model evaluating generations on hidden test cases.</li> <li><strong>CodeT</strong>: Uses self-generated unit tests to score function implementations.</li> <li><strong>Self-Debugging</strong>: Employs debugging components to improve existing implementations.</li> <li><strong>CodeRL</strong>: A reinforcement learning framework for code generation and debugging.</li> </ul> <p><strong>Evaluation Metrics</strong>:</p> <ul> <li><strong>Pass@1</strong>: The accuracy of the first code generation sample passing all test cases.</li> <li><strong>False Positive Rate</strong>: The proportion of incorrect solutions that pass self-generated unit tests.</li> <li><strong>Accuracy</strong>: The overall correctness of the code implementations.</li> </ul> <p><strong>Results</strong>:</p> <ul> <li><strong>HumanEval</strong>: Reflexion achieved a Pass@1 accuracy of 91% in Python, surpassing the previous state-of-the-art GPT-4 at 80%.</li> <li><strong>MBPP</strong>: Reflexion maintained high performance in Python and Rust, demonstrating its language-agnostic capabilities.</li> <li><strong>LeetcodeHardGym</strong>: Reflexion outperformed GPT-4 by achieving a higher pass rate on challenging Leetcode problems.</li> </ul> <p>In summary, the Reflexion framework demonstrated significant improvements across various tasks by leveraging reflective feedback stored in memory, highlighting its effectiveness and potential for further enhancement as LLM capabilities evolve.</p> <p>An example scenario for three of the main tasks is as follows:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/reflexion_example.png" alt="Solution" width="1000" height="400"/></p> <h3 id="conclusion">Conclusion</h3> <p>The Reflexion framework significantly advances the field of LLM training by introducing memory-based learning and verbal reinforcement, which allows models to improve iteratively without the need for computationally expensive fine-tuning. Through the use of reflective feedback stored in memory, Reflexion achieves state-of-the-art performance across various tasks, including sequential decision-making, coding, and language reasoning. It surpasses existing methods, such as GPT-4, by demonstrating substantial improvements in benchmarks like the HumanEval coding test. By mimicking human reflective learning processes and leveraging the inherent strengths of LLMs, Reflexion provides a flexible and efficient alternative to traditional RL methods, representing a significant leap forward in the development of intelligent language agents.</p>]]></content><author><name></name></author><category term="papers"/><category term="weekly-review"/><category term="nlp"/><summary type="html"><![CDATA[Top NLP Papers Published from May 27 to June 02]]></summary></entry><entry><title type="html">Paper Review - Week 12</title><link href="https://shirintahmasebi.github.io/blog/2024/week-12/" rel="alternate" type="text/html" title="Paper Review - Week 12"/><published>2024-03-24T00:00:00+00:00</published><updated>2024-03-24T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2024/week-12</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2024/week-12/"><![CDATA[<p>Several interesting papers are published in this week in NLP. Here is a list of them:</p> <ul> <li><a href="/blog/2024/week-12/#detoxifying-large-language-models-via-knowledge-editing">Detoxifying Large Language Models via Knowledge Editing</a></li> <li><a href="/blog/2024/week-12/#llm2llm-boosting-llms-with-novel-iterative-data-enhancement">LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement</a></li> </ul> <hr/> <h2 id="detoxifying-large-language-models-via-knowledge-editing"><a href="https://arxiv.org/pdf/2403.14472.pdf">Detoxifying Large Language Models via Knowledge Editing</a></h2> <h3 id="introduction">Introduction</h3> <p>This paper addresses the critical need for robust methodologies to detoxify LLMs, ensuring their outputs remain safe and constructive across diverse applications. They introduce the concept of knowledge editing as a targeted approach to mitigate the generation of unsafe content, with a focus on a novel method named Detoxifying with Intraoperative Neural Monitoring (DINM). By analyzing and adjusting the internal knowledge of LLMs, DINM aims to reduce toxicity at its source without compromising the models’ overall performance. They also developed SafeEdit, a comprehensive benchmark designed to evaluate the effectiveness of detoxification techniques across a spectrum of unsafe scenarios.</p> <h3 id="research-gap">Research Gap</h3> <p>The research gap addressed in the paper is the challenge of detoxifying LLMs to prevent them from generating toxic (unsafe) responses to user inputs. The focus is on modifying the LLM itself so that it inherently avoids producing toxic responses while maintaining its ability to answer safe questions effectively. While existing methods (such as Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and Direct Preference Optimization (DPO)) have made some progress in making LLMs safer, they often fail to adequately handle carefully crafted adversarial inputs that can elicit harmful content. This vulnerability poses a significant risk, especially as LLMs become more integrated into everyday applications. The paper identifies a critical need for more effective and efficient methods that can permanently adjust the internal knowledge and parameters of LLMs to reduce toxicity across different inputs.</p> <p>The paper introduces two key contributions: the <strong>SafeEdit</strong> benchmark and the <strong>Detoxifying with Intraoperative Neural Monitoring (DINM)</strong> method. SafeEdit provides a comprehensive framework for evaluating the effectiveness of different detoxification approaches across nine unsafe categories, considering both the ability to mitigate toxic responses and the impact on general model performance. The DINM method represents a novel approach to LLM detoxification, leveraging knowledge editing techniques to make targeted adjustments to the model’s parameters, specifically within identified toxic regions. This method not only demonstrates a high degree of effectiveness in reducing toxicity with minimal impact on the LLM’s overall capabilities but also offers significant improvements in efficiency, requiring fewer resources and steps compared to traditional methods. Generally, this paper offers a promising approach for enhancing the safety and reliability of LLMs, marking a significant step forward in the topic of responsible AI.</p> <h3 id="solution">Solution</h3> <p>Their solution is divided into two parts: the <strong>SafeEdit</strong> benchmark and the <strong>Detoxifying with Intraoperative Neural Monitoring (DINM)</strong> method.</p> <h4 id="safeedit">SafeEdit</h4> <p>The SafeEdit framework is designed for evaluating the effectiveness of different detoxification approaches on LLMs. The process of creating such framework is:</p> <ol> <li> <p><strong>Categories of Unsafety</strong>: They defined nine distinct categories of unsafe content (e.g., bias, ethics, political, etc) to ensure a comprehensive coverage of the types of harmful content an LLM might generate. This categorization helps in systematically addressing the wide spectrum of toxicity.</p> </li> <li> <p><strong>Generating Unsafe Questions</strong>: For each of the nine unsafety categories, the LLM (GPT-4, in this case) is tasked to generate 60 unique questions that violate OpenAI’s usage policy. This step is crucial as it leverages the model’s capability to generate diverse instances of potential real-world unsafe queries.</p> </li> <li> <p><strong>Collection of Adversarial Prompts</strong>: The authors collected 48 attack prompts from various sources, including recent papers and handwritten sources. These prompts are designed to trick the LLM into generating responses that might reveal security vulnerabilities or produce harmful content.</p> </li> <li> <p><strong>Reevaluation by Humans</strong>: Human evaluation plays a critical role in ensuring the quality and effectiveness of both the harmful questions and the adversarial prompts. This step ensures that the dataset accurately represents a wide range of potential security threats.</p> </li> <li> <p><strong>Response Generation</strong>: The process of generating responses involves two rounds. In the first round, the LLM is instructed to respond to the harmful question by refusing to fulfill the request (<em>safe</em> response). In the second round, the goal is to elicit an <em>unsafe</em> response from the LLM, without the prior instruction to avoid harm.</p> </li> <li> <p><strong>Dataset Construction</strong>: The concatenation of harmful questions and attack prompts, along with their corresponding safe and unsafe responses, constitutes the dataset referred to as \(D_{edit}\). This dataset forms the basis for evaluating the detoxification effectiveness of various approaches.</p> </li> <li> <p><strong>General Performance Evaluation (D_cons)</strong>: In addition to \(D_{edit}\), another dataset (\(D_{cons}\)) consisting of instruction-following instances is used to assess whether the detoxification process affects the LLM’s performance on general tasks. This is critical for ensuring that the process of detoxifying the model does not degrade its utility for safe requests.</p> </li> <li> <p><strong>Evaluation Goal</strong>: The ultimate objective is to modify the LLM in such a way that it generates safe responses to the potentially harmful queries in \(D_{edit}\), without compromising its ability to respond effectively and accurately to the benign queries in \(D_{cons}\).</p> </li> </ol> <p>This framework aims to provide a comprehensive and systematic means of assessing and improving the safety of LLMs in handling potentially harmful or unsafe queries. The following image represents an overview of the SafeEdit construction process:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/dtoxifying_llm_safeedit.png" alt="The SafeEdit Overview" width="850" height="250"/></p> <h4 id="dinm">DINM</h4> <p>The DINM method is as follows:</p> <ol> <li> <p><strong>Identification of Toxic Region</strong>: DINM starts by receiving a safe response and an unsafe response corresponding to a potentially toxic question from \(D_{edit}\) and then compares the hidden states of the LLM in the corresponding layer. Through this comparison, DINM identifies the layer within the model that shows the greatest difference in hidden states between safe and unsafe responses. This layer is named the “toxic region,” and it’s worth mentioning that this identification is instance-specific, meaning it can vary from one instance (input) to another.</p> </li> <li> <p><strong>Tunable and Frozen Parameters</strong>: The model’s parameters are conceptually divided into two groups for the tuning process: the parameters within the identified toxic layer (\(W_{l_{toxic}}^{V}\)) which are subject to tuning, and the rest of the model’s parameters, which are kept frozen. This distinction ensures that the tuning process specifically targets reducing toxicity without broadly altering the model’s overall knowledge or capabilities.</p> </li> <li><strong>Loss Function for Tuning</strong>: The loss function used to update the tunable parameters in the toxic layer is indeed a weighted sum of two components: <ul> <li>The <strong>negative log likelihood</strong> of generating the safe response given the toxic question. This part of the loss encourages the model to favor safe responses over unsafe ones.</li> <li>The <strong>KL divergence</strong> between the probability distributions generated by both the detoxified LLM (the one being tuned) and the regular LLM (the undetoxified one) for a non-toxic question from \(D_{cons}\). This part ensures that the detoxification process does not compromise the model’s ability to handle general, non-toxic content.</li> </ul> </li> <li><strong>Objective</strong>: The overall objective of the tuning process is to adjust the toxic layer’s parameters so that the model becomes less likely to generate unsafe responses, while maintaining its performance on unrelated tasks. The dual-component loss function helps balance these goals by pushing the model towards safer outputs without losing its general utility.</li> </ol> <p>In summary, DINM offers a targeted approach to detoxify LLMs by identifying and tuning instance-specific toxic regions within the model, using a loss function that balances the reduction of toxic outputs with the preservation of general performance. The following image represents an overview of the DINM method:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/dtoxifying_llm_dinm.png" alt="The DINM Method" width="400" height="250"/></p> <h3 id="experiments">Experiments</h3> <p>The main goal of the experiments is to assess how effectively the DINM method can detoxify LLMs, reducing their propensity to generate unsafe responses, while maintaining or minimizing impact on their general performance.</p> <ul> <li><strong>Benchmark</strong>: The SafeEdit benchmark, which encompasses various unsafe scenarios categorized into nine types, is used to systematically evaluate the performance of different detoxification approaches.</li> <li><strong>Metrics</strong>: The evaluation considers several metrics, including detoxification success rate, defense generalization across different unsafe inputs, and the impact on general performance (e.g., fluency, knowledge question answering, content summarization).</li> <li><strong>Baselines</strong>: Several baseline methods are compared against DINM to benchmark its performance in detoxifying LLMs: <strong>SFT</strong>, <strong>RLHF</strong>, <strong>DPO</strong>, <strong>MEND</strong>, and <strong>Ext-Sub</strong>.</li> </ul> <p>The experiments demonstrate that DINM is particularly effective at detoxifying LLMs. It shows a significant improvement in reducing toxic outputs across various unsafe scenarios without severely impacting the model’s overall capabilities. Compared to traditional approaches like SFT, RLHF, and DPO, DINM offers a more targeted and efficient way to reduce toxicity. It achieves better results in detoxification success rates and defense generalization, indicating its superior ability to generalize across different types of unsafe content. While detoxification methods can potentially degrade a model’s performance on general tasks, DINM manages to maintain a balance, ensuring that the model’s ability to perform unrelated tasks remains largely unaffected.</p> <h3 id="conclusion">Conclusion</h3> <p>This paper has demonstrated the potential of knowledge editing, particularly through the Detoxifying with Intraoperative Neural Monitoring (DINM) method, as a promising avenue for reducing the toxicity of LLMs. By systematically identifying and adjusting the toxic regions within LLMs, DINM offers a promising approach to detoxification that balances safety with the maintaining the general performance of the model.</p> <h2 id="llm2llm-boosting-llms-with-novel-iterative-data-enhancement"><a href="https://arxiv.org/pdf/2403.15042.pdf">LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement</a></h2> <h3 id="introduction-1">Introduction</h3> <p>The core issue addressed in this paper is the challenge of improving reasoning abilities of Large Language Models (LLMs) with limited training data. Under such data scarcity, fine-tuning, while effective for adapting models to specific tasks, may not adequately address the challenge of increasing LLM reasoning capabilities. The LLM2LLM approach introduces a novel solution that focuses on generating targeted, challenging data points to directly improve reasoning abilities.</p> <h3 id="research-gap-1">Research Gap</h3> <p>The paper targets the challenge of enhancing reasoning capabilities of LLMs in situations where available training data is scarce. Distinguishing from other data augmentation techniques, this research focuses on selectively augmenting only those data points which the LLM initially fails to predict correctly, rather than all data points. This method aims at efficient improvement, concentrating efforts on revising only the model’s weaknesses rather than expanding the whole training dataset.</p> <h3 id="solution-1">Solution</h3> <h4 id="llm2llm">LLM2LLM</h4> <p>The solution proposed in this paper is called LLM2LLM, the steps of which are as follows:</p> <ul> <li>First, an LLM with a good enough reasoning ability is chosen as the teacher model to follow the instructions for data augmentation. It does not necessarily need to be bigger than the student model.</li> <li>Second, the seed dataset is fed as input to the student model. The performance of the model is measured and the data points for which the prediction of the model was incorrect are filtered.</li> <li>Third, these filtered data points are fed to the teacher model for being augmented. The augmented data should be directly produced from the seed data points. In other words, they never use the augmented data points to generate another set of augmented data.</li> <li>The two datasets (seed and augmented) are passed to the student model for another iteration of evaluation.</li> <li>This process can be repeated \(n\) times, depending on the target level of accuracy.</li> </ul> <p>They have used LLaMa2 for the student model and GPT3.5, GPT4-Turbo, and LLaMa2 for the teacher model. An overview of LLM2LLM is depicted in the following figure:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/llm2llm.png" alt="The LLM2LLM Method" width="800" height="400"/></p> <h4 id="knowledge-distillation-and-data-augmentation">Knowledge Distillation and Data Augmentation</h4> <p>Knowledge distillation and data augmentation both use teacher and student models but serve different purposes. In knowledge distillation, the teacher model’s knowledge is transferred to the smaller student model, aiming to retain performance while reducing model size. However, data augmentation focuses on enriching the training data—often through the teacher model generating new examples—to improve the student model’s learning and generalization. The key difference lies in their objectives: distillation seeks to compress and transfer knowledge, while augmentation aims to expand the training dataset for better model robustness.</p> <p><strong>Comment:</strong> The paper (the last paragraph on the first page) mentions fine-tuning LLMs as a solution for handling very long contexts. However, this perspective may overlook the inherent limitations of fine-tuning in expanding the model’s capacity to directly manage longer sequences. While fine-tuning optimizes a model’s performance on specific tasks, it does not directly increase the model’s ability to process or comprehend longer contexts.</p> <h3 id="experiments-1">Experiments</h3> <p>The primary aspects of the experimental setups are as follows:</p> <ul> <li><strong>Tasks:</strong> Mathematical reasoning, question answering, sentiment analysis, and text comprehension</li> <li><strong>Dataset:</strong> GSM8K, CaseHOLD, SNIPS, TREC, SST-2</li> <li><strong>Baselines:</strong> Fine-tunned LLaMa2, AugGPT, EDA</li> <li><strong>Evaluation Metrics:</strong> Accuracy (%)</li> </ul> <h3 id="conclusion-1">Conclusion</h3> <p>This paper proposes a method to enhance the performance of LLMs in scenarios with limited training data. It introduces an iterative data augmentation strategy using a “teacher” LLM to generate synthetic data based on incorrect predictions by a “student” LLM. This process aims to improve the student model’s performance by only focusing on challenging examples. The results show significant improvements in various datasets, highlighting the method’s potential to reduce the need for extensive data augmentation and improve LLMs’ effectiveness in data-constrained tasks.</p>]]></content><author><name></name></author><category term="papers"/><category term="weekly-review"/><category term="nlp"/><summary type="html"><![CDATA[Top NLP Papers Published from March 18 to March 24]]></summary></entry><entry><title type="html">Paper Review - Week 8</title><link href="https://shirintahmasebi.github.io/blog/2024/week-8/" rel="alternate" type="text/html" title="Paper Review - Week 8"/><published>2024-02-25T00:00:00+00:00</published><updated>2024-02-25T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2024/week-8</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2024/week-8/"><![CDATA[<p>Several interesting papers are published in this week in NLP. Here is a list of them:</p> <ul> <li><a href="/blog/2024/week-8/#critic-large-language-models-can-self-correct-with-tool-interactive-critiquing">CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</a></li> </ul> <hr/> <h2 id="critic-large-language-models-can-self-correct-with-tool-interactive-critiquing"><a href="https://arxiv.org/pdf/2305.11738">CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</a></h2> <h3 id="introduction">Introduction</h3> <p>Large language models (LLMs) have shown remarkable progress across various natural language processing tasks, but they still face significant challenges such as generating inaccurate information, producing faulty code, and occasionally creating harmful or toxic content. Traditional methods to address these issues often involve extensive retraining or reliance on large-scale annotated datasets, which can be both resource-intensive and task-specific. This paper introduces the CRITIC framework, a novel approach that allows LLMs to iteratively improve their outputs by integrating feedback from external tools. By leveraging resources such as search engines, code interpreters, and toxicity detectors, CRITIC enables LLMs to refine their responses in a manner similar to how humans use external references to validate and correct their work. This method enhances the model’s reliability and versatility across a wide range of tasks without the need for additional training or annotations.</p> <h3 id="research-gap">Research Gap</h3> <p>Existing methods for improving the performance of LLMs often rely on resource-intensive techniques like fine-tuning or RLHF, which may not generalize well across different tasks. The primary gap this work addresses is the need for a versatile, task-agnostic framework that enables LLMs to iteratively improve their outputs without extensive retraining. Unlike traditional approaches, the CRITIC framework introduces a novel method that incorporates external tools, such as search engines and code interpreters, to provide actionable feedback, which is then used to refine the model’s output in subsequent iterations. This approach significantly enhances the reliability and accuracy of LLMs across diverse tasks without the need for task-specific adjustments.</p> <p><strong>Difference Between CRITIC and Self-Refine</strong> While both CRITIC and Self-Refine aim to iteratively improve LLM outputs, they differ fundamentally in their approach. Self-Refine relies solely on the model’s internal feedback mechanisms to identify and correct errors, which can be limiting if the model’s initial reasoning is flawed. In contrast, CRITIC enhances this process by integrating external feedback from tools like search engines and code interpreters, providing more robust and reliable corrections. This external validation makes CRITIC more effective across a wider range of tasks, ensuring that the LLM’s improvements are grounded in accurate, externally verified information.</p> <h3 id="solution-critic">Solution: CRITIC</h3> <p><strong>CRITIC</strong> (Correcting with Tool-Interactive Critiquing) is a framework designed to enable LLMs to iteratively improve their outputs by interacting with external tools. The method is based on the idea that LLMs, which often operate as “black boxes,” can enhance their output quality by receiving and incorporating feedback from external sources, similar to how humans refine their work.</p> <h4 id="key-steps-in-the-critic-method">Key Steps in the CRITIC Method:</h4> <ol> <li><strong>Initial Output Generation</strong>: <ul> <li>The LLM first generates an initial output based on an input prompt using its internal parametric knowledge.</li> <li>This output is produced in a typical LLM fashion, without any external interaction at this stage.</li> </ul> </li> <li><strong>Verification with External Tools</strong>: <ul> <li>After generating the initial output, the LLM interacts with one or more external tools to evaluate specific aspects of the output. These tools could include: <ul> <li><strong>Search Engines</strong> (e.g., Google) for fact-checking.</li> <li><strong>Code Interpreters</strong> for verifying the correctness of generated code.</li> <li><strong>Calculators</strong> for checking numerical accuracy.</li> <li><strong>Toxicity Detectors</strong> (e.g., PERSPECTIVE API) for assessing the safety and appropriateness of the content.</li> </ul> </li> <li>The tools return feedback, often in the form of critiques, which highlight issues like factual inaccuracies, logical errors, or harmful content.</li> </ul> </li> <li><strong>Integrating Critiques</strong>: <ul> <li>The LLM doesn’t merely receive these critiques as isolated pieces of feedback. Instead, these critiques are integrated with the original input to form a new, enriched prompt.</li> <li>This new prompt typically consists of: - The original input or question. - The initial output generated by the LLM. - The critiques or feedback provided by the external tools.</li> </ul> </li> <li><strong>Correcting the Output</strong>: <ul> <li>The LLM is then prompted with this enriched input (original input + critiques) to generate a revised output.</li> <li>This corrected output aims to address the issues identified in the critiques.</li> </ul> </li> <li><strong>Iterative Process</strong>: <ul> <li>The “Verify → Correct” cycle can be repeated multiple times. In each iteration, the LLM generates a new output based on the latest critiques, and this output is again verified using the external tools.</li> <li>The process continues until the feedback indicates that the output meets the desired quality standards (e.g., factual accuracy, correctness, or reduced toxicity).</li> </ul> </li> <li><strong>Stopping Criteria</strong>: <ul> <li>The iterative process can stop when one of the following conditions is met: <ul> <li>The feedback from external tools suggests that the output is satisfactory.</li> <li>The maximum number of iterations is reached.</li> <li>The output remains unchanged across consecutive iterations, indicating convergence.</li> </ul> </li> </ul> </li> </ol> <p>The procedure and the psudo-code are depicted in the following two figures:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/critic_architecture.png" alt="The Architecture" width="700" height="250"/></p> <p style="text-align:center;"><img src="/assets/img/weekly-review/critic_algorithm.png" alt="The Architecture" width="750" height="250"/></p> <h4 id="advantages-of-the-critic-method">Advantages of the CRITIC Method:</h4> <ul> <li><strong>Enhanced Reliability</strong>: By incorporating external feedback, CRITIC addresses the limitations of LLMs in self-correcting their outputs, leading to more reliable results.</li> <li><strong>Versatility</strong>: The method can be applied to various tasks (e.g., question answering, mathematical reasoning, toxicity reduction) without requiring task-specific retraining or extensive human annotations.</li> <li><strong>Practicality</strong>: CRITIC utilizes in-context learning and text-to-text APIs, making it accessible and practical for a wide range of applications.</li> </ul> <h3 id="experiments">Experiments</h3> <p>The CRITIC framework was evaluated across three distinct tasks: free-form question answering, mathematical program synthesis, and toxicity reduction.</p> <h4 id="free-form-question-answering">Free-Form Question Answering</h4> <p><strong>Objective</strong>: To assess the truthfulness and accuracy of LLM-generated answers to open-ended questions.</p> <ul> <li><strong>Datasets</strong>: <ul> <li><strong>AmbigNQ</strong>: An enhanced version of Natural Questions, focusing on ambiguous queries.</li> <li><strong>TriviaQA</strong>: A large-scale question-answering dataset with questions and answers sourced from trivia games.</li> <li><strong>HotpotQA</strong>: A dataset designed for multi-hop reasoning, where multiple pieces of evidence are needed to answer a question.</li> </ul> </li> <li> <p><strong>LLMs</strong>: Text-Davinci-003, ChatGPT, LLaMA-2</p> </li> <li><strong>Tools for Extracting Feedback</strong>: <ul> <li><strong>Google Search API</strong>: Used to fact-check the responses by querying the search engine and extracting relevant snippets from the top-ranked search results.</li> </ul> </li> <li><strong>Baselines</strong>: <ul> <li><strong>Vanilla Few-Shot Prompting</strong>: The LLM directly generates answers based on a few-shot prompt.</li> <li><strong>Chain-of-Thought (CoT) Prompting</strong>: The LLM generates step-by-step reasoning before providing the final answer.</li> <li><strong>Self-Consistency</strong>: Multiple outputs are generated, and the most consistent answer is selected.</li> <li><strong>ReAct</strong>: A retrieval-augmented method where reasoning and retrieved knowledge are combined.</li> </ul> </li> <li><strong>Results</strong>: <ul> <li><strong>CRITIC</strong> significantly outperformed the baselines, showing marked improvements in F1 scores across AmbigNQ, TriviaQA, and HotpotQA datasets.</li> <li>CRITIC achieved substantial gains over the initial CoT results and surpassed the ReAct method by effectively combining intrinsic LLM knowledge with external feedback.</li> <li>The use of external tools was critical, as the model’s own critiques (CRITIC without tools) contributed marginally to the improvement.</li> </ul> </li> </ul> <h4 id="mathematical-program-synthesis">Mathematical Program Synthesis</h4> <p><strong>Objective</strong>: To evaluate the correctness and executability of programs generated by the LLM for solving mathematical problems.</p> <ul> <li><strong>Datasets</strong>: <ul> <li><strong>GSM8k</strong>: A dataset focused on grade-school math word problems.</li> <li><strong>SVAMP</strong>: A dataset for solving varied arithmetic math problems.</li> <li><strong>TabMWP</strong>: A dataset that combines tabular data with word problems, requiring the model to synthesize and compute.</li> </ul> </li> <li> <p><strong>LLMs</strong>: Text-Davinci-003, ChatGPT, LLaMA-2-70B</p> </li> <li><strong>Tools for Extracting Feedback</strong>: <ul> <li><strong>Python Interpreter</strong>: Used to execute the generated programs and provide feedback in the form of error messages or execution results.</li> </ul> </li> <li><strong>Baselines</strong>: <ul> <li><strong>Vanilla Few-Shot Prompting</strong>: Direct program generation based on a few-shot prompt.</li> <li><strong>Program-of-Thought (PoT)</strong>: A method where the LLM writes programs to solve mathematical problems.</li> </ul> </li> <li><strong>Results</strong>: <ul> <li><strong>CRITIC</strong> showed notable improvements over the PoT baseline, particularly in the GSM8k, SVAMP, and TabMWP datasets.</li> <li>The CRITIC framework’s ability to incorporate interpreter feedback led to significant performance gains, especially when paired with larger LLMs.</li> <li>Removing the execution feedback (CRITIC without tools) led to reduced and unstable performance, highlighting the importance of external tool interaction.</li> </ul> </li> </ul> <h4 id="toxicity-reduction">Toxicity Reduction</h4> <p><strong>Objective</strong>: To reduce the generation of toxic content while maintaining fluency and diversity in LLM outputs.</p> <ul> <li><strong>Dataset</strong>: <ul> <li><strong>REALTOXICITYPROMPTS</strong>: A dataset designed to elicit potentially toxic responses from LLMs, used for evaluating the ability to generate non-toxic text.</li> </ul> </li> <li> <p><strong>LLMs</strong>: Text-Davinci-003, ChatGPT</p> </li> <li><strong>Tools for Extracting Feedback</strong>: <ul> <li><strong>PERSPECTIVE API</strong>: A tool for assessing the toxicity levels of generated text, providing scores for overall toxicity and specific toxic attributes (e.g., insult, profanity).</li> </ul> </li> <li><strong>Baselines</strong>: <ul> <li><strong>Learning Methods</strong>: Including PPLM, GeDi, DEXPERT, DAPT, PPO, and Quark, which use various reinforcement learning and controlled text generation techniques.</li> <li><strong>Self-Correct</strong>: A method where LLMs iteratively reduce toxicity using self-feedback.</li> </ul> </li> <li><strong>Results</strong>: <ul> <li><strong>CRITIC</strong> substantially lowered the probability of generating toxic content while preserving the fluency and diversity of the output, outperforming many of the existing methods.</li> <li>The integration of external feedback from the PERSPECTIVE API proved crucial, as CRITIC without external tools showed less effective toxicity reduction.</li> <li>CRITIC’s performance in toxicity reduction was on par with supervised state-of-the-art methods, despite not requiring additional training data.</li> </ul> </li> </ul> <p>An example scenario of the framework is depicted in the following figure:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/critic_example.png" alt="The Architecture" width="700" height="700"/></p> <h3 id="conclusion">Conclusion</h3> <p>The CRITIC framework represents a significant advancement in the iterative improvement of LLM outputs by incorporating external tool-based feedback into the correction process. Unlike traditional self-correction methods that rely solely on the model’s internal mechanisms, CRITIC effectively combines the LLM’s intrinsic capabilities with external validation, resulting in more accurate and reliable outputs across various tasks such as question answering, mathematical reasoning, and toxicity reduction. The experimental results demonstrate that CRITIC outperforms existing methods by providing a practical, task-agnostic solution that does not require extensive retraining or additional data. This work paves the way for more robust and trustworthy AI systems, highlighting the importance of external feedback in the continuous self-improvement of language models.</p>]]></content><author><name></name></author><category term="papers"/><category term="weekly-review"/><category term="nlp"/><summary type="html"><![CDATA[Top NLP Papers Published from February 19 to February 25]]></summary></entry><entry><title type="html">Paper Review - Week 2</title><link href="https://shirintahmasebi.github.io/blog/2024/week-2/" rel="alternate" type="text/html" title="Paper Review - Week 2"/><published>2024-01-14T00:00:00+00:00</published><updated>2024-01-14T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2024/week-2</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2024/week-2/"><![CDATA[<p>Several interesting papers are published in this week in NLP. Here is a list of them:</p> <ul> <li><a href="/blog/2024/week-2/#mixtral-of-experts">Mixtral of Experts</a></li> </ul> <hr/> <h2 id="mixtral-of-experts"><a href="https://arxiv.org/pdf/2401.04088.pdf">Mixtral of Experts</a></h2> <h3 id="introduction">Introduction</h3> <p>In the cutting-edge realm of AI and natural language processing, the Mixtral 8x7B emerges as a groundbreaking development, introducing a new paradigm in transformer-based language models. This innovative model, built on the Sparse Mixture of Experts (SMoE) framework, evolves from the established Mistral 7B model, with a critical advancement in its architecture. Mixtral is distinguished by its unique design, integrating 8 feedforward blocks, or ‘experts’, into each transformer layer. This structure enables a dynamic selection process where each token, at every layer, is processed by two specifically chosen experts, allowing the model to efficiently utilize its immense 47B parameter capacity while actively employing only 13B parameters during inference. By using such transformative approach, Mixtral shows exceptional performance in benchmarks spanning mathematics, code generation, and multilingual tasks. The paper also introduces the Mixtral 8x7B – Instruct, a variant fine-tuned for instruction-following, which notably outperforms leading models in human-centric evaluations.</p> <h3 id="research-gap">Research Gap</h3> <p>The Mixtral 8x7B paper addresses a critical research gap in the field of transformer-based language models, focusing on improving inference efficiency without sacrificing performance. Traditional large-scale models like GPT-3.5 and Llama 2 70B, while powerful, often struggle with the balance between their extensive parameter counts and the need for efficient inference. These models typically require substantial computational resources for processing inputs, making them less efficient in scenarios that demand quick response times or involve large batch sizes.</p> <p>Mixtral 8x7B confronts this challenge by integrating a Sparse Mixture of Experts (SMoE) framework into its transformer architecture. This innovative approach allows the model to selectively utilize its parameters during inference, activating only a fraction of the available <em>experts</em> for each token. As a result, Mixtral dramatically reduces the active parameter count during inference, leading to faster response times and improved throughput. This selective expert activation does not compromise the model’s ability to handle complex tasks, such as mathematics, code generation, and multilingual processing.</p> <h3 id="solution-mixtral">Solution: Mixtral</h3> <p>The Mixtral 8x7B model introduces a transformative approach to transformer-based architecture, addressing the inefficiencies of previous models. As depicted in Figure 1, the traditional transformer employs a standard sequence of operations, starting from input embedding, followed by positional encoding, multi-head attention, and a feed-forward network, with layer normalization steps in between.</p> <figure style="text-align:center;"> <img src="/assets/img/weekly-review/mixtral_transformer_original.png" alt="The Architecture" width="250" height="300"/> <figcaption>Figure 1. Original Transformer Architecture </figcaption> </figure> <p>In contrast, the Mixtral model, shown in Figure 2, redefines this architecture by integrating a routing mechanism within the transformer layers. Each layer in Mixtral consists of 8 FFN, termed <em>experts</em>. Unlike the uniform processing in traditional transformers, Mixtral employs a router network that selects two of these experts at each layer to process the input token. This selective processing is a key differentiator, enabling Mixtral to leverage a large number of parameters while maintaining computational efficiency during inference.</p> <figure style="text-align:center;"> <img src="/assets/img/weekly-review/mixtral_transformer_moe.png" alt="The Architecture" width="800" height="600"/> <figcaption>Figure 2. Mixtral Architecture</figcaption> </figure> <h4 id="router-functionality">Router Functionality</h4> <p>The router is the cornerstone of Mixtral’s efficiency, functioning as a dynamic selector of experts. At each layer, for every input token, the router chooses two experts to process the token. This is illustrated in Figure 2 where the router sits atop the layer norm and directs inputs to the chosen experts. The selection process is governed by a gating network, represented by the following equation:</p> \[\sum_{i=0}^{n-1} G(x)_i \cdot E_i(x)\] <p>Here, \(G(x)_i\) is a scalar representing the output of the gating network for the i-th expert, while \(E_i(x)\) is the output of the i-th expert. The gating vector \(G(x)\) is designed to be sparse, meaning most of its elements are zero, and computation is only performed for the non-zero elements, corresponding to the top \(K\) experts with the highest gating values. This sparsity ensures that only the most relevant experts for a given token are engaged.</p> <p>The gating mechanism is mathematically formalized as:</p> \[G(x) := \text{Softmax}(\text{TopK}(x \cdot W_g))\] <p>where \(W_g\) is a weight matrix and TopK selects the top \(K\) logits, setting all others to negative infinity, effectively zeroing them out. In Mixtral, \(K\) is set to 2, meaning that for each token, only two experts are actively contributing to the model’s output, hence the designation <strong>8x7B</strong> – eight experts per layer in a 7 billion parameter model.</p> <p>Through this method, Mixtral 8x7B achieves a balance between the expansive capacity of a large parameter set and the practical constraints of inference time and computational resource usage.</p> <h4 id="integration-of-experts">Integration of Experts</h4> <p>Within the innovative framework of Mixtral 8x7B, as visualized in Figure 2, the router selects two experts for each token. Once these experts have processed the token independently, their outputs are not left in isolation. Instead, the outputs are combined through an additive process to form the final output for the token. This is mathematically represented as a weighted sum, where the output \(y\) for an input token \(x\) is calculated using the equation:</p> \[y = \sum_{i=0}^{n-1} \text{Softmax}(\text{Top2}(x \cdot W_g))_i \cdot SwiGLU_i(x)\] <p>Each term in the sum corresponds to one of the two selected experts, with the softmax function ensuring that the gating network’s outputs \(G(x)\) act as weights, conferring the appropriate influence to each expert’s contribution.</p> <p><strong>Note 1:</strong> No details are provided regarding the training dataset.</p> <p><strong>Note 2:</strong> Another distinction between Mixtral 8x7B and the conventional transformer lies in the substitution of the standard FFN with the SwiGLU activation function within its expert layers.</p> <h3 id="experiments">Experiments</h3> <p>The primary aspects of the experimental setups are as follows:</p> <ul> <li><strong>Tasks:</strong> Commonsense Reasoning, World Knowledge, Reading Comprehension, Mathematical Problem-solving, and Code Generation</li> <li><strong>Dataset:</strong> For commonsense reasoning, datasets such as Hellaswag, Winogrande, and CommonsenseQA were used. World knowledge was tested using the NaturalQuestions and TriviaQA datasets. Reading comprehension abilities were gauged via BoolQ and QuAC. For mathematics and code generation, the GSM8K and Humaneval datasets were instrumental, respectively.</li> <li><strong>Baselines:</strong> Llama2 70B and GPT-3.5</li> </ul> <p>One interesting finding from the experiments is the apparent randomness in the selection of experts by the router for processing tokens, with no noticeable pattern in expert choice across different inputs. However, their analysis reveals a tendency for consecutive tokens to be processed by the same expert, suggesting a behavior of localized expertise within the model’s architecture.</p> <h3 id="conclusion">Conclusion</h3> <p>In conclusion, the Mixtral 8x7B model leverages the potential of integrating Sparse Mixture of Experts (SMoE) within a transformer architecture to enhance efficiency and effectiveness in language processing tasks. Through its innovative router mechanism and SwiGLU activation functions, Mixtral demonstrates superior performance across a range of benchmarks.</p>]]></content><author><name></name></author><category term="papers"/><category term="weekly-review"/><category term="nlp"/><summary type="html"><![CDATA[Top NLP Papers Published from January 08 to January 14]]></summary></entry><entry><title type="html">Paper Review - Week 51</title><link href="https://shirintahmasebi.github.io/blog/2023/week-51/" rel="alternate" type="text/html" title="Paper Review - Week 51"/><published>2023-12-24T00:00:00+00:00</published><updated>2023-12-24T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2023/week-51</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2023/week-51/"><![CDATA[<p>Here is the list of the most interesting papers published in this week:</p> <ul> <li><a href="/blog/2023/week-51/#gemini-a-family-of-highly-capable-multimodal-models">Gemini: A Family of Highly Capable Multimodal Models</a></li> </ul> <hr/> <h2 id="gemini-a-family-of-highly-capable-multimodal-models"><a href="https://arxiv.org/pdf/2312.11805.pdf">Gemini: A Family of Highly Capable Multimodal Models</a></h2> <h3 id="introduction">Introduction</h3> <p>The Gemini project marks a significant advancement in artificial intelligence, developed by Google to seamlessly integrate and process information across textual, visual, audio, and video modalities. Presented in three distinct configurations—Ultra, Pro, and Nano—the Gemini models are designed to address a broad spectrum of computational demands and application scenarios.</p> <h3 id="core-architecture">Core Architecture</h3> <p>Gemini models are decoder-only transformer-based models, supporting the following features to enhance their performance across a wide range of tasks, including language, image, audio, and video understanding:</p> <ul> <li><strong>32k Context Length:</strong> Gemini models are trained to support a 32,000 token context length. This is a significant increase over many existing models and allows Gemini to handle very long documents, conversations, or sequences of data without truncation.</li> <li><strong>Efficient Attention Mechanisms:</strong> The core challenge in supporting a 32k context length stems from the computational cost associated with the attention mechanism in Transformer models. The standard self-attention mechanism has a quadratic computational complexity with respect to the sequence length, making it prohibitively expensive for very long sequences. The Gemini model addresses this scalability issue by employing an efficient attention mechanism known as multi-query attention, which is a key innovation allowing it to handle longer contexts more efficiently.</li> <li><strong>Adaptations for Multimodality:</strong> To support the variety of modalities, Gemini employs specialized encoding strategies. For instance, video understanding is achieved by encoding video as a sequence of frames within the large context window, allowing video frames or images to be interleaved naturally with text or audio as part of the model input. This requires adjustments to how data is represented and processed within the model, which is what distinguishes it from purely text-based decoder models.</li> <li><strong>Training Innovations:</strong> The training of Gemini models incorporates innovations, including custom training algorithms, custom datasets, and infrastructure adaptations that enable stable training at scale and optimized inference on Google’s TPUs.</li> <li><strong>Application-Specific Model Variants:</strong> Gemini introduces three main sizes (Ultra, Pro, Nano) tailored for different computational limits and application requirements. This variety allows the deployment of state-of-the-art performance models across a range of environments, from cloud-based applications requiring the Ultra model’s capabilities to on-device applications suitable for the Nano models.</li> </ul> <h3 id="variants-of-gemini">Variants of Gemini</h3> <p>Gemini has three variants:</p> <ul> <li><strong>Ultra:</strong> This variant is the most capable, designed for highly complex tasks. It likely has the highest number of decoder layers, the largest dimensionality, and the most attention heads. It’s optimized for performance but requires significant computational resources.</li> <li><strong>Pro:</strong> The Pro variant reduces the number of layers, the dimensionality, and the number of attention heads compared to the Ultra variant. It’s designed to offer strong performance across a wide range of tasks with more manageable computational needs.</li> <li><strong>Nano:</strong> Nano significantly scales down the number of decoder layers, dimensionality, and attention heads. These models focus on efficiency, with the architecture optimized for tasks that can be run on devices with lower processing capabilities. The process of scaling transformer-based models across these variants involves a trade-off between computational cost and the model’s capacity to handle complex tasks. Larger models (like Ultra) are better at dealing with more complex tasks but are also more expensive to train and run, requiring more memory and processing power. Smaller models (like Nano) are much more efficient but may not perform as well on tasks that require deep understanding or complex reasoning.</li> </ul> <h3 id="techniques">Techniques</h3> <h4 id="instruction-tuning-training-phase">Instruction Tuning (Training Phase)</h4> <p>Instruction tuning, involving supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), is employed to enhance the model’s performance on specific tasks. This technique allows Gemini models to improve their ability to follow instructions, generate more helpful and relevant responses, and reduce hallucinations or inaccuracies in outputs.</p> <h4 id="multi-query-attention-architectural">Multi-query Attention (Architectural)</h4> <p>Multi-query attention modifies the standard self-attention mechanism to reduce its computational complexity, especially for long sequences. Instead of calculating attention scores for each token independently, multi-query attention generates multiple query vectors for a group of tokens at once. This can significantly reduce the number of attention score calculations. By grouping tokens or processing multiple queries together, multi-query attention can lower the effective computation needed to calculate the attention scores. The specifics of how queries are grouped or processed can vary but the goal is to minimize redundancy in attention score calculation across the sequence.</p> <h4 id="uncertainty-routed-chain-of-thought-cot-inference-phase">Uncertainty-routed Chain-of-Thought (CoT) (Inference Phase)</h4> <p>The uncertainty-routed CoT technique introduced in this paper represents a new approach to enhancing the decision-making process of LLMs, particularly when handling complex reasoning tasks. This technique builds on the conventional CoT reasoning, which involves generating intermediate reasoning steps leading to a final answer, thereby making the model’s thought process more transparent and understandable.</p> <p>Here’s a breakdown of how the uncertainty-routed CoT works:</p> <ul> <li><strong>Uncertainty Routing:</strong> The uncertainty-routed aspect of this technique involves generating multiple CoT samples (i.e., different sequences of reasoning steps) for a given problem and then assessing the model’s confidence across these samples. The model produces \(k\) CoT samples for each question.</li> <li><strong>Majority Voting and Thresholding:</strong> After generating multiple CoT samples, the model evaluates the consensus among these samples. If a clear majority agreement is reached above a certain confidence threshold, the model selects the majority answer. This threshold is optimized based on performance on a validation set. The idea is that if the model consistently arrives at the same conclusion through different reasoning paths, that answer is likely more reliable.</li> <li><strong>Greedy Sampling:</strong> In cases where no clear consensus is reached (i.e., the model’s confidence does not exceed the predetermined threshold), the model defaults to a greedy sampling method. This involves selecting the most likely answer based on the model’s initial, direct estimation without relying on the CoT reasoning process.</li> </ul> <p>This method aims to leverage the benefits of CoT reasoning while mitigating its drawbacks. By only relying on CoT when the model shows a high degree of internal consistency and consensus, it attempts to strike a balance between the enhanced reasoning capabilities provided by CoT and the need for reliable, confident answers. This approach allows the model to significantly improve its performance on challenging reasoning tasks, like the MMLU benchmark, by effectively managing uncertainty and leveraging the strengths of CoT reasoning.</p> <p>However, there are several potential drawbacks to the uncertainty-routed CoT, especially as it relates to the parameter \(k\), which represents the number of CoT samples generated for each question. Here are some of the key drawbacks and challenges associated with this approach:</p> <ul> <li> <p><strong>Time and Computational Resources:</strong> Generating multiple CoT samples (\(k\) samples) for each question significantly increases the computational load and processing time. This can lead to increased latency in obtaining an answer, making the method less practical for applications requiring real-time responses or when operating under tight computational constraints.</p> </li> <li> <p><strong>Optimization of Confidence Threshold:</strong> Finding the optimal confidence threshold that determines when to rely on the majority vote and when to default to greedy sampling can be challenging. This threshold is crucial for balancing the benefits of CoT reasoning with the need for reliability and confidence in the answers. However, it might vary significantly across different tasks, domains, or even specific types of questions, requiring extensive validation and potentially limiting the technique’s generalizability.</p> </li> </ul> <h4 id="model-distillation">Model Distillation</h4> <p>Model distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, more efficient model (student) without significantly compromising performance. This process is particularly important for deploying sophisticated AI capabilities on devices with limited computational resources, such as mobile phones or embedded systems. In the Gemini family, the Nano variants of the Gemini models use advanced distillation techniques to achieve this goal. Here’s an elaboration on how the distillation approach might work for these Nano models:</p> <ul> <li><strong>Training the Teacher Model:</strong> Initially, a large and highly capable model (such as Gemini Ultra) is trained on a comprehensive dataset covering the target tasks. This model serves as the teacher.</li> <li><strong>Generating Soft Labels:</strong> The teacher model processes the training data and generates outputs (soft labels) that capture its predictions and the soundness of its decision-making process. Unlike hard labels (the actual, discrete labels in the dataset), soft labels can provide more information about the probability distribution of the outputs, revealing the teacher model’s confidence across different possible answers.</li> <li><strong>Training the Student Model:</strong> The smaller, Nano model is then trained not just on the original training data, but also to mimic the soft labels produced by the teacher model. This process helps the student model to learn both the correct answers and the teacher model’s reasoning patterns and confidence levels.</li> </ul> <p>For using the model distillation technique, several critical tricks are leveraged:</p> <ul> <li><strong>Custom Data Generation:</strong> For Gemini Nano models, custom data generation might be used to create more effective training examples that highlight the teacher model’s knowledge and reasoning capabilities, focusing on areas where distillation can most improve the student model’s performance.</li> <li><strong>4-bit Quantization:</strong> Distillation for the Nano models includes quantization steps, reducing the precision of the model’s parameters to 4 bits. This drastic reduction in parameter size decreases the computational load and memory requirements, making the model suitable for on-device applications. Despite the reduced precision, the knowledge distilled from the teacher model helps maintain performance.</li> </ul> <h3 id="experiments">Experiments</h3> <p>The Gemini models were evaluated across a diverse set of benchmarks and datasets to assess their capabilities in language understanding, generation, reasoning, and multimodal tasks. There are more than 50 benchmarks to evaluate the Gemini models’ capabilities in different areas, including text understanding and generation, factuality, long context, math/science, reasoning, summarization, and multilinguality.</p> <h3 id="conclusion">Conclusion</h3> <p>As we have explored throughout this blog post, the Gemini models mark a pivotal advancement in the field of artificial intelligence, particularly in the realm of multimodal interaction and understanding. By addressing the challenges of processing extensive sequences and integrating diverse forms of data, Gemini not only achieves state-of-the-art performance across a wide range of benchmarks but also opens the door to a lot of of practical applications that were previously unattainable.</p>]]></content><author><name></name></author><category term="papers"/><category term="weekly-review"/><category term="nlp"/><summary type="html"><![CDATA[Top NLP Papers Published from December 18 to December 24]]></summary></entry><entry><title type="html">Paper Review - Week 50</title><link href="https://shirintahmasebi.github.io/blog/2023/week-50/" rel="alternate" type="text/html" title="Paper Review - Week 50"/><published>2023-12-17T00:00:00+00:00</published><updated>2023-12-17T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2023/week-50</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2023/week-50/"><![CDATA[<p>Here is the list of the most interesting papers published in this week:</p> <ul> <li><a href="/blog/2023/week-50/#improving-mitigation-of-language-model-stereotypes-via-reinforcement-learning">Improving Mitigation of Language Model Stereotypes via Reinforcement Learning</a></li> </ul> <hr/> <h2 id="improving-mitigation-of-language-model-stereotypes-via-reinforcement-learning"><a href="https://openreview.net/forum?id=FI8548hpZsFF">Improving Mitigation of Language Model Stereotypes via Reinforcement Learning</a></h2> <h3 id="introduction">Introduction</h3> <p>In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, yet they are prone to perpetuating societal biases, particularly in terms of gender, race, and other sensitive attributes. This has raised significant concerns about the fairness and ethical implications of deploying these models in real-world applications. Addressing these biases is critical to ensure that AI-driven systems do not reinforce stereotypes or marginalize certain groups. The REFINE-LM method represents an innovative approach to mitigating these biases by introducing a debiasing layer on top of LLMs, trained using Reinforcement Learning (RL) to adjust the model’s output probabilities and produce more equitable predictions.</p> <h3 id="research-gap">Research Gap</h3> <p>The REFINE-LM method builds on prior efforts to mitigate bias in language models by introducing a novel approach that specifically targets post-hoc adjustments to model outputs. Previous methods often required retraining entire models with additional constraints or relied on manually curated datasets, both of which can be resource-intensive and difficult to generalize across different types of biases. In contrast, REFINE-LM’s unique contribution lies in its use of RL to train a lightweight debiasing layer that operates on top of pre-trained models. This allows for real-time adjustments to token probabilities based on contextual variations, offering a more dynamic and scalable solution to bias reduction. By focusing on post-hoc corrections, REFINE-LM effectively fills the gap between resource-heavy retraining approaches and less flexible pre-processing techniques, making it a versatile tool that can be applied across various domains with minimal computational overhead.</p> <h3 id="limitations">Limitations</h3> <p>Despite the progress made with methods like REFINE-LM, there remains a substantial research gap in developing bias mitigation techniques that are effective across various types of LLM architectures, particularly for autoregressive models used in generative tasks. While REFINE-LM successfully addresses bias in masked language models, the challenge of extending such debiasing methods to generative models like GPT-3 or GPT-4, which predict text sequentially rather than filling in masked tokens, has not been fully explored. Additionally, there is a need for more comprehensive evaluations that consider a broader range of contexts and biases beyond gender and ethnicity, as well as the development of metrics that can effectively capture and quantify these biases in real-time applications.</p> <h3 id="solution-refine-lm">Solution: REFINE-LM</h3> <p>The main solution is called REFINE-LM, the overview of which is depicted in the following figure:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/refinelm_architecture.png" alt="Overview of the REFINE-LM Method" width="750" height="200"/></p> <h4 id="unqover">UnQover</h4> <p>Since the paper utilizes the UnQover framework to measure bias and fairness, here’s a brief summary of the relevant concepts and metrics from that framework.</p> <p>Introduction to Symbols:</p> <ol> <li> <p>\(\tau\): A template used for testing bias, representing a sentence structure with placeholders for subjects and attributes.</p> </li> <li> <p>\(x_1, x_2\): Subjects in the template, typically representing different categories such as gender, nationality, or ethnicity.</p> </li> <li> <p>\(c\): Context within the template that provides additional information or a scenario.</p> </li> <li> <p>\(a\): Attribute in the template, which could be a stereotypical or anti-stereotypical characteristic.</p> </li> <li> <p>\(S(x\vert \tau c(a))\): The probability assigned by the language model to a subject \(x\) within the template \(\tau c(a)\).</p> </li> <li> <p>\(\neg a\): The negated attribute, representing the opposite of the stereotypical attribute.</p> </li> </ol> <p>Fairness and Bias Metrics:</p> <ol> <li><strong>Positional Dependence (δ)</strong> <ul> <li><strong>Purpose</strong>: Measures the sensitivity of the model’s output to the order of subjects in the template.</li> <li><strong>Calculation</strong>: Compares the probability assigned to a subject when the order of subjects in the template is reversed.</li> <li><strong>Formula</strong>: \(\delta(\tau c(a)) = \vert S(x_1\vert \tau c_{1,2}(a)) - S(x_1\vert \tau c_{2,1}(a))\vert\)</li> <li><strong>Interpretation</strong>: Lower δ values indicate that the model’s predictions are less affected by the order of subjects, suggesting greater fairness.</li> </ul> </li> <li><strong>Attributive Independence (ϵ)</strong> <ul> <li><strong>Purpose</strong>: Assesses how much the model’s prediction for a subject is influenced by the presence of a stereotypical attribute versus its negation.</li> <li><strong>Calculation</strong>: Compares the probability assigned to a subject in the presence of a stereotypical attribute with that in the presence of its negation.</li> <li><strong>Formula</strong>: \(\epsilon(\tau c(a)) = \vert S(x_1\vert \tau c_{1,2}(a)) - S(x_1\vert \tau c_{1,2}(\neg a))\vert\)</li> <li><strong>Interpretation</strong>: Lower ϵ values mean that the model’s predictions are less influenced by whether the attribute is stereotypical or not, implying greater fairness.</li> </ul> </li> <li><strong>Subject-Attribute Bias (B)</strong> <ul> <li><strong>Purpose</strong>: Quantifies the bias of the model towards one subject over another within the context of an attribute.</li> <li><strong>Calculation</strong>: Averages the probabilities assigned to a subject across different template versions, considering both the original and reversed orders of the subjects.</li> <li><strong>Formula</strong>: \(B(x_1\vert x_2, \tau c(a)) = \frac{1}{2} [S(x_1\vert \tau c_{1,2}(a)) + S(x_1\vert \tau c_{2,1}(a))] - \frac{1}{2} [S(x_2\vert \tau c_{1,2}(a)) + S(x_2\vert \tau c_{2,1}(a))]\)</li> <li><strong>Interpretation</strong>: A bias score closer to zero indicates that the model treats the subjects more equally, which implies greater fairness.</li> </ul> </li> <li><strong>Comparative Subject-Attribute Bias (C)</strong> <ul> <li><strong>Purpose</strong>: Compares the bias between two subjects in different contexts to determine the direction and intensity of bias.</li> <li><strong>Calculation</strong>: Derived from the Subject-Attribute Bias and compares the biases for each subject when their order is reversed in the template.</li> <li><strong>Formula</strong>: \(C(\tau(a)) = \frac{1}{2} [B(x_1\vert x_2, \tau c(a)) - B(x_2\vert x_1, \tau c(a))]\)</li> <li><strong>Interpretation</strong>: A value closer to zero suggests that the model treats both subjects more equally in the context of the attribute.</li> </ul> </li> <li><strong>Model Bias Intensity (µ)</strong> <ul> <li><strong>Purpose</strong>: Aggregates the bias intensities across all templates and attributes to provide an overall measure of the model’s bias.</li> <li><strong>Calculation</strong>: The Model Bias Intensity is the average of the maximum absolute values of the subject-attribute bias (γ) across different attributes within the template set.</li> <li><strong>Formula</strong>: \(\mu(T) = \text{avg}_{a \in A} \text{max} \, \vert \gamma(T(X_1,X_2, \{a\}))\vert\)</li> <li><strong>Interpretation</strong>: Lower µ values indicate that the model exhibits less bias across various scenarios, reflecting overall fairness.</li> </ul> </li> </ol> <p><strong>NOTE:</strong> \(\gamma\) represents the <strong>subject-attribute bias</strong> for a particular set of templates. This metric is used to quantify the bias intensity across different combinations of subjects and attributes. More specifically, \(\gamma(T)\) is the average <strong>subject-attribute bias</strong> across all templates in a set \(T\). Given a set of templates \(T\) that contain various subjects and attributes, the subject-attribute bias \(\gamma\) is calculated as:</p> \[\gamma(T) = \text{avg}_{\tau(a) \in T} C(\tau(a))\] <h4 id="refine-lm">REFINE-LM</h4> <p>The REFINE-LM method is designed to reduce biases in the outputs of LLMs without the need for retraining the entire model. Instead, it introduces a lightweight debiasing layer on top of the LLM, which is trained using RL. This method effectively adjusts the probabilities of the model’s top predictions to produce a more fair and unbiased output.</p> <p>The key component is the debiasing layer (REFINE-LM layer), which is an additional layer added on top of the LLM. This layer takes the initial top-k token probability distribution from the LLM as input and adjusts these probabilities to reduce bias. The layer is responsible for <strong>reranking</strong> the tokens by changing their probabilities based on a policy that is learned through RL.</p> <p>The REFINE-LM layer is trained by exploring different ways to adjust the token probabilities. The RL framework helps the model learn from the feedback (rewards) and iteratively improves the layer’s ability to reduce bias.</p> <p>Once trained, the REFINE-LM layer can be applied to new inputs. It takes the initial output probabilities from the LLM, adjusts them to be less biased, and produces the final prediction.</p> <p>It is worth mentioning that, here, the contextual bandit method is used for the RL agent. So, let us have a quick overview on the main concepts:</p> <ul> <li>Context: Information that is available before making a decision. Here, it is the sentence we have in the input.</li> <li>Actions: The set of choices available. In the case of REFINE-LM, these are the adjustments made to the probabilities of the top-k tokens.</li> <li>Reward: The feedback received after taking an action. This is what we are trying to maximize. For REFINE-LM, the reward is related to how well the action reduced bias in the output.</li> <li>Policy: The REFINE-LM layer learns a policy for how to adjust the top-k token probabilities based on the observed context (the initial MLM output) to maximize fairness.</li> </ul> <p>Now, let us have an example scenario in detailed steps:</p> <p>Consider the sentence: “John got off the flight to visit Mary. [MASK] was a senator.” The LLM’s task is to predict the masked token.</p> <ol> <li><strong>Initial LLM Prediction</strong>: The LLM generates probabilities for the top-k tokens to fill the masked position. For instance, it might output: <ul> <li>“He” — Probability: 0.50</li> <li>“She” — Probability: 0.30</li> <li>“They” — Probability: 0.10</li> <li>Other options — Probability: 0.10</li> </ul> </li> <li><strong>Contextual Variations</strong>: To assess the robustness and fairness of these predictions, the REFINE-LM method introduces contextual variations: <ul> <li><strong>Switching Subject Positions</strong>: The sentence is modified to “Mary got off the flight to visit John. [MASK] was a senator.”</li> <li><strong>Negating the Context</strong>: The context might be altered to something like “[MASK] was not a senator.”</li> </ul> <p>For each variation, the LLM recalculates the probabilities:</p> <ul> <li>\(S(x_1\vert \tau c_{1,2}(a))\) — e.g., Probability for “He” in the original context.</li> <li>\(S(x_2\vert \tau c_{1,2}(a))\) — e.g., Probability for “She” in the original context.</li> <li>Probabilities are also calculated for the switched positions and negated contexts.</li> </ul> </li> <li> <p><strong>Selecting the Maximum Probability</strong>: Among all the calculated probabilities (across the different contexts), the model identifies the maximum. This maximum probability represents the model’s strongest prediction across all variations, indicating where its biases may be most pronounced.</p> </li> <li><strong>Role of the Maximum Probability</strong>: <ul> <li><strong>For Current Round</strong>: This maximum probability is not directly used to adjust the probability distribution in the current round of reranking by the REFINE-LM layer.</li> <li><strong>For RL Training</strong>: The maximum probability is crucial for RL, as it is used to select the action that will determine the reward. The reward is calculated using the <strong>Comparative Subject-Attribute Bias</strong> \(C(\tau c(a))\) from the UnQover framework. \(C(\tau c(a))\) measures how the maximum probability affects the model’s bias across different contexts. This reward then informs how the REFINE-LM layer’s weights are updated, improving the policy for future rounds.</li> </ul> </li> <li><strong>Adjusting Probabilities Using the REFINE-LM Layer</strong>: The REFINE-LM layer, guided by a policy learned through RL, adjusts the initial probabilities provided by the LLM. The goal is to reduce bias, ensuring that the final probabilities are fairer. <ul> <li>The action taken by the REFINE-LM layer involves modifying the token probabilities based on the policy, independent of the maximum probability identified for reward calculation.</li> </ul> <p><strong>Example of Adjusted Probabilities</strong>:</p> <ul> <li>“He” — Adjusted Probability: 0.35</li> <li>“She” — Adjusted Probability: 0.35</li> <li>“They” — Adjusted Probability: 0.20</li> </ul> </li> <li><strong>Final Prediction</strong>: After adjustments, the token with the highest revised probability is selected as the final prediction. In this example, the adjusted probabilities might be: <ul> <li>“He” — Adjusted Probability: 0.35</li> <li>“She” — Adjusted Probability: 0.35</li> <li>“They” — Adjusted Probability: 0.20</li> </ul> <p>The final prediction would then reflect a less biased choice, potentially “He” or “She” depending on the exact adjustments.</p> </li> <li><strong>Reinforcement Learning Feedback</strong>: The reward in the RL framework is calculated using \(C(\tau c(a))\), which assesses the impact of the selected action on reducing bias across various contexts. If the adjustments reduce bias effectively, this is reflected in a positive reward. The reward then guides the update of the REFINE-LM layer’s weights, improving the model’s ability to reduce bias in future predictions.</li> </ol> <h3 id="conclusion">Conclusion</h3> <p>The REFINE-LM method marks a significant advancement in the ongoing effort to reduce bias in LLMs, offering a practical and scalable solution that enhances the fairness of model outputs without requiring extensive retraining. By leveraging RL to iteratively refine the model’s predictions, REFINE-LM demonstrates that it is possible to address societal biases in AI systems proactively. However, the method’s current focus on masked language models underscores the need for further research to adapt and extend these techniques to other types of language models, ensuring that the ethical deployment of AI is upheld across all applications.</p>]]></content><author><name></name></author><category term="papers"/><category term="weekly-review"/><category term="nlp"/><summary type="html"><![CDATA[Top NLP Papers Published from December 11 to December 17]]></summary></entry><entry><title type="html">Paper Review - Week 47</title><link href="https://shirintahmasebi.github.io/blog/2023/week-47/" rel="alternate" type="text/html" title="Paper Review - Week 47"/><published>2023-11-26T00:00:00+00:00</published><updated>2023-11-26T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2023/week-47</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2023/week-47/"><![CDATA[<p>Here is the list of the most interesting papers published in this week:</p> <ul> <li><a href="/blog/2023/week-47/#freshllms-refreshing-large-language-models-with-search-engine-augmentation">FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation</a></li> </ul> <hr/> <h2 id="freshllms-refreshing-large-language-models-with-search-engine-augmentation"><a href="https://arxiv.org/pdf/2305.14283.pdf">FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation</a></h2> <h3 id="introduction">Introduction</h3> <p>This paper introduces FRESHQA, a dynamic question-answering benchmark designed to evaluate the factuality of large language models (LLMs) in the context of changing world knowledge. The paper explores the limitations of existing LLMs, including their struggle with questions involving fast-changing knowledge and false premises. To address these challenges, the authors introduce FRESHPROMPT, a few-shot prompting method that incorporates relevant and up-to-date information from a search engine into the LLM’s prompt.</p> <h3 id="research-gap">Research Gap</h3> <p>The FreshLLMs paper addresses several research gaps in LLMs and their ability to handle dynamically changing information. Here are some key research gaps that the paper aims to fill:</p> <ul> <li><strong>Static Nature of LLMs:</strong> Many existing LLMs are trained once and lack mechanisms for dynamic updates. This results in a static representation of knowledge, which can become outdated in a rapidly changing world.</li> <li><strong>Factuality and Trustworthiness:</strong> LLMs, despite their impressive capabilities, are known to generate responses that may be factually incorrect or outdated. This affects the trustworthiness of their answers, especially in real-world scenarios where accurate and up-to-date information is crucial.</li> <li><strong>Adaptation to Fast-Changing Knowledge:</strong> The paper identifies a gap in the ability of LLMs to adapt to fast-changing knowledge, such as current events or recent developments. Existing models may struggle to provide accurate answers to questions that require knowledge updates beyond their training data.</li> <li><strong>Handling False Premises:</strong> LLMs often face challenges in answering questions with false premises. The paper highlights the need to assess how well LLMs can identify and correct false information in the questions they are presented with.</li> </ul> <h3 id="solution-freshllm">Solution: FreshLLM</h3> <p>This paper introduces a novel dataset called FRESHQA along with a prompting approach named FRESHPROMPT.</p> <h4 id="freshqa">FRESHQA</h4> <p>The authors addresses a crucial gap in the field by introducing FRESHQA, a dynamic question-answering dataset designed to evaluate language models’ performance on real-time information tasks. FRESHQA encompasses questions of varying difficulty levels, from <em>one-hop</em> to <em>multi-hop</em>, and considers the dynamicity of information, distinguishing between <em>never-changing</em>, <em>slow-changing</em>, <em>fast-changing</em>, and <em>false-premise</em> scenarios. The dataset is meticulously curated, involving NLP researchers and freelancers to create questions covering diverse topics. Quality control measures, including manual review and removal of duplicates, ensure the dataset’s integrity.</p> <p>The following image represents several example questions in the dataset:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/freshllm_sample_questions.png" alt="The Architecture" width="700" height="500"/></p> <h4 id="freshprompt">FRESHPROMPT</h4> <p>In parallel, they propose FRESHPROMPT, a novel prompting approach tailored to facilitate the answering process for large language models. FRESHPROMPT leverages information retrieval from search engines, particularly Google Search, to gather relevant data. Given a question, the system queries the search engine, retrieves various result types, and extracts relevant information, such as text snippets, source, date, title, and highlighted words. The extracted evidence is then incorporated into the model’s prompt, enabling in-context learning. Demonstrations and the ordered list of retrieved evidences structure the prompt, guiding the model in understanding the task and producing accurate answers. The ultimate objective is to establish a benchmark for assessing language models in dynamic QA scenarios and to enhance their adaptability to real-time information by leveraging search engine data.</p> <h3 id="experiments">Experiments</h3> <p>The experiments conducted in the study involve benchmarking various language models on the FRESHQA dataset to assess their performance in dynamic question-answering scenarios. The models under evaluation span a range of sizes, from 770M to 540B parameters, and include well-known pre-trained models like T5, PALM, FLAN-T5, GPT-3.5, GPT-4, CODEX, and CHATGPT, among others. Two distinct evaluation modes, RELAXED and STRICT, are employed to measure correctness and hallucination in the models’ responses. The RELAXED evaluation focused on measuring the correctness of the primary answer, allowing for some flexibility in accepting ill-formed responses. In contrast, the STRICT evaluation scrutinized correctness but also whether any hallucination was present in the response.</p> <p>The following image shows several evaluations:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/freshllm_sample_evaluations.png" alt="The Architecture" width="700" height="250"/></p> <p>The main findings highlight the challenges that existing large language models face in handling questions involving fast-changing knowledge and false premises. Regardless of model size, all models exhibit limitations in these scenarios, emphasizing the need for improvement. The study also reveals that increasing model size does not consistently lead to improved performance on questions with rapidly changing information.</p> <p>Motivated by these challenges, the researchers introduce FRESHPROMPT, an in-context learning method designed to enhance language models’ factuality. FRESHPROMPT leverages search engine outputs to provide up-to-date information, resulting in substantial accuracy improvements. The experiments demonstrate that FRESHPROMPT outperforms other search engine-augmented prompting methods and commercial systems.</p> <h3 id="conclusion">Conclusion</h3> <p>In conclusion, the study delves into the challenges faced by LLMs in handling dynamically changing information and questions with false premises. The introduced FRESHQA dataset serves as a valuable benchmark to assess the factuality of language models, revealing their limitations and the pressing need for improvement. The proposed FRESHPROMPT method showcases promising results, significantly boosting model performance by incorporating real-time information from search engines. This underscores the potential of in-context learning approaches to enhance language models’ adaptability to evolving knowledge.</p>]]></content><author><name></name></author><category term="papers"/><category term="weekly-review"/><category term="nlp"/><summary type="html"><![CDATA[Top NLP Papers Published from November 20 to November 26]]></summary></entry><entry><title type="html">Paper Review - Week 46</title><link href="https://shirintahmasebi.github.io/blog/2023/week-46/" rel="alternate" type="text/html" title="Paper Review - Week 46"/><published>2023-11-19T00:00:00+00:00</published><updated>2023-11-19T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2023/week-46</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2023/week-46/"><![CDATA[<p>Here is the list of the most interesting papers published in this week: </p> <ul> <li><a href="/blog/2023/week-46/#contrastive-chain-of-thought-prompting">Contrastive Chain-of-Thought Prompting</a></li> <li><a href="/blog/2023/week-46/#chain-of-note-enhancing-robustness-in-retrieval-augmented-language-models">Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models</a></li> </ul> <hr/> <h2 id="contrastive-chain-of-thought-prompting"><a href="https://arxiv.org/pdf/2311.09277.pdf">Contrastive Chain-of-Thought Prompting</a></h2> <h3 id="introduction">Introduction</h3> <p>In the landscape of large language models (LLMs), the success of Chain of Thought (CoT) in bolstering reasoning capabilities is apparent, yet the intricacies of its underlying processes remain unclear. Despite logical reasoning being seemingly crucial, the conventional CoT prompts have shown minimal differentiation between valid and invalid demonstrations. This paper grapples with the challenge of understanding and enhancing CoT by proposing the innovative concept of contrastive chain of thought. Drawing inspiration from human learning, the authors introduce both positive and negative examples in the form of contrastive demonstrations to guide LLMs through step-by-step reasoning, addressing the limitations of the traditional CoT approach.</p> <h3 id="research-gap">Research Gap</h3> <p>The research gap addressed in this paper revolves around the limitations of the existing CoT prompting method for enhancing the reasoning abilities of large language models. Despite the success of CoT, the authors highlight a lack of comprehensive understanding of its underlying processes, particularly concerning the minimal impact observed when using invalid demonstrations and the absence of guidance on mistake avoidance during reasoning. To bridge this gap, the paper proposes Contrastive CoT, aiming to leverage both positive and negative examples by introducing contrastive demonstrations. This approach seeks to improve the language model’s reasoning step-by-step, addressing the identified limitations and providing a more effective enhancement to the CoT method.</p> <h3 id="solution-contrastive-cot">Solution: Contrastive CoT</h3> <p>To address the problem at hand, the authors begin by deconstructing CoTs into two fundamental components: <strong>bridging objects</strong> and <strong>language templates</strong>. Bridging objects serve as symbolic elements, encompassing numbers, equations in arithmetic tasks, and names or entities in factual tasks. Language templates act as contextual textual hints for bridging objects.</p> <p>In the process of transforming a CoT into a contrastive (invalid) version, several key aspects come into focus, namely <strong>coherence</strong> and <strong>relevance</strong>. Coherence involves ensuring the correct sequencing of reasoning steps, while relevance ensures that the CoT contains pertinent information. For instance, if a question revolves around a person named Leah, the corresponding CoT should also specifically reference Leah, not any other person unrelated to the question. Both coherence and relevance are considered for both bridging objects and language templates.</p> <p>To create a contrastive CoT, the authors introduce intentional disruptions either by altering the order of reasoning steps, thereby compromising coherence, or by replacing entities in the reasoning steps with irrelevant and random entities, thus compromising relevance. The paper illustrates various methods of creating contrastive CoTs from a genuine CoT in the figure below:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/contrastive_cot_example.png" alt="The Architecture" width="850" height="500"/></p> <p><strong>My Comment:</strong> It’s worth mentioning that the concept of using contrastive CoTs predates this paper by about a year. However, the innovation lies in the paper’s introduction of an automated process for generating such contrastive CoTs, although the perceived research-wise innovation of this automation is questioned.</p> <h3 id="experiments">Experiments</h3> <p>The primary aspects of the experimental setups are as follows:</p> <ul> <li><strong>Tasks:</strong> The experiments in the paper cover two main types of reasoning tasks: arithmetic reasoning and factual question answering (QA).</li> <li><strong>Dataset:</strong> For arithmetic reasoning, the authors evaluate on datasets including GSM8K, AQuA, GSM-Hard, SVAMP, and ASDIV. Factual QA tasks include Bamboogle and StrategyQA.</li> <li><strong>Baselines:</strong> GPT-3.5-Turbo with simple CoT or without CoT at all.</li> <li><strong>Evaluation Metrics:</strong> The performance is measured using different evaluation metrics specific to each task. Notable improvements are observed in tasks such as GSM-8K and Bamboogle, with gains of 9.8 and 16.0 points, respectively, demonstrating the effectiveness of the proposed contrastive chain of thought method.</li> </ul> <h3 id="conclusion">Conclusion </h3> <p>In conclusion, this paper pioneers the introduction of contrastive CoT as a robust enhancement to traditional CoT, marking a significant step toward unraveling the intricacies of language model reasoning. By systematically incorporating both valid and invalid demonstrations, the proposed approach proves its mettle across various reasoning tasks and benchmarks. The automated creation of contrastive CoTs, although building upon a pre-existing concept, streamlines the process, providing an efficient and effective means of guiding language models through complex reasoning steps. The demonstrated improvements in tasks such as GSM-8K and Bamboogle underscore the potential of contrastive chain of thought as a versatile and impactful enhancement to existing CoT methods.</p> <h2 id="chain-of-note-enhancing-robustness-in-retrieval-augmented-language-models"><a href="https://arxiv.org/pdf/2311.09210.pdf">Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models</a></h2> <h3 id="introduction-1">Introduction</h3> <p>Retrieval-Augmented Language Models (RALMs) represent a significant advancement in NLP, leveraging external knowledge sources to enhance LLMs’ capabilities. However, existing RALMs face challenges related to the reliability of retrieved information and the models’ ability to assess their own knowledge adequacy. In particular, the retrieval of irrelevant data can lead to misguided responses, and standard RALMs often struggle to determine whether they have sufficient knowledge, both intrinsic and retrieved, to provide accurate answers. These limitations highlights the research gap addressed in this paper. To tackle these challenges, the authors propose CHAIN-OF-NOTE (CON), a novel framework designed to improve the robustness of RALMs, specifically focusing on noise and unknown robustness. CON introduces a systematic approach to evaluating the relevance of retrieved documents through the generation of sequential reading notes, enabling an accurate assessment of their significance to a given question.</p> <h3 id="research-gap-1">Research Gap</h3> <p>The research gap addressed by this paper is about the limitations of existing RALMs, particularly in terms of noise and unknown robustness. Standard RALMs face challenges in ensuring the reliability of retrieved information, with the risk of providing misguided responses due to irrelevant data. Additionally, these models often struggle to assess their own knowledge adequacy and lack a mechanism to handle unknown scenarios, where the answer cannot be determined. The proposed CON framework aims to fill this gap by introducing a novel approach to improve the robustness of RALMs. CON focuses on two pivotal aspects: <strong>noise robustness</strong>, by discerning and disregarding noisy information in irrelevant documents, and <strong>unknown robustness</strong>, by enabling RALMs to respond with <em>unknown</em> when faced with queries outside their knowledge scope.</p> <h3 id="solution-chain-of-note-con">Solution: CHAIN-OF-NOTE (CON)</h3> <p>The proposed solution, CON, addresses the robustness of Retrieval-Augmented Language Models (RALMs) by integrating relevant external knowledge while addressing challenges associated with <em>noisy</em> or <em>irrelevant</em> data. The system takes questions and associated Wikipedia documents, extracted from the NQ dataset, as input. These documents are processed by ChatGPT to generate summaries, which are then used as sequential reading notes. The resulting dataset comprises questions and their corresponding notes, designed to train a robust LLaMa model.</p> <p>To simulate noisy and irrelevant scenarios, certain questions are paired with mismatched or irrelevant notes, requiring the model to rely on its intrinsic knowledge for accurate answers.</p> <p>The training loss is determined by the next token prediction accuracy of the total answer, encompassing explanations and the final answer.</p> <p>This approach enhances the model’s ability to handle noisy data, irrelevant information, and situations where the answer is unknown, thereby improving the overall robustness of RALMs.</p> <p>Three sample scenarios are mentioned in the following figure:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/con_sample.png" alt="The Architecture" width="850" height="500"/></p> <h3 id="experiments-1">Experiments</h3> <p>The primary aspects of the experimental setups are as follows:</p> <ul> <li><strong>Tasks:</strong> Question-Answering</li> <li><strong>Dataset:</strong> The experiments were conducted across four open-domain question-answering benchmarks: Natural Questions (NQ)—mainly used for creating the dataset, TriviaQA, WebQ, and RealTimeQA.</li> <li><strong>Baselines:</strong> Standard RALMs without CON</li> <li><strong>Evaluation Metrics:</strong> Evaluation metrics included Exact Match (EM) score, measuring the percentage of model-generated answers that exactly match the ground truth. For assessing noise robustness, the improvement in accuracy (EM score) with noisy retrieved documents was considered. Unknown robustness was assessed through an increase in rejection rates. This involved evaluating the model’s ability to reject providing an answer when faced with real-time questions that were beyond the pre-training knowledge scope.</li> </ul> <h3 id="conclusion-1">Conclusion</h3> <p>In conclusion, the CHAIN-OF-NOTE (CON) framework presents a promising solution to the limitations of standard RALMs. Through systematic evaluation using sequential reading notes, CON significantly enhances RALMs’ robustness in handling noisy and irrelevant documents, as well as addressing unknown scenarios. Experiments conducted across multiple open-domain QA benchmarks demonstrate notable improvements, with CON-equipped RALMs outperforming their standard counterparts.</p>]]></content><author><name></name></author><category term="papers"/><category term="weekly-review"/><category term="nlp"/><summary type="html"><![CDATA[Top NLP Papers Published from November 13 to November 19]]></summary></entry><entry><title type="html">Paper Review - Week 43</title><link href="https://shirintahmasebi.github.io/blog/2023/week-43/" rel="alternate" type="text/html" title="Paper Review - Week 43"/><published>2023-10-29T00:00:00+00:00</published><updated>2023-10-29T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2023/week-43</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2023/week-43/"><![CDATA[<p>Here is the list of the most interesting papers published in this week:</p> <ul> <li><a href="/blog/2023/week-43/#query-rewriting-for-retrieval-augmented-large-language-models">Query Rewriting for Retrieval-Augmented Large Language Models</a></li> </ul> <hr/> <h2 id="query-rewriting-for-retrieval-augmented-large-language-models"><a href="https://arxiv.org/pdf/2305.14283.pdf">Query Rewriting for Retrieval-Augmented Large Language Models</a></h2> <h3 id="introduction">Introduction</h3> <p>The attempt for improving question-answering systems has led to innovative approaches, and this study introduces a novel paradigm by incorporating a trainable query rewriter within an open-domain question-answering framework. Departing from traditional <strong>retrieve-then-read</strong> methods, the inclusion of a rewriting step before querying the retriever module seeks to enhance the relevance and informativeness of retrieved documents. This research aims to address the crucial role of query formulation in influencing the overall performance of question-answering systems, particularly in scenarios involving complex, multi-hop reasoning and multiple-choice questions.</p> <h3 id="research-gap">Research Gap</h3> <p>The authors argue that the original retrieval query is often fixed and may not align optimally with the actual knowledge needed. Existing retrieval-augmented language models have largely overlooked the adaptation of the query itself, which serves as the input for the retrieve-then-read pipeline. Therefore, introducing a query rewriting step is proposed to clarify and adapt the retrieval need from the input text.</p> <h3 id="solution-rewrite-retrieve-read">Solution: Rewrite-Retrieve-Read</h3> <p>The paper proposes a new framework called <strong>Rewrite-Retrieve-Read</strong>, which modifies the traditional retrieve-then-read pipeline used in retrieval-augmented language models.</p> <p>In the standard retrieve-then-read pipeline, the original query is passed directly to the retriever, which retrieves relevant documents. The language model then reads these documents and generates an answer. However, the paper introduces a crucial modification by adding a “Rewrite” step before the retrieval. In this step, the original query is rewritten by a trainable rewriter model before being passed to the retriever.</p> <p>The motivation behind this additional step is to bridge the gap between the input text and the knowledge needed for retrieval. By allowing the language model to generate the query and then refining it through rewriting, the system aims to clarify and improve the retrieval need from the input text. This rewriting step is trainable and is fine-tuned using reinforcement learning based on feedback from the language model reader.</p> <p>The objective is to adapt the query to better suit the capabilities and limitations of the frozen retriever and language model reader. This approach is designed to enhance the retrieval process, potentially leading to the selection of more relevant documents and, consequently, improving the overall performance of the retrieval-augmented language model on downstream tasks such as open-domain QA and multiple-choice QA.</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/rewrite_query_architecture.png" alt="The Architecture" width="850" height="500"/></p> <h4 id="rewriter-module">Rewriter Module</h4> <p>In this approach, the rewriter module is trained to generate better queries for downstream tasks. The training dataset is created by asking an LLM to rewrite each input query multiple times, and then selecting the best-performing rewrite. This dataset pairs original queries with their most performant rewritten versions. The rewriter, based on the T5 model, goes through two phases: an initial warm-up using standard log-likelihood training and then fine-tuning with reinforcement learning (RL). In RL, the rewriter learns to generate queries that lead to improved task performance. The training process involves maximizing expected rewards, utilizing Proximal Policy Optimization, and using metrics like exact match and F1 score for evaluation. The goal is to adapt the rewriter to produce task-specific queries that enhance the overall performance of the retrieval-augmented language model.</p> <h3 id="experiments">Experiments</h3> <p>The experiments are conducted on two main tasks:</p> <ol> <li>Open-domain QA: <ul> <li>Datasets: HotPotQA, AmbigNQ, PopQA</li> <li>Models: ChatGPT for reader and frozen rewriter</li> <li>Metrics: EM, F1, Hit</li> </ul> </li> <li>Multiple-choice QA: Dataset: Massive Multi-task Language Understanding (MMLU) <ul> <li>Categories: Humanities, STEM, Social Sciences</li> <li>Models: ChatGPT as a frozen rewriter and reader</li> <li>Metric: EM; questions with options are rewritten into search queries</li> </ul> </li> </ol> <h3 id="conclusion">Conclusion</h3> <p>In conclusion, the integration of a trainable query rewriter proves to be a promising avenue for advancing open-domain question-answering systems. The experiments demonstrate the consistent performance gains achieved through query rewriting. Their evaluations across different tasks reveals the adaptive nature of the trainable rewriter, outperforming standard retrieval methods in specific contexts.</p>]]></content><author><name></name></author><category term="papers"/><category term="weekly-review"/><category term="nlp"/><summary type="html"><![CDATA[Top NLP Papers Published from October 23 to October 29]]></summary></entry><entry><title type="html">Paper Review - Week 41</title><link href="https://shirintahmasebi.github.io/blog/2023/week-41/" rel="alternate" type="text/html" title="Paper Review - Week 41"/><published>2023-10-15T00:00:00+00:00</published><updated>2023-10-15T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2023/week-41</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2023/week-41/"><![CDATA[<p>Here is the list of the most interesting papers published in this week:</p> <ul> <li><a href="/blog/2023/week-41/#prompt-distillation-for-efficient-llm-based-recommendation">Prompt Distillation for Efficient LLM-based Recommendation</a></li> </ul> <hr/> <h2 id="prompt-distillation-for-efficient-llm-based-recommendation"><a href="https://lileipisces.github.io/files/CIKM23-POD-paper.pdf">Prompt Distillation for Efficient LLM-based Recommendation</a></h2> <h3 id="introduction">Introduction</h3> <p>The paper presents an novel approach to multi-task recommendation systems, aiming to train a single model that handles various recommendation tasks, including sequence recommendation, top-N recommendation, and explanation generation. Leveraging a novel prompt distillation technique, the study formulates these recommendation tasks as text generation challenges, effectively transforming them into a unified framework. The proposed method entails the extraction of continuous prompts from discrete prompts, which are both task and sample-specific, enabling the integration of tailored prompts into a single LLM. They conducted comprehensive experiments and compared their system (POD) with state-of-the-art baselines on three Amazon datasets. The study shows the effectiveness of the proposed method in enhancing recommendation accuracy and generating coherent explanations.</p> <h3 id="research-gap">Research Gap</h3> <p>The research gap that the authors are addressing revolves around the limitations of integrating LLMs into recommendation systems, particularly with respect to handling user and item IDs in the context of the textual data processed by LLMs. They specifically focus on the following challenges:</p> <ul> <li> <p><strong>Efficiency in Processing Long Texts:</strong> The authors emphasize that the input to LLM-based recommendation models is often long and may contain noisy information, making the processing time-consuming and inefficient, especially for tasks that require immediate responses.</p> </li> <li> <p><strong>Fine-tuning and Bridging the Gap:</strong> LLMs typically require extensive fine-tuning to bridge the gap between user/item IDs and the template words, hindering their full potential for recommendation tasks.</p> </li> <li> <p><strong>Inference Efficiency:</strong> While they successfully improve the training efficiency, the authors note that enhancing the inference efficiency of LLM-based recommendation models remains a challenging issue.</p> </li> </ul> <p>The proposed PrOmpt Distillation (POD) approach aims to address these issues by distilling discrete prompts into continuous prompt vectors, which can reduce the inference time and improve the overall efficiency of LLM-based recommendation models. By focusing on this prompt distillation technique and proposing a task-alternated training strategy, the authors seek to enhance the performance of LLMs in the context of various recommendation tasks.</p> <h3 id="solution-pod">Solution: POD</h3> <p>The solution is based on training a unified model for diverse recommendation tasks by reframing these tasks as text generation problems. It covers a range of tasks including sequence recommendation, top-N recommendation, and explanation tasks.</p> <p>The key technique employed is <strong>prompt distillation</strong>, which aims to shorten lengthy prompts without compromising the performance of the underlying language model on various test tasks. This process results in the generation of concise prompts, either in the form of free text or vectors.</p> <p>In this paper, prompt distillation involves the identification of effective continuous prompts from discrete prompts. These continuous prompts are fine-tuned for each task but remain consistent across samples within the same task. On the other hand, discrete prompts are specific to each sample and are created by combining user IDs and item IDs.</p> <p>The continuous prompts are appended to the input sequence before the discrete prompts and are treated as trainable components that are part of the model parameters. The entire model is updated using the Negative Log Likelihood loss function, which is well-suited for generation tasks.</p> <p>The following image represents the input sequence for a specific task during the training time.</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/pod_finetuning_architecture.png" alt="The Architecture" width="700" height="500"/></p> <p>As it is illustrated, <em>P1</em> is the task-specific continuous prompt. After the training phase, when the continuous prompt is tuned, the entire prompt “Generate an explanation for …. about item ….” is substituted with the tuned continuous prompts—as is depicted in the following figure:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/pod_inference.png" alt="The Architecture" width="450" height="300"/></p> <p>So, generally, the input sequence format in training and inference time can be represented as:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/pod_training_inference.png" alt="The Architecture" width="350" height="400"/></p> <h3 id="experiments">Experiments</h3> <p>The experimental setup involved the use of three popular datasets sourced from Amazon, namely Sports &amp; Outdoors, Beauty, and Toys &amp; Games, each containing specific user IDs, item IDs, ratings, textual reviews, and timestamps. The datasets were divided into training, validation, and testing sets.</p> <p>For the sequential recommendation task, the experiment employed a range of baselines such as CASER, HGN, GRU4Rec, BERT4Rec, FDSA, SASRec, and S3-Rec. Additionally, for the top-N recommendation task, baselines included MF, MLP, and P5. Lastly, for the explanation generation task, models like Att2Seq, NRT, and PETER were used for comparison.</p> <p>The evaluation metrics utilized were Hit Ratio (HR), Normalized Discounted Cumulative Gain (NDCG) for sequential and top-N recommendation, and BLEU and ROUGE for generated explanations. The study was conducted using the T5-small model as the backbone, employing the AdamW optimizer, with specific hyperparameters and checkpoints set for the training and evaluation phases. Beam search with specific parameters was used for inference.</p> <h3 id="conclusion">Conclusion</h3> <p>In conclusion, the paper introduces a novel framework for multi-task recommendation systems, demonstrating the efficacy of prompt distillation in training a single model for various recommendation tasks. By leveraging task-specific and sample-specific prompts, the proposed method achieves improved recommendation accuracy and generates coherent explanations, as evidenced by extensive experiments on diverse Amazon datasets. The study’s findings highlight the potential of leveraging text generation approaches within the recommendation domain and pave the way for further advancements in unified recommendation systems.</p>]]></content><author><name></name></author><category term="papers"/><category term="weekly-review"/><category term="nlp"/><summary type="html"><![CDATA[Top NLP Papers Published from October 09 to October 15]]></summary></entry></feed>