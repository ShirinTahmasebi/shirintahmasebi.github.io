<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://shirintahmasebi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shirintahmasebi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-01-18T07:41:24+00:00</updated><id>https://shirintahmasebi.github.io/feed.xml</id><title type="html">Shirin Tahmasebi</title><subtitle>Personal Webpage </subtitle><entry><title type="html">Paper Review - Week 47</title><link href="https://shirintahmasebi.github.io/blog/2023/week-47/" rel="alternate" type="text/html" title="Paper Review - Week 47"/><published>2023-11-26T00:00:00+00:00</published><updated>2023-11-26T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2023/week-47</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2023/week-47/"><![CDATA[<p>Here is the list of the most interesting papers published in this week:</p> <ul> <li><a href="/blog/2023/week-47/#freshllms-refreshing-large-language-models-with-search-engine-augmentation">FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation</a></li> </ul> <hr/> <h2 id="freshllms-refreshing-large-language-models-with-search-engine-augmentation"><a href="https://arxiv.org/pdf/2305.14283.pdf">FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation</a></h2> <h3 id="introduction">Introduction</h3> <p>This paper introduces FRESHQA, a dynamic question-answering benchmark designed to evaluate the factuality of large language models (LLMs) in the context of changing world knowledge. The paper explores the limitations of existing LLMs, including their struggle with questions involving fast-changing knowledge and false premises. To address these challenges, the authors introduce FRESHPROMPT, a few-shot prompting method that incorporates relevant and up-to-date information from a search engine into the LLM’s prompt.</p> <h3 id="research-gap">Research Gap</h3> <p>The FreshLLMs paper addresses several research gaps in LLMs and their ability to handle dynamically changing information. Here are some key research gaps that the paper aims to fill:</p> <ul> <li><strong>Static Nature of LLMs:</strong> Many existing LLMs are trained once and lack mechanisms for dynamic updates. This results in a static representation of knowledge, which can become outdated in a rapidly changing world.</li> <li><strong>Factuality and Trustworthiness:</strong> LLMs, despite their impressive capabilities, are known to generate responses that may be factually incorrect or outdated. This affects the trustworthiness of their answers, especially in real-world scenarios where accurate and up-to-date information is crucial.</li> <li><strong>Adaptation to Fast-Changing Knowledge:</strong> The paper identifies a gap in the ability of LLMs to adapt to fast-changing knowledge, such as current events or recent developments. Existing models may struggle to provide accurate answers to questions that require knowledge updates beyond their training data.</li> <li><strong>Handling False Premises:</strong> LLMs often face challenges in answering questions with false premises. The paper highlights the need to assess how well LLMs can identify and correct false information in the questions they are presented with.</li> </ul> <h3 id="solution-freshllm">Solution: FreshLLM</h3> <p>This paper introduces a novel dataset called FRESHQA along with a prompting approach named FRESHPROMPT.</p> <h4 id="freshqa">FRESHQA</h4> <p>The authors addresses a crucial gap in the field by introducing FRESHQA, a dynamic question-answering dataset designed to evaluate language models’ performance on real-time information tasks. FRESHQA encompasses questions of varying difficulty levels, from <em>one-hop</em> to <em>multi-hop</em>, and considers the dynamicity of information, distinguishing between <em>never-changing</em>, <em>slow-changing</em>, <em>fast-changing</em>, and <em>false-premise</em> scenarios. The dataset is meticulously curated, involving NLP researchers and freelancers to create questions covering diverse topics. Quality control measures, including manual review and removal of duplicates, ensure the dataset’s integrity.</p> <p>The following image represents several example questions in the dataset:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/freshllm_sample_questions.png" alt="The Architecture" width="700" height="500"/></p> <h4 id="freshprompt">FRESHPROMPT</h4> <p>In parallel, they propose FRESHPROMPT, a novel prompting approach tailored to facilitate the answering process for large language models. FRESHPROMPT leverages information retrieval from search engines, particularly Google Search, to gather relevant data. Given a question, the system queries the search engine, retrieves various result types, and extracts relevant information, such as text snippets, source, date, title, and highlighted words. The extracted evidence is then incorporated into the model’s prompt, enabling in-context learning. Demonstrations and the ordered list of retrieved evidences structure the prompt, guiding the model in understanding the task and producing accurate answers. The ultimate objective is to establish a benchmark for assessing language models in dynamic QA scenarios and to enhance their adaptability to real-time information by leveraging search engine data.</p> <h3 id="experiments">Experiments</h3> <p>The experiments conducted in the study involve benchmarking various language models on the FRESHQA dataset to assess their performance in dynamic question-answering scenarios. The models under evaluation span a range of sizes, from 770M to 540B parameters, and include well-known pre-trained models like T5, PALM, FLAN-T5, GPT-3.5, GPT-4, CODEX, and CHATGPT, among others. Two distinct evaluation modes, RELAXED and STRICT, are employed to measure correctness and hallucination in the models’ responses. The RELAXED evaluation focused on measuring the correctness of the primary answer, allowing for some flexibility in accepting ill-formed responses. In contrast, the STRICT evaluation scrutinized correctness but also whether any hallucination was present in the response.</p> <p>The following image shows several evaluations:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/freshllm_sample_evaluations.png" alt="The Architecture" width="700" height="250"/></p> <p>The main findings highlight the challenges that existing large language models face in handling questions involving fast-changing knowledge and false premises. Regardless of model size, all models exhibit limitations in these scenarios, emphasizing the need for improvement. The study also reveals that increasing model size does not consistently lead to improved performance on questions with rapidly changing information.</p> <p>Motivated by these challenges, the researchers introduce FRESHPROMPT, an in-context learning method designed to enhance language models’ factuality. FRESHPROMPT leverages search engine outputs to provide up-to-date information, resulting in substantial accuracy improvements. The experiments demonstrate that FRESHPROMPT outperforms other search engine-augmented prompting methods and commercial systems.</p> <h3 id="conclusion">Conclusion</h3> <p>In conclusion, the study delves into the challenges faced by LLMs in handling dynamically changing information and questions with false premises. The introduced FRESHQA dataset serves as a valuable benchmark to assess the factuality of language models, revealing their limitations and the pressing need for improvement. The proposed FRESHPROMPT method showcases promising results, significantly boosting model performance by incorporating real-time information from search engines. This underscores the potential of in-context learning approaches to enhance language models’ adaptability to evolving knowledge.</p>]]></content><author><name></name></author><category term="papers"/><category term="weekly-review"/><category term="nlp"/><summary type="html"><![CDATA[Top NLP Papers Published from November 20 to November 26]]></summary></entry><entry><title type="html">Paper Review - Week 46</title><link href="https://shirintahmasebi.github.io/blog/2023/week-46/" rel="alternate" type="text/html" title="Paper Review - Week 46"/><published>2023-11-19T00:00:00+00:00</published><updated>2023-11-19T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2023/week-46</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2023/week-46/"><![CDATA[<p>Here is the list of the most interesting papers published in this week: </p> <ul> <li><a href="/blog/2023/week-46/#contrastive-chain-of-thought-prompting">Contrastive Chain-of-Thought Prompting</a></li> <li><a href="/blog/2023/week-46/#chain-of-note-enhancing-robustness-in-retrieval-augmented-language-models">Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models</a></li> </ul> <hr/> <h2 id="contrastive-chain-of-thought-prompting"><a href="https://arxiv.org/pdf/2311.09277.pdf">Contrastive Chain-of-Thought Prompting</a></h2> <h3 id="introduction">Introduction</h3> <p>In the landscape of large language models (LLMs), the success of Chain of Thought (CoT) in bolstering reasoning capabilities is apparent, yet the intricacies of its underlying processes remain unclear. Despite logical reasoning being seemingly crucial, the conventional CoT prompts have shown minimal differentiation between valid and invalid demonstrations. This paper grapples with the challenge of understanding and enhancing CoT by proposing the innovative concept of contrastive chain of thought. Drawing inspiration from human learning, the authors introduce both positive and negative examples in the form of contrastive demonstrations to guide LLMs through step-by-step reasoning, addressing the limitations of the traditional CoT approach.</p> <h3 id="research-gap">Research Gap</h3> <p>The research gap addressed in this paper revolves around the limitations of the existing CoT prompting method for enhancing the reasoning abilities of large language models. Despite the success of CoT, the authors highlight a lack of comprehensive understanding of its underlying processes, particularly concerning the minimal impact observed when using invalid demonstrations and the absence of guidance on mistake avoidance during reasoning. To bridge this gap, the paper proposes Contrastive CoT, aiming to leverage both positive and negative examples by introducing contrastive demonstrations. This approach seeks to improve the language model’s reasoning step-by-step, addressing the identified limitations and providing a more effective enhancement to the CoT method.</p> <h3 id="solution-contrastive-cot">Solution: Contrastive CoT</h3> <p>To address the problem at hand, the authors begin by deconstructing CoTs into two fundamental components: <strong>bridging objects</strong> and <strong>language templates</strong>. Bridging objects serve as symbolic elements, encompassing numbers, equations in arithmetic tasks, and names or entities in factual tasks. Language templates act as contextual textual hints for bridging objects.</p> <p>In the process of transforming a CoT into a contrastive (invalid) version, several key aspects come into focus, namely <strong>coherence</strong> and <strong>relevance</strong>. Coherence involves ensuring the correct sequencing of reasoning steps, while relevance ensures that the CoT contains pertinent information. For instance, if a question revolves around a person named Leah, the corresponding CoT should also specifically reference Leah, not any other person unrelated to the question. Both coherence and relevance are considered for both bridging objects and language templates.</p> <p>To create a contrastive CoT, the authors introduce intentional disruptions either by altering the order of reasoning steps, thereby compromising coherence, or by replacing entities in the reasoning steps with irrelevant and random entities, thus compromising relevance. The paper illustrates various methods of creating contrastive CoTs from a genuine CoT in the figure below:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/contrastive_cot_example.png" alt="The Architecture" width="850" height="500"/></p> <p><strong>My Comment:</strong> It’s worth mentioning that the concept of using contrastive CoTs predates this paper by about a year. However, the innovation lies in the paper’s introduction of an automated process for generating such contrastive CoTs, although the perceived research-wise innovation of this automation is questioned.</p> <h3 id="experiments">Experiments</h3> <p>The primary aspects of the experimental setups are as follows:</p> <ul> <li><strong>Tasks:</strong> The experiments in the paper cover two main types of reasoning tasks: arithmetic reasoning and factual question answering (QA).</li> <li><strong>Dataset:</strong> For arithmetic reasoning, the authors evaluate on datasets including GSM8K, AQuA, GSM-Hard, SVAMP, and ASDIV. Factual QA tasks include Bamboogle and StrategyQA.</li> <li><strong>Baselines:</strong> GPT-3.5-Turbo with simple CoT or without CoT at all.</li> <li><strong>Evaluation Metrics:</strong> The performance is measured using different evaluation metrics specific to each task. Notable improvements are observed in tasks such as GSM-8K and Bamboogle, with gains of 9.8 and 16.0 points, respectively, demonstrating the effectiveness of the proposed contrastive chain of thought method.</li> </ul> <h3 id="conclusion">Conclusion </h3> <p>In conclusion, this paper pioneers the introduction of contrastive CoT as a robust enhancement to traditional CoT, marking a significant step toward unraveling the intricacies of language model reasoning. By systematically incorporating both valid and invalid demonstrations, the proposed approach proves its mettle across various reasoning tasks and benchmarks. The automated creation of contrastive CoTs, although building upon a pre-existing concept, streamlines the process, providing an efficient and effective means of guiding language models through complex reasoning steps. The demonstrated improvements in tasks such as GSM-8K and Bamboogle underscore the potential of contrastive chain of thought as a versatile and impactful enhancement to existing CoT methods.</p> <h2 id="chain-of-note-enhancing-robustness-in-retrieval-augmented-language-models"><a href="https://arxiv.org/pdf/2311.09210.pdf">Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models</a></h2> <h3 id="introduction-1">Introduction</h3> <p>Retrieval-Augmented Language Models (RALMs) represent a significant advancement in NLP, leveraging external knowledge sources to enhance LLMs’ capabilities. However, existing RALMs face challenges related to the reliability of retrieved information and the models’ ability to assess their own knowledge adequacy. In particular, the retrieval of irrelevant data can lead to misguided responses, and standard RALMs often struggle to determine whether they have sufficient knowledge, both intrinsic and retrieved, to provide accurate answers. These limitations highlights the research gap addressed in this paper. To tackle these challenges, the authors propose CHAIN-OF-NOTE (CON), a novel framework designed to improve the robustness of RALMs, specifically focusing on noise and unknown robustness. CON introduces a systematic approach to evaluating the relevance of retrieved documents through the generation of sequential reading notes, enabling an accurate assessment of their significance to a given question.</p> <h3 id="research-gap-1">Research Gap</h3> <p>The research gap addressed by this paper is about the limitations of existing RALMs, particularly in terms of noise and unknown robustness. Standard RALMs face challenges in ensuring the reliability of retrieved information, with the risk of providing misguided responses due to irrelevant data. Additionally, these models often struggle to assess their own knowledge adequacy and lack a mechanism to handle unknown scenarios, where the answer cannot be determined. The proposed CON framework aims to fill this gap by introducing a novel approach to improve the robustness of RALMs. CON focuses on two pivotal aspects: <strong>noise robustness</strong>, by discerning and disregarding noisy information in irrelevant documents, and <strong>unknown robustness</strong>, by enabling RALMs to respond with <em>unknown</em> when faced with queries outside their knowledge scope.</p> <h3 id="solution-chain-of-note-con">Solution: CHAIN-OF-NOTE (CON)</h3> <p>The proposed solution, CON, addresses the robustness of Retrieval-Augmented Language Models (RALMs) by integrating relevant external knowledge while addressing challenges associated with <em>noisy</em> or <em>irrelevant</em> data. The system takes questions and associated Wikipedia documents, extracted from the NQ dataset, as input. These documents are processed by ChatGPT to generate summaries, which are then used as sequential reading notes. The resulting dataset comprises questions and their corresponding notes, designed to train a robust LLaMa model.</p> <p>To simulate noisy and irrelevant scenarios, certain questions are paired with mismatched or irrelevant notes, requiring the model to rely on its intrinsic knowledge for accurate answers.</p> <p>The training loss is determined by the next token prediction accuracy of the total answer, encompassing explanations and the final answer.</p> <p>This approach enhances the model’s ability to handle noisy data, irrelevant information, and situations where the answer is unknown, thereby improving the overall robustness of RALMs.</p> <p>Three sample scenarios are mentioned in the following figure:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/con_sample.png" alt="The Architecture" width="850" height="500"/></p> <h3 id="experiments-1">Experiments</h3> <p>The primary aspects of the experimental setups are as follows:</p> <ul> <li><strong>Tasks:</strong> Question-Answering</li> <li><strong>Dataset:</strong> The experiments were conducted across four open-domain question-answering benchmarks: Natural Questions (NQ)—mainly used for creating the dataset, TriviaQA, WebQ, and RealTimeQA.</li> <li><strong>Baselines:</strong> Standard RALMs without CON</li> <li><strong>Evaluation Metrics:</strong> Evaluation metrics included Exact Match (EM) score, measuring the percentage of model-generated answers that exactly match the ground truth. For assessing noise robustness, the improvement in accuracy (EM score) with noisy retrieved documents was considered. Unknown robustness was assessed through an increase in rejection rates. This involved evaluating the model’s ability to reject providing an answer when faced with real-time questions that were beyond the pre-training knowledge scope.</li> </ul> <h3 id="conclusion-1">Conclusion</h3> <p>In conclusion, the CHAIN-OF-NOTE (CON) framework presents a promising solution to the limitations of standard RALMs. Through systematic evaluation using sequential reading notes, CON significantly enhances RALMs’ robustness in handling noisy and irrelevant documents, as well as addressing unknown scenarios. Experiments conducted across multiple open-domain QA benchmarks demonstrate notable improvements, with CON-equipped RALMs outperforming their standard counterparts.</p>]]></content><author><name></name></author><category term="papers"/><category term="weekly-review"/><category term="nlp"/><summary type="html"><![CDATA[Top NLP Papers Published from November 13 to November 19]]></summary></entry><entry><title type="html">Paper Review - Week 43</title><link href="https://shirintahmasebi.github.io/blog/2023/week-43/" rel="alternate" type="text/html" title="Paper Review - Week 43"/><published>2023-10-29T00:00:00+00:00</published><updated>2023-10-29T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2023/week-43</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2023/week-43/"><![CDATA[<p>Here is the list of the most interesting papers published in this week:</p> <ul> <li><a href="/blog/2023/week-43/#query-rewriting-for-retrieval-augmented-large-language-models">Query Rewriting for Retrieval-Augmented Large Language Models</a></li> </ul> <hr/> <h2 id="query-rewriting-for-retrieval-augmented-large-language-models"><a href="https://arxiv.org/pdf/2305.14283.pdf">Query Rewriting for Retrieval-Augmented Large Language Models</a></h2> <h3 id="introduction">Introduction</h3> <p>The attempt for improving question-answering systems has led to innovative approaches, and this study introduces a novel paradigm by incorporating a trainable query rewriter within an open-domain question-answering framework. Departing from traditional <strong>retrieve-then-read</strong> methods, the inclusion of a rewriting step before querying the retriever module seeks to enhance the relevance and informativeness of retrieved documents. This research aims to address the crucial role of query formulation in influencing the overall performance of question-answering systems, particularly in scenarios involving complex, multi-hop reasoning and multiple-choice questions.</p> <h3 id="research-gap">Research Gap</h3> <p>The authors argue that the original retrieval query is often fixed and may not align optimally with the actual knowledge needed. Existing retrieval-augmented language models have largely overlooked the adaptation of the query itself, which serves as the input for the retrieve-then-read pipeline. Therefore, introducing a query rewriting step is proposed to clarify and adapt the retrieval need from the input text.</p> <h3 id="solution-rewrite-retrieve-read">Solution: Rewrite-Retrieve-Read</h3> <p>The paper proposes a new framework called <strong>Rewrite-Retrieve-Read</strong>, which modifies the traditional retrieve-then-read pipeline used in retrieval-augmented language models.</p> <p>In the standard retrieve-then-read pipeline, the original query is passed directly to the retriever, which retrieves relevant documents. The language model then reads these documents and generates an answer. However, the paper introduces a crucial modification by adding a “Rewrite” step before the retrieval. In this step, the original query is rewritten by a trainable rewriter model before being passed to the retriever.</p> <p>The motivation behind this additional step is to bridge the gap between the input text and the knowledge needed for retrieval. By allowing the language model to generate the query and then refining it through rewriting, the system aims to clarify and improve the retrieval need from the input text. This rewriting step is trainable and is fine-tuned using reinforcement learning based on feedback from the language model reader.</p> <p>The objective is to adapt the query to better suit the capabilities and limitations of the frozen retriever and language model reader. This approach is designed to enhance the retrieval process, potentially leading to the selection of more relevant documents and, consequently, improving the overall performance of the retrieval-augmented language model on downstream tasks such as open-domain QA and multiple-choice QA.</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/rewrite_query_architecture.png" alt="The Architecture" width="850" height="500"/></p> <h4 id="rewriter-module">Rewriter Module</h4> <p>In this approach, the rewriter module is trained to generate better queries for downstream tasks. The training dataset is created by asking an LLM to rewrite each input query multiple times, and then selecting the best-performing rewrite. This dataset pairs original queries with their most performant rewritten versions. The rewriter, based on the T5 model, goes through two phases: an initial warm-up using standard log-likelihood training and then fine-tuning with reinforcement learning (RL). In RL, the rewriter learns to generate queries that lead to improved task performance. The training process involves maximizing expected rewards, utilizing Proximal Policy Optimization, and using metrics like exact match and F1 score for evaluation. The goal is to adapt the rewriter to produce task-specific queries that enhance the overall performance of the retrieval-augmented language model.</p> <h3 id="experiments">Experiments</h3> <p>The experiments are conducted on two main tasks:</p> <ol> <li>Open-domain QA: <ul> <li>Datasets: HotPotQA, AmbigNQ, PopQA</li> <li>Models: ChatGPT for reader and frozen rewriter</li> <li>Metrics: EM, F1, Hit</li> </ul> </li> <li>Multiple-choice QA: Dataset: Massive Multi-task Language Understanding (MMLU) <ul> <li>Categories: Humanities, STEM, Social Sciences</li> <li>Models: ChatGPT as a frozen rewriter and reader</li> <li>Metric: EM; questions with options are rewritten into search queries</li> </ul> </li> </ol> <h3 id="conclusion">Conclusion</h3> <p>In conclusion, the integration of a trainable query rewriter proves to be a promising avenue for advancing open-domain question-answering systems. The experiments demonstrate the consistent performance gains achieved through query rewriting. Their evaluations across different tasks reveals the adaptive nature of the trainable rewriter, outperforming standard retrieval methods in specific contexts.</p>]]></content><author><name></name></author><category term="papers"/><category term="weekly-review"/><category term="nlp"/><summary type="html"><![CDATA[Top NLP Papers Published from October 23 to October 29]]></summary></entry><entry><title type="html">Paper Review - Week 41</title><link href="https://shirintahmasebi.github.io/blog/2023/week-41/" rel="alternate" type="text/html" title="Paper Review - Week 41"/><published>2023-10-15T00:00:00+00:00</published><updated>2023-10-15T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2023/week-41</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2023/week-41/"><![CDATA[<p>Here is the list of the most interesting papers published in this week:</p> <ul> <li><a href="/blog/2023/week-41/#prompt-distillation-for-efficient-llm-based-recommendation">Prompt Distillation for Efficient LLM-based Recommendation</a></li> </ul> <hr/> <h2 id="prompt-distillation-for-efficient-llm-based-recommendation"><a href="https://lileipisces.github.io/files/CIKM23-POD-paper.pdf">Prompt Distillation for Efficient LLM-based Recommendation</a></h2> <h3 id="introduction">Introduction</h3> <p>The paper presents an novel approach to multi-task recommendation systems, aiming to train a single model that handles various recommendation tasks, including sequence recommendation, top-N recommendation, and explanation generation. Leveraging a novel prompt distillation technique, the study formulates these recommendation tasks as text generation challenges, effectively transforming them into a unified framework. The proposed method entails the extraction of continuous prompts from discrete prompts, which are both task and sample-specific, enabling the integration of tailored prompts into a single LLM. They conducted comprehensive experiments and compared their system (POD) with state-of-the-art baselines on three Amazon datasets. The study shows the effectiveness of the proposed method in enhancing recommendation accuracy and generating coherent explanations.</p> <h3 id="research-gap">Research Gap</h3> <p>The research gap that the authors are addressing revolves around the limitations of integrating LLMs into recommendation systems, particularly with respect to handling user and item IDs in the context of the textual data processed by LLMs. They specifically focus on the following challenges:</p> <ul> <li> <p><strong>Efficiency in Processing Long Texts:</strong> The authors emphasize that the input to LLM-based recommendation models is often long and may contain noisy information, making the processing time-consuming and inefficient, especially for tasks that require immediate responses.</p> </li> <li> <p><strong>Fine-tuning and Bridging the Gap:</strong> LLMs typically require extensive fine-tuning to bridge the gap between user/item IDs and the template words, hindering their full potential for recommendation tasks.</p> </li> <li> <p><strong>Inference Efficiency:</strong> While they successfully improve the training efficiency, the authors note that enhancing the inference efficiency of LLM-based recommendation models remains a challenging issue.</p> </li> </ul> <p>The proposed PrOmpt Distillation (POD) approach aims to address these issues by distilling discrete prompts into continuous prompt vectors, which can reduce the inference time and improve the overall efficiency of LLM-based recommendation models. By focusing on this prompt distillation technique and proposing a task-alternated training strategy, the authors seek to enhance the performance of LLMs in the context of various recommendation tasks.</p> <h3 id="solution-pod">Solution: POD</h3> <p>The solution is based on training a unified model for diverse recommendation tasks by reframing these tasks as text generation problems. It covers a range of tasks including sequence recommendation, top-N recommendation, and explanation tasks.</p> <p>The key technique employed is <strong>prompt distillation</strong>, which aims to shorten lengthy prompts without compromising the performance of the underlying language model on various test tasks. This process results in the generation of concise prompts, either in the form of free text or vectors.</p> <p>In this paper, prompt distillation involves the identification of effective continuous prompts from discrete prompts. These continuous prompts are fine-tuned for each task but remain consistent across samples within the same task. On the other hand, discrete prompts are specific to each sample and are created by combining user IDs and item IDs.</p> <p>The continuous prompts are appended to the input sequence before the discrete prompts and are treated as trainable components that are part of the model parameters. The entire model is updated using the Negative Log Likelihood loss function, which is well-suited for generation tasks.</p> <p>The following image represents the input sequence for a specific task during the training time.</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/pod_finetuning_architecture.png" alt="The Architecture" width="700" height="500"/></p> <p>As it is illustrated, <em>P1</em> is the task-specific continuous prompt. After the training phase, when the continuous prompt is tuned, the entire prompt “Generate an explanation for …. about item ….” is substituted with the tuned continuous prompts—as is depicted in the following figure:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/pod_inference.png" alt="The Architecture" width="450" height="300"/></p> <p>So, generally, the input sequence format in training and inference time can be represented as:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/pod_training_inference.png" alt="The Architecture" width="350" height="400"/></p> <h3 id="experiments">Experiments</h3> <p>The experimental setup involved the use of three popular datasets sourced from Amazon, namely Sports &amp; Outdoors, Beauty, and Toys &amp; Games, each containing specific user IDs, item IDs, ratings, textual reviews, and timestamps. The datasets were divided into training, validation, and testing sets.</p> <p>For the sequential recommendation task, the experiment employed a range of baselines such as CASER, HGN, GRU4Rec, BERT4Rec, FDSA, SASRec, and S3-Rec. Additionally, for the top-N recommendation task, baselines included MF, MLP, and P5. Lastly, for the explanation generation task, models like Att2Seq, NRT, and PETER were used for comparison.</p> <p>The evaluation metrics utilized were Hit Ratio (HR), Normalized Discounted Cumulative Gain (NDCG) for sequential and top-N recommendation, and BLEU and ROUGE for generated explanations. The study was conducted using the T5-small model as the backbone, employing the AdamW optimizer, with specific hyperparameters and checkpoints set for the training and evaluation phases. Beam search with specific parameters was used for inference.</p> <h3 id="conclusion">Conclusion</h3> <p>In conclusion, the paper introduces a novel framework for multi-task recommendation systems, demonstrating the efficacy of prompt distillation in training a single model for various recommendation tasks. By leveraging task-specific and sample-specific prompts, the proposed method achieves improved recommendation accuracy and generates coherent explanations, as evidenced by extensive experiments on diverse Amazon datasets. The study’s findings highlight the potential of leveraging text generation approaches within the recommendation domain and pave the way for further advancements in unified recommendation systems.</p>]]></content><author><name></name></author><category term="papers"/><category term="weekly-review"/><category term="nlp"/><summary type="html"><![CDATA[Top NLP Papers Published from October 09 to October 15]]></summary></entry><entry><title type="html">Paper Review - Week 39</title><link href="https://shirintahmasebi.github.io/blog/2023/week-39/" rel="alternate" type="text/html" title="Paper Review - Week 39"/><published>2023-10-01T00:00:00+00:00</published><updated>2023-10-01T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2023/week-39</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2023/week-39/"><![CDATA[<p>Here is the list of the most interesting papers published in this week:</p> <ul> <li><a href="/blog/2023/week-39/#chain-of-verification-reduces-hallucination-in-llms">Chain-of-Verification Reduces Hallucination in LLMs</a></li> </ul> <hr/> <h2 id="chain-of-verification-reduces-hallucination-in-llms"><a href="https://arxiv.org/pdf/2309.11495.pdf">Chain-of-Verification Reduces Hallucination in LLMs</a></h2> <h3 id="introduction">Introduction</h3> <p>In the world of Large Language Models (LLMs), generating accurate responses is a challenge, especially when it comes to lesser-known facts. Despite having massive amounts of training data, models often end up creating plausible but incorrect information, known as hallucination. This paper tackles this issue head-on by introducing the Chain-of-Verification (CoVe) method. CoVe is all about making language models think twice before they speak. It guides the model through a four-step process: first, it generates a response; then, it plans and executes verification questions to fact-check itself, and finally, it refines the initial response based on the verification. The goal is to reduce hallucinations and make language models more reliable.</p> <h3 id="research-gap">Research Gap</h3> <p>The authors addresses the challenge of hallucination in large language models (LLMs). Hallucination refers to the generation of plausible yet incorrect factual information by language models. The paper identifies this issue as an unsolved problem in the context of LLMs, particularly when dealing with lesser-known facts or facts occurring relatively rarely in training corpora.</p> <h3 id="solution-cove">Solution: CoVe</h3> <p>The CoVe (Chain-of-Verification) method consists of four main steps aimed at reducing hallucinations in LLMs. These steps are performed by prompting the same language model in different ways to obtain the desired responses at each stage. The overall goal of CoVe is to leverage the model’s ability to reason and fact-check its own responses, leading to a final verified response with reduced hallucinations. The method aims to provide a more accurate and reliable output by iteratively assessing and correcting the model’s initial responses.</p> <p>Here’s a brief explanation of each step:</p> <ol> <li> <p><strong>Generate Baseline Response:</strong> The process begins by having the language model generate an initial response to a given query. This initial response serves as the baseline that the method aims to improve upon.</p> </li> <li> <p><strong>Plan Verifications:</strong> Conditioned on both the original query and the baseline response, the model is prompted to generate a series of verification questions. These questions are designed to test the factual claims made in the baseline response. The language model is free to phrase these questions in any form, and they don’t necessarily match the original text’s wording.</p> </li> <li> <p><strong>Execute Verifications:</strong> The planned verification questions are then answered independently. Instead of relying on external tools, the language model itself is used to check its own work. The goal is to assess whether there are any inconsistencies or mistakes in the original response by systematically answering the verification questions.</p> </li> <li> <p><strong>Generate Final Verified Response:</strong> Based on the answers to the verification questions and any identified inconsistencies, a final improved response is generated. This step incorporates the results of the verification process, aiming to produce a response that is more accurate and less prone to hallucination than the baseline generated in the first step.</p> </li> </ol> <p>The <strong>Execute Verifications</strong> step in the CoVe method involves answering the planned verification questions. The paper explores different variants of this step to further improve the overall performance. Here’s a brief explanation of each variant:</p> <ol> <li> <p><strong>Joint:</strong> In the joint method, the planning and execution of verification questions are accomplished using a single prompt. The few-shot demonstrations provided to the model include both the verification questions and their corresponding answers immediately after the questions. This approach aims to streamline the process but has potential downsides, as the verification answers may condition on the initial response, possibly leading to repetition of hallucinations.</p> </li> <li> <p><strong>Two-Step:</strong> The two-step approach separates the planning and execution into distinct steps, each with its own prompt. The planning prompt conditions on the baseline response, generating the verification questions. The answers to these questions are then obtained in a separate execution step, where the context given to the prompt contains only the questions and not the original baseline response. This separation aims to avoid repetition and potential biases introduced by the baseline response.</p> </li> <li> <p><strong>Factored:</strong> The factored approach involves answering each verification question independently as separate prompts. The key distinction is that these prompts do not contain the original baseline response, reducing the likelihood of simply copying or repeating hallucinations. This method provides the advantage of removing potential interference not only from the baseline response but also between answer contexts.</p> </li> <li> <p><strong>Factor+Revise:</strong> The factor+revise approach introduces an extra step after answering the verification questions. In this step, the CoVe pipeline explicitly cross-checks whether the verification answers indicate an inconsistency with the original responses. This is executed as a deliberate step via an additional prompt. Unlike the verification questions and answers, this phase needs to condition on both the baseline response and the verification question and answer pairs.</p> </li> </ol> <p>These variants offer different ways to structure the execution of verification questions, with the aim of improving the correctness of the overall response and reducing hallucinations. The paper investigates and compares these variants across various tasks to assess their effectiveness in mitigating inaccuracies.</p> <h3 id="experiments">Experiments</h3> <p>The primary aspects of the experimental setups are as follows:</p> <ul> <li><strong>Tasks:</strong> <ol> <li><strong>Wikidata:</strong> <ul> <li>Task: List-based questions generated using the Wikidata API.</li> <li>Example: “Who are some [Profession]s who were born in [City]?”</li> </ul> </li> <li><strong>Wiki-Category List:</strong> <ul> <li>Task: Set-generation task using the QUEST dataset created from Wikipedia Category lists.</li> <li>Example: “Name some Mexican animated horror films.”</li> </ul> </li> <li><strong>MultiSpanQA:</strong> <ul> <li>Task: Reading comprehension benchmark with questions having multiple independent answers.</li> <li>Example: “Who invented the first printing press and in what year?”</li> </ul> </li> <li><strong>Longform Generation of Biographies:</strong> <ul> <li>Task: Generating biographies using the prompt “Tell me a bio of &lt;entity&gt;.”</li> </ul> </li> </ol> </li> <li><strong>Dataset:</strong> The experiments use specific datasets for each task, including automatically generated questions from the Wikidata API, the QUEST dataset derived from Wikipedia Category lists, and benchmarks like MultiSpanQA for reading comprehension.</li> <li><strong>Baselines:</strong> Llama 65B and Llama 2 (Instruction Fine-Tuned)</li> <li><strong>Evaluation Metrics:</strong></li> <li><strong>Wikidata and Wiki-Category List:</strong> <ul> <li>Precision (micro-averaged) for measuring performance on list-based questions.</li> </ul> </li> <li><strong>MultiSpanQA:</strong> <ul> <li>F1 score for closed-book question-answering tasks.</li> </ul> </li> <li><strong>Longform Generation of Biographies:</strong> <ul> <li>FACTSCORE metric, which uses retrieval-augmented language models for fact-checking responses.</li> </ul> </li> </ul> <h3 id="conclusion">Conclusion</h3> <p>This paper takes on the challenge of fixing a persistent problem in language models, hallucination. By introducing deliberate reasoning steps, CoVe offers a systematic approach for language models to assess and correct their own responses. The experiments conducted across diverse language tasks, including list-based questions, closed-book QA, and longform text generation, showcase the effectiveness of CoVe in mitigating hallucinations. The exploration of variants, such as joint, two-step, factored, and factor+revise, provides valuable insights into the nuances of verification processes. Notably, CoVe demonstrates substantial improvements over baseline responses, outperforming existing methods. The findings underscore the significance of incorporating reasoning mechanisms in LLMs to enhance their accuracy, paving the way for further advancements in mitigating hallucination challenges and fostering more reliable language understanding models.</p>]]></content><author><name></name></author><category term="papers"/><category term="weekly-review"/><category term="nlp"/><summary type="html"><![CDATA[Top NLP Papers Published from September 25 to October 01]]></summary></entry><entry><title type="html">LoRA - Low-Rank Adaptation of LLMs</title><link href="https://shirintahmasebi.github.io/blog/2023/lora/" rel="alternate" type="text/html" title="LoRA - Low-Rank Adaptation of LLMs"/><published>2023-09-25T00:00:00+00:00</published><updated>2023-09-25T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2023/lora</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2023/lora/"><![CDATA[<p>In what follows, I am providing a brief summary of the interesting paper named <a href="https://arxiv.org/pdf/2106.09685.pdf">LoRA</a>.</p> <h2 id="introduction">Introduction</h2> <p>The focus of this paper is a groundbreaking approach called Low-Rank Adaptation (LoRA) in the context of large language models (LLMs). In the world of artificial intelligence and natural language processing, LLMs like GPT-3 have gained immense attention for their language generation and understanding capabilities. However, these models are notably large and resource-intensive, posing challenges for efficient fine-tuning and adaptation to specific tasks. LoRA presents an innovative solution to this challenge. At its core, LoRA revolves around the idea that the weight matrices within LLMs often exhibit low-rank properties, indicating significant redundancy in their parameters. By tapping into this redundancy, LoRA introduces a way to drastically reduce the number of parameters involved in fine-tuning, making the process more computationally efficient. In this paper, we delve into the intricacies of LoRA, exploring its principles, benefits, and implications for the world of deep learning.</p> <h2 id="background-matrix-rank">Background: Matrix Rank</h2> <p>The rank of a matrix signifies the maximum number of linearly independent rows or columns it possesses. In simpler terms, it tells us how many rows or columns in the matrix cannot be expressed as combinations of the others. A matrix with full rank means that all its rows and columns are independent, while a lower rank indicates linear dependencies among rows or columns. When the rank is much smaller than the total number of rows and columns, it implies high correlation or redundancy, allowing us to approximate the matrix using a smaller, lower-rank one while preserving its core characteristics.</p> <h2 id="low-rank-weight-matrices-in-llms">Low-Rank Weight Matrices in LLMs</h2> <p>Large language models like GPT-3 often feature weight matrices with low-rank properties. This means that many of the parameters in these matrices are redundant or highly correlated. Surprisingly, we can represent these weight matrices with fewer parameters while maintaining their essential capabilities. Various methods can make these large matrices smaller, including matrix factorization, weight pruning, quantization, and low-rank updates. In the paper, the primary approach employed to reduce large weight matrices is Low-Rank Update, a technique where low-rank updates are learned and applied to pre-trained model weights to adapt them to downstream tasks.</p> <h2 id="research-gap">Research Gap</h2> <p>Existing solutions for adapting pre-trained language models to new tasks can introduce increased inference latency. This happens for several reasons:</p> <ul> <li>Adapter Layers: Some methods introduce adapter layers between the pre-trained model’s layers. These layers add extra computations during inference, causing a cumulative effect when multiple adapters are used.</li> <li>Specialized Architectures: Certain approaches propose specialized model structures or modifications that require additional processing during inference.</li> <li>Parameter Explosion: Fine-tuning the entire pre-trained model can substantially increase the number of parameters, leading to higher computation and memory requirements.</li> <li>Complex Optimization Techniques: Some methods involve complex optimization techniques during fine-tuning, adding computational overhead during inference.</li> </ul> <p>In contrast, Low-Rank Adaptation (LoRA), as presented in the paper, focuses on minimizing additional computations during inference while effectively adapting pre-trained models to new tasks. By reducing the rank of weight matrices and performing low-rank updates, LoRA achieves this with fewer parameters and minimal impact on inference speed.</p> <h2 id="solution-lora---a-two-phase-approach">Solution: LoRA - A Two-Phase Approach</h2> <p>LoRA consists of two distinct phases: <strong>pre-training</strong> and <strong>fine-tuning</strong>. During pre-training, LoRA operates like a regular language model, using only the pre-trained weight matrix (\(W_0\)) for the forward pass and backpropagation. No additional matrices like \(A\) and \(B\) are involved at this stage.</p> <p>In the fine-tuning phase for downstream tasks, LoRA introduces the low-rank update \(\triangle W = AB\), where \(B\) and \(A\) are smaller matrices with trainable parameters specific to the task. During the forward pass, it computes \(h = (W_0 + \triangle W)x\), where W0 remains frozen, and \(\triangle W\) is task-specific and updatable. During backpropagation, gradients for \(A\) and \(B\) are computed separately and updated. This approach reduces gradient computation complexity and minimizes the impact on inference speed.</p> <p>However, it’s important to note that during fine-tuning, LoRA needs to store additional parameters for \(A\) and \(B\), resulting in some extra memory overhead compared to regular models that only store pre-trained weights. This trade-off allows LoRA to efficiently adapt to new tasks while preserving the knowledge gained during pre-training.</p> <h3 id="matrix-updates">Matrix Updates</h3> <p>In a low-rank update scenario, if the size of the original weight matrix \(W\) is \(d \times k\), and the input vector has size \(k\), the matrices \(A\) and \(B\) typically have dimensions:</p> <ul> <li>\(A: r \times k\), where \(r\) is the rank of matrix \(W\).</li> <li>\(B: d \times r\). This arrangement maintains compatibility with the original weight matrix while introducing a low-rank structure to the update \(\triangle W\).</li> </ul> <h3 id="inference">Inference</h3> <p>During inference for downstream tasks, the weight matrix used is \(W_0 + AB\), where \(W_0\) represents the weight matrix from the pre-training phase. This combination leverages the knowledge gained during pre-training and adapts it to the specific downstream task.</p> <h2 id="conclusion">Conclusion</h2> <p>In conclusion, Low-Rank Adaptation (LoRA) emerges as a game-changing technique for the world of deep learning, particularly in the realm of large language models. This paper has unpacked the essence of LoRA, shedding light on how it leverages the inherent low-rank structure of weight matrices to dramatically reduce the computational and memory requirements during fine-tuning. Unlike some existing approaches, LoRA strives to minimize inference latency while maintaining the efficacy of fine-tuning. As we navigate the ever-evolving landscape of artificial intelligence, LoRA’s contribution stands out as an elegant and efficient way to adapt pre-trained models to a plethora of real-world applications. This paper’s exploration of LoRA not only provides valuable insights into the future of model adaptation but also sparks discussions on the ways we can continue to make AI more accessible, resource-friendly, and capable.</p>]]></content><author><name></name></author><category term="papers"/><category term="nlp"/><summary type="html"><![CDATA[LoRA Paper Review]]></summary></entry><entry><title type="html">Paper Review - Week 36</title><link href="https://shirintahmasebi.github.io/blog/2023/week-36/" rel="alternate" type="text/html" title="Paper Review - Week 36"/><published>2023-09-10T00:00:00+00:00</published><updated>2023-09-10T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2023/week-36</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2023/week-36/"><![CDATA[<p>Here is the list of the most interesting papers published in this week:</p> <ul> <li><a href="/blog/2023/week-36/#large-language-models-as-optimizers">Large Language Models as Optimizers</a></li> </ul> <hr/> <h2 id="large-language-models-as-optimizers"><a href="https://arxiv.org/pdf/2309.03409.pdf">Large Language Models as Optimizers</a></h2> <h3 id="introduction">Introduction</h3> <p>In this paper, the authors are using LLMs to solve optimization problems. The fundamental idea is to use optimized prompts as input for the language model, rather than explicitly constructing complex mathematical models or relying on conventional solvers like CPLEX or Gurobi. This inspiring approach simplifies how optimization problems are presented to the model, providing flexibility and reducing the requirement for domain-specific expertise. It also demonstrates that language models can effectively handle diverse optimization problems and may outperform traditional heuristic methods in certain cases.</p> <h3 id="research-gap">Research Gap</h3> <p>The advantages of using LLMs for such purpose can be considered as follows:</p> <ul> <li>By using optimized prompts, the need for creating complex mathematical models or constraints for optimization problems is reduced. This simplifies the way optimization problems are presented to the model.</li> <li>Solving complex optimization problems requires expertise in both the problem domain and optimization techniques. Using language models with optimized prompts potentially requires less domain-specific knowledge, making optimization more accessible.</li> <li>The authors also demonstrate that LLMs can perform well on various optimization problems, even outperforming traditional heuristic approaches in some cases.</li> <li>The optimized prompts found for one problem can sometimes be applied to similar problems, as demonstrated in the transferability experiments.</li> </ul> <p>Generally, the authors demonstrated that using LLMs with optimized prompts is a promising approach for addressing optimization problems, offering simplicity, flexibility, and potential benefits over traditional methods. However, it’s important to note that this approach might not be suitable for all optimization problems and has its own limitations, as mentioned in the research.</p> <h3 id="solution-orpo-framework">Solution: ORPO Framework</h3> <p>The researchers propose a framework named OPRO. In this framework, they create a meta-prompt. The meta-prompt consists of two key components:</p> <ul> <li>Task Description: This part of the meta-prompt provides a description of the optimization problem to be solved. It typically includes problem-specific details.</li> <li>Generated Prompts and Corresponding Accuracy Pairs: This part consists of pairs of generated prompts (candidate instructions) and their corresponding accuracy scores. These prompts are generated by the language model during the optimization process. They represent potential instructions for solving the problem.</li> </ul> <p style="text-align:center;"><img src="/assets/img/weekly-review/orpo_framework_architecture.png" alt="The Architecture" width="350" height="400"/></p> <p>Now, we can review the whole optimization process and its steps; To make the whole process as clear as possible, let us leverage an example. Let us say we have an optimization problem related to scheduling a set of tasks. So, we want to find the most efficient way to assign these tasks to workers while minimizing the overall time taken.</p> <p><strong>Step 1:</strong> The optimization process is initiated by providing the meta-prompt to an LLM. As mentioned earlier the meta-prompt contains two parts: task description and generated prompts and their corresponding accuracy pair. For example, the meta prompt can be:</p> <ul> <li>Task Description: You need to assign these tasks to workers while minimizing the total time taken. The tasks have different durations and dependencies.</li> <li>Pairs of Prompts and Their Accuracy: <ul> <li>“Minimize time by finding the optimal task assignment.” (Prompt) - 80% accuracy</li> <li>“Distribute tasks efficiently to reduce total time.” (Prompt) - 75% accuracy</li> <li>“Schedule tasks to minimize the overall duration.” (Prompt) - 85% accuracy</li> </ul> </li> </ul> <p>The LLM takes in the entire context, including the task description and the pairs of prompts and accuracy scores. It uses the information in the meta-prompt to generate instructions and attempts to solve the optimization problem. For example, let us assume that the LLM generate the following instruction and solution:</p> <ul> <li>Instruction: Optimize the task assignment for minimal time.</li> <li>Solution: Assign task A to worker 1, task B to worker 2, and task C to worker 3 for the shortest completion time.</li> </ul> <p><strong>Step 2:</strong> Now, the generated solution should be evaluated to measure its accuracy. The accuracy of the provided solution is typically measured by comparing it to the known or optimal solution for the given optimization problem. If the problem is a mathematical optimization task, accuracy could be determined by calculating the error or the deviation of the LLM’s solution from the known optimal solution. For example, if the LLM is solving the Traveling Salesman Problem, accuracy could be measured as the percentage difference between the length of the path found by the LLM and the length of the shortest path possible. The smaller the difference, the higher the accuracy. Let us say that, in our example, the accuracy is 87%.</p> <p><strong>Step 3:</strong> The pair of the LLM-generated prompt and its corresponding accuracy score is added to the meta-prompt. In each optimization step, this new information is appended to the existing meta-prompt. So, here, the updated meta-prompt would be:</p> <ul> <li>Task Description: You need to assign these tasks to workers while minimizing the total time taken. The tasks have different durations and dependencies.</li> <li>Pairs of Prompts and Their Accuracy: <ul> <li>“Minimize time by finding the optimal task assignment.” (Prompt) - 80% accuracy</li> <li>“Distribute tasks efficiently to reduce total time.” (Prompt) - 75% accuracy</li> <li>“Schedule tasks to minimize the overall duration.” (Prompt) - 85% accuracy</li> <li>“Optimize the task assignment for minimal time.” (Prompt) - 87% accuracy</li> </ul> </li> </ul> <p><strong>Step 4:</strong> Now, the updated meta-prompt is used to repeat steps 2 and 3. This process continues until one of the following conditions is met:</p> <ul> <li>The LLM’s generated solutions reach a satisfactory level of accuracy, and further optimization does not significantly improve the results</li> <li>A predefined maximum number of optimization steps is reached.</li> </ul> <h3 id="experiments">Experiments</h3> <p>The authors focus on two optimization problems for their experiments:</p> <ul> <li>Prompt Optimization for Natural Language Tasks: This part focuses on optimizing prompts for natural language tasks. They conduct experiments on datasets such as GSM8K (grade school math word problems) and Big-Bench Hard (BBH), which consists of various challenging tasks. The primary objective is to optimize prompts for these datasets to maximize task accuracy. The OPRO framework was quite successful in optimizing prompts for various natural language tasks. Instructions generated through prompt optimization improved task accuracy significantly compared to initial or baseline prompts. The instructions generated by OPRO demonstrated high transferability and outperformed baseline prompts on different datasets within the same domain.</li> <li>Traveling Salesman Problem (TSP): The researchers also apply their framework, OPRO, to the Traveling Salesman Problem, a classic combinatorial optimization problem. They evaluate the performance of OPRO in comparison to heuristic algorithms and measure the optimality gap. The OPRO framework, when applied to the TSP, demonstrated promising results on smaller-scale TSP problems. However, its performance on larger-scale TSP problems degraded, with heuristics outperforming the language models.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>This paper presents an innovative approach that leverages Large Language Models (LLMs) to address optimization problems through prompt optimization. The OPRO framework simplifies the presentation of optimization tasks to LLMs, potentially reducing the need for domain-specific expertise and outperforming traditional heuristics in specific cases. The experiments conducted on natural language tasks demonstrate OPRO’s success in improving task accuracy and transferability, while its performance in solving the Traveling Salesman Problem (TSP) varies, excelling in smaller-scale instances but being outperformed by heuristics in larger-scale problems. This approach showcases the promise of LLMs in optimization but is subject to domain-specific constraints and varying levels of effectiveness.</p>]]></content><author><name></name></author><category term="papers"/><category term="weekly-review"/><category term="nlp"/><summary type="html"><![CDATA[Top NLP Papers Published from September 04 to September 10]]></summary></entry><entry><title type="html">Selective Fairness in Recommendation via Prompts</title><link href="https://shirintahmasebi.github.io/blog/2023/selective-fairness-in-recommenders/" rel="alternate" type="text/html" title="Selective Fairness in Recommendation via Prompts"/><published>2023-09-06T00:00:00+00:00</published><updated>2023-09-06T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2023/selective-fairness-in-recommenders</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2023/selective-fairness-in-recommenders/"><![CDATA[<p>In this post, I am going to review the <a href="https://arxiv.org/pdf/2205.04682.pdf">PFRec</a> paper. The paper focuses on the challenge of recommendation fairness, particularly in scenarios where users have multiple sensitive attributes, such as age, gender, or country, and they may want to customize the fairness of their recommendations based on these attributes.</p> <p>The key contribution of this work is the introduction of a selective fairness task, where users can flexibly choose which sensitive attributes should be considered in making recommendations. To address this, the paper proposes a novel parameter-efficient framework called Prompt-based Fairness-Aware Recommendation (PFRec), which leverages Transformer-based models and adapters for fairness-aware tuning. The contributions of the paper include highlighting the challenges of selective fairness, introducing personalized attribute-specific prompts, and demonstrating the effectiveness of PFRec in various fairness scenarios.</p> <p>It’s important to note that the paper distinguishes between two main phases: pre-tuning and fine-tuning. Pre-tuning focuses on training a Transformer-based model to predict the next user interaction by processing user interactions one at a time. During this phase, the Binary Cross-Entropy (BCE) loss is used to update the weights of the Transformer.</p> <p>In the pre-tuning phase, the Transformer processes user interactions sequentially, aiming to predict the next interaction in the sequence. The BCE loss is employed to update the weights of the Transformer. The Transformer architecture typically consists of multiple layers and attention mechanisms, and it captures sequential patterns in user behavior.</p> <p>Following the pre-tuning phase, the Transformer’s weights are frozen, and the fine-tuning phase, which is bias-aware, begins. The goal of fine-tuning is to address fairness concerns, specifically for the \(k\)-th combination of user-sensitive attributes. To achieve this, a prompt is generated based on the k-th attributes and passed to an adapter-augmented Transformer.</p> <p>The adapter is introduced as a linear transformation applied to the LayerNorm of the Transformer’s input. This adapter architecture significantly reduces the number of parameters that need updating compared to fine-tuning the entire Transformer. The adapter’s parameters are optimized to make the user representation produced by the adapter-augmented Transformer as bias-free as possible with respect to the selected attributes.</p> <p>To further enhance fairness, an adversarial training component is introduced. An adversary classifier takes the user representation generated by the adapter-augmented Transformer and attempts to predict the user’s sensitive attributes based on this representation. The loss of the adversary classifier, along with the BCE loss of the adapter-augmented Transformer, forms the overall loss used to update the adapter. Adversarial training encourages the adapter to reduce biases in the user representation, as a successful adversary indicates remaining biases.</p> <p>In summary, the paper presents a novel framework, PFRec, which allows for selective fairness customization in recommendation systems. It achieves this through a two-phase approach, where pre-tuning focuses on sequence prediction and fine-tuning introduces adapters and adversarial training to address bias-aware fairness concerns.</p> <p>Now, in what follows, the used datasets and the experiments are explained. The experiments in the paper are conducted using two public datasets: <strong>CIKM</strong> and <strong>AliEC</strong>. The CIKM dataset is an E-commerce recommendation dataset. It has 88,000 items and 60,000 users with 2.1 million click instances. Each user has 3 attributes: gender, age, consumption level. The AliEC dataset contains nearly 100,000 users and 8.8 million click instances. Each user has two attributes gender and age.</p> <p>Two benchmars are designed for evaluation:</p> <ul> <li> <p><strong>Single-Attribute Fairness:</strong> The paper evaluates the proposed PFRec framework on both single-attribute fairness settings and multi-attribute fairness settings. Single-attribute fairness focuses on considering the fairness of recommendations with respect to a single user-sensitive attribute, such as gender or age.</p> </li> <li> <p><strong>Multi-Attribute Fairness:</strong> In addition to single-attribute fairness, the paper also examines multi-attribute fairness settings. This means that the fairness-aware modeling can take into account multiple user-sensitive attributes simultaneously to provide fairness in recommendations.</p> </li> </ul> <p>The paper compares the performance of PFRec with several baselines, including:</p> <ul> <li><strong>SASRec:</strong> This is the same pre-training model used for learning user representations, without any fairness considerations. It serves as a baseline for evaluating PFRec’s fairness-aware capabilities.</li> <li><strong>BERT4Rec:</strong> BERT4Rec is mentioned as another strong BERT-based sequential recommendation model. While the paper doesn’t provide specific details, it implies that BERT4Rec is one of the models used for comparison with PFRec.</li> </ul> <p>Performance measurement metrics can be divided into two categories:</p> <ol> <li>Accuracy Metrics: <ul> <li> <p><strong>HIT@N</strong>: HIT@N measures the accuracy of the top-N recommendations. It calculates how many of the user’s actual interactions are included in the top-N recommended items.</p> </li> <li> <p><strong>NDCG@N</strong> (Normalized Discounted Cumulative Gain): NDCG@N is another ranking-based metric that evaluates the quality of the top-N recommendations. It takes into account both the relevance and ranking of recommended items.</p> </li> </ul> </li> <li>Fairness Metrics: <ul> <li><strong>F1</strong>: To evaluate fairness performance, the paper employs micro-averaged F1 of the adversarial classifier. This metric is used to measure how well the model achieves fairness in recommendations. Since the adversarial classifier is used to measure F, smaller values of F1 indicate better fairness performance.</li> </ul> </li> </ol>]]></content><author><name></name></author><category term="papers"/><category term="nlp"/><summary type="html"><![CDATA[Selective Fairness Paper Review]]></summary></entry><entry><title type="html">Paper Review - Week 35</title><link href="https://shirintahmasebi.github.io/blog/2023/week-35/" rel="alternate" type="text/html" title="Paper Review - Week 35"/><published>2023-09-03T00:00:00+00:00</published><updated>2023-09-03T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2023/week-35</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2023/week-35/"><![CDATA[<p>Here is the list of the most interesting papers published in this week:</p> <ul> <li><a href="/blog/2023/week-35/#recmind-large-language-model-powered-agent-for-recommendation">RecMind: Large Language Model Powered Agent For Recommendation</a></li> </ul> <hr/> <h2 id="recmind-large-language-model-powered-agent-for-recommendation"><a href="https://arxiv.org/pdf/2308.14296.pdf">RecMind: Large Language Model Powered Agent For Recommendation</a></h2> <h3 id="introduction">Introduction</h3> <p>The paper is introducing a novel recommendation system called RecMind that utilizes Large Language Models (LLMs) as its core component. The system integrates various tools, such as a database tool for accessing domain-specific knowledge, a search tool for real-time information, and a text summarization tool. RecMind also employs a planning mechanism that breaks down complex recommendation tasks into smaller, manageable sub-goals. One of the key highlights of the paper is the introduction of the Self-Inspiring (SI) planning algorithm, which enables the system to consider all previous planning paths at each intermediate step, leading to better decision-making and recommendations.</p> <h3 id="research-gap">Research Gap</h3> <p>This paper focuses of addressing the following research gaps:</p> <ul> <li> <p><strong>Limited Use of External Knowledge:</strong> Most existing recommendation systems often struggle to effectively utilize external knowledge due to constraints in model scale and data size. RecMind includes a database tool, a search tool, and a text summarization tool. These tools enable RecMind to access external knowledge beyond what is inherently present in the LLM’s parameters.</p> </li> <li> <p><strong>Lack of Generalizability:</strong> Current recommendation methods are often designed for specific tasks and struggle to generalize to unseen recommendation scenarios.RecMind incorporates a planning mechanism that breaks down complex tasks into smaller, manageable sub-goals. This planning approach enables RecMind to efficiently handle diverse recommendation scenarios, thereby enhancing its generalizability across various tasks.</p> </li> <li> <p><strong>Underutilization of Reasoning Capabilities:</strong> The reasoning capabilities of LLMs are not fully leveraged in existing research, leading to suboptimal performance in complex recommendation tasks.RecMind proposes the Self-Inspiring (SI) planning algorithm, which allows RecMind to consider all previous planning paths at each intermediate step. By retaining and utilizing historical planning information, RecMind’s SI algorithm significantly enhances the model’s ability to comprehend complex recommendation tasks and make better-informed decisions.</p> </li> </ul> <h3 id="solution-recmind">Solution: RecMind</h3> <p>RecMind is composed of three components: <strong>Planning Component</strong>, <strong>Memory Component</strong>, and <strong>Tools</strong>. Let us have a brief overview of each of these components:</p> <ul> <li><strong>Planning Component:</strong> The planning component is pivotal in breaking down the entirety of the recommendation task into more manageable sub-goals. This process is facilitated by the implementation of the Self-Inspiring (SI) algorithm, which enables the system to iteratively plan and execute tasks by considering previous states.</li> <li><strong>Memory Component:</strong> The memory component is divided into two categories: <strong>Personalized Memory</strong> and <strong>World Knowledge</strong>. Personalized Memory encompasses the historical interactions of the user, thereby enabling RecMind to understand individual preferences and behaviors. On the other hand, World Knowledge encompasses a broader spectrum, incorporating metadata about items as well as real-time data sourced from the web. This collective memory framework empowers RecMind with a profound understanding of user preferences and item characteristics.</li> <li><strong>Tools:</strong> The tools component plays a crucial role in providing the necessary data for the World Knowledge segment. This component can access various tools, including Database Tools, Search Tools, and Text Summarization Tool. These tools serve the purpose of enriching RecMind’s knowledge base, allowing it to efficiently acquire external information and enhance the depth and precision of its recommendations.</li> </ul> <p>Here is a brief overview of the architecture of RecMind:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/recmind_architecture.png" alt="The Architecture" width="850" height="500"/></p> <h4 id="self-inpiring-si-algorithm">Self-Inpiring (SI) Algorithm</h4> <p>This algorithm is inspired by the Tree-of-Thoughts approach. So, let us first define SI: The SI algorithm is a novel planning technique that allows the system to consider all previous planning paths at each intermediate step. It generates better planning by integrating multiple reasoning paths, retaining all previous states from the history of paths when generating a new state. SI focuses on utilizing all historical states to provide more comprehensive and informed decision-making, thereby improving the overall planning ability of RecMind.</p> <p>To compare SI with ToT, both methods fundamentally involve the exploration of a tree of possible paths or thoughts. However, the crucial distinction lies in the approach to traversal within this tree.</p> <p>In the ToT method, various traversal strategies like BFS or DFS are used to navigate the tree, enabling the consideration of different paths but possibly leading to the exclusion of certain historical states or paths based on the selected search strategy.</p> <p>Conversely, the SI algorithm conducts an exhaustive search over the entire tree, ensuring that all historical states and paths are thoroughly explored and considered during the recommendation process. This exhaustive search approach in SI aims to leverage all available historical information, leading to a more comprehensive decision-making process.</p> <p>Therefore, while both approaches involve navigating a tree of potential paths, the distinction lies in the traversal methodology, with ToT employing various traversal strategies and SI conducting a comprehensive exploration of the entire tree.</p> <p>A comparison of SI and ToT is depicted in the following figure:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/recmind_si_vs_tot.png" alt="The Architecture" width="550" height="500"/></p> <p>Also, an example scenario for prompting in each of these approaches is mentioned in the following figure:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/recmind_prompting_example.png" alt="The Architecture" width="850" height="500"/></p> <h3 id="experiments">Experiments</h3> <p>The experiments conducted in the paper aimed to evaluate the performance of the proposed recommendation system, RecMind, in various recommendation tasks: sequential recommendation, point-wise, list-wise tasks . The primary aspects of the experimental setup are outlined below:</p> <p>The experimental setup involved the use of three popular datasets sourced from Amazon, namely Sports &amp; Outdoors, Beauty, and Toys &amp; Games, each containing specific user IDs, item IDs, ratings, textual reviews, and timestamps. The datasets were divided into training, validation, and testing sets.</p> <p>The evaluation metrics utilized were Hit Ratio (HR), Normalized Discounted Cumulative Gain (NDCG) for sequential and top-N recommendation, and BLEU and ROUGE for generated explanations. Also, the core LLM used in RecMind is GPT3.5.</p> <p>The performance of RecMind was compared with multiple baselines, including P5, ChatGPT (zero-shot and few-shot), and RecMind-CoT, as well as RecMind-ToT with BFS and DFS. Additional baselines were considered for each task to ensure comprehensive evaluation.</p> <h3 id="conclusion">Conclusion</h3> <p>In conclusion, this paper introduces RecMind, a comprehensive recommendation system that leverages the capabilities of the large language model GPT-3.5. By incorporating a Planning component that decomposes the recommendation task into manageable subgoals using the Self-Inspiring (SI) algorithm, and a Memory component that comprises both Personalized Memory and World Knowledge, RecMind demonstrates a robust understanding of user-item interactions. The utilization of various Tools, including Database Tools, Search Tools, and a Text Summarization Tool, enables RecMind to access and process the necessary data from diverse sources. Through extensive experiments on diverse recommendation tasks, including sequential recommendation, point-wise, list-wise recommendation, explanation generation, and review summarization, RecMind’s performance is compared with several baselines, showcasing its effectiveness in providing precise and explainable recommendations across various scenarios. The results demonstrate the potential of RecMind in addressing the challenges of recommendation systems and improving user satisfaction and engagement.</p>]]></content><author><name></name></author><category term="papers"/><category term="weekly-review"/><category term="nlp"/><summary type="html"><![CDATA[Top NLP Papers Published from August 28 to September 03]]></summary></entry><entry><title type="html">Paper Review - Week 34</title><link href="https://shirintahmasebi.github.io/blog/2023/week-34/" rel="alternate" type="text/html" title="Paper Review - Week 34"/><published>2023-08-27T00:00:00+00:00</published><updated>2023-08-27T00:00:00+00:00</updated><id>https://shirintahmasebi.github.io/blog/2023/week-34</id><content type="html" xml:base="https://shirintahmasebi.github.io/blog/2023/week-34/"><![CDATA[<p>Here is the list of the most interesting papers published in this week:</p> <ul> <li><a href="/blog/2023/week-34/#llmrec-benchmarking-large-language-models-on-recommendation-task">LLMRec: Benchmarking Large Language Models on Recommendation Task</a></li> </ul> <hr/> <h2 id="llmrec-benchmarking-large-language-models-on-recommendation-task"><a href="https://arxiv.org/pdf/2308.12241.pdf">LLMRec: Benchmarking Large Language Models on Recommendation Task</a></h2> <h3 id="introduction">Introduction</h3> <p>Despite the developments in LLMs, the integration of LLMs into the domain of recommendation systems has not been thoroughly explored. This research aims to bridge this gap by proposing LLMRec, an LLM-based recommender system crafted for benchmarking LLMs across various recommendation tasks.</p> <h3 id="solution-llmrec">Solution: LLMRec</h3> <p>The key steps involved in the LLMRec system are:</p> <ul> <li><strong>Prompt Generation:</strong> Creating task-specific prompts using modules like task description, behavior injection, and format indicator modules. It seems to utilize template-based prompts for the different recommendation tasks, and it doesn’t explicitly mention an approach for optimizing the prompts themselves.</li> <li><strong>Model Processing:</strong> Passing the created prompts to the LLMs, with the option of fine-tuning the models when necessary.</li> <li><strong>Output Refinement:</strong> Evaluating the results using the output refinement module, which checks and refines the output of the LLMs, particularly addressing any issues related to hallucination or non-compliance with the desired output format. It doesn’t explicitly mention specific guidelines for the output refinement module. However, the concept of the output refinement module implies the need for a mechanism that can check and correct the output of the LLMs, particularly when it comes to hallucinations or outputs that do not meet the desired format or quality.</li> <li><strong>Evaluation:</strong> Comparing the performance of the LLMs with various benchmarks and baseline methods for different recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization.</li> </ul> <p>An overview of the architecture of LLMRec is depicted in the following figure:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/llmrec_benchmark_architecture.png" alt="The Architecture" width="850" height="500"/></p> <h3 id="experiments">Experiments</h3> <p>The primary aspects of the experimental setup are outlined below:</p> <ul> <li><strong>Benchmarks:</strong> The benchmarks include various recommendation tasks, such as rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. The system evaluates the performance of off-the-shelf LLMs, including ChatGPT, ChatGLM, LLaMA, and Alpaca, and compares their results with state-of-the-art methods and baseline models for each specific recommendation task.</li> <li><strong>Datasets:</strong> The study primarily focuses on evaluating the Beauty category within the Amazon dataset, which comprises customer review text and associated metadata for products.</li> <li><strong>Evaluation Metric:</strong> For <strong>rating prediction</strong>, the evaluation metrics used are Root Mean Square Error (RMSE) and Mean Absolute Error (MAE). For <strong>sequential and direct recommendation tasks</strong>, the evaluation metrics include HR and NDCG. For <strong>explanation generation and review summarization</strong>, the evaluation metrics involve BLEU and ROUGE.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>In conclusion, this study has shed light on the potential of LLMs in the field of recommendation systems. Through the development and evaluation of the LLMRec system, they have demonstrated that LLMs, while exhibiting limited proficiency in certain accuracy-based tasks, showcase significant promise in explainability-based tasks.</p>]]></content><author><name></name></author><category term="papers"/><category term="weekly-review"/><category term="nlp"/><summary type="html"><![CDATA[Top NLP Papers Published from August 21 to August 27]]></summary></entry></feed>