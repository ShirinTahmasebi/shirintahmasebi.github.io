<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> SafeEdit | Shirin Tahmasebi </title> <meta name="author" content="Shirin Tahmasebi"> <meta name="description" content="Detoxifying Large Language Models via Knowledge Editing "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?ce48ee9bc248ee6b18f3aeefc03ed2f9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shirintahmasebi.github.io/bias/2024/safe_edit/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Shirin Tahmasebi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">CV </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">Publications</a> <a class="dropdown-item " href="/background/">Background</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Notes </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/bias/">Bias</a> <a class="dropdown-item " href="/tokenization/">Tokenization</a> <a class="dropdown-item " href="/llm_judge/">LLM as Evaluator</a> <a class="dropdown-item " href="/prompt/">Prompt Engineering</a> <a class="dropdown-item " href="/compression/">LLM Compression</a> <a class="dropdown-item " href="/agents/">LLM Agents</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/recsys/">Recommendation Systems</a> <a class="dropdown-item " href="/other/">Others</a> </div> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">SafeEdit</h1> <p class="post-meta"> March 24, 2024 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2024   ·   <i class="fa-solid fa-hashtag fa-sm"></i> llm   <i class="fa-solid fa-hashtag fa-sm"></i> bias   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#detoxifying-large-language-models-via-knowledge-editing">Detoxifying Large Language Models via Knowledge Editing</a> <ul> <li class="toc-entry toc-h3"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h3"><a href="#research-gap">Research Gap</a></li> <li class="toc-entry toc-h3"> <a href="#solution">Solution</a> <ul> <li class="toc-entry toc-h4"><a href="#safeedit">SafeEdit</a></li> <li class="toc-entry toc-h4"><a href="#dinm">DINM</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#experiments">Experiments</a></li> <li class="toc-entry toc-h3"><a href="#conclusion">Conclusion</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="detoxifying-large-language-models-via-knowledge-editing"><a href="https://arxiv.org/pdf/2403.14472.pdf" rel="external nofollow noopener" target="_blank">Detoxifying Large Language Models via Knowledge Editing</a></h2> <h3 id="introduction">Introduction</h3> <p>This paper addresses the critical need for robust methodologies to detoxify LLMs, ensuring their outputs remain safe and constructive across diverse applications. They introduce the concept of knowledge editing as a targeted approach to mitigate the generation of unsafe content, with a focus on a novel method named Detoxifying with Intraoperative Neural Monitoring (DINM). By analyzing and adjusting the internal knowledge of LLMs, DINM aims to reduce toxicity at its source without compromising the models’ overall performance. They also developed SafeEdit, a comprehensive benchmark designed to evaluate the effectiveness of detoxification techniques across a spectrum of unsafe scenarios.</p> <h3 id="research-gap">Research Gap</h3> <p>The research gap addressed in the paper is the challenge of detoxifying LLMs to prevent them from generating toxic (unsafe) responses to user inputs. The focus is on modifying the LLM itself so that it inherently avoids producing toxic responses while maintaining its ability to answer safe questions effectively. While existing methods (such as Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and Direct Preference Optimization (DPO)) have made some progress in making LLMs safer, they often fail to adequately handle carefully crafted adversarial inputs that can elicit harmful content. This vulnerability poses a significant risk, especially as LLMs become more integrated into everyday applications. The paper identifies a critical need for more effective and efficient methods that can permanently adjust the internal knowledge and parameters of LLMs to reduce toxicity across different inputs.</p> <p>The paper introduces two key contributions: the <strong>SafeEdit</strong> benchmark and the <strong>Detoxifying with Intraoperative Neural Monitoring (DINM)</strong> method. SafeEdit provides a comprehensive framework for evaluating the effectiveness of different detoxification approaches across nine unsafe categories, considering both the ability to mitigate toxic responses and the impact on general model performance. The DINM method represents a novel approach to LLM detoxification, leveraging knowledge editing techniques to make targeted adjustments to the model’s parameters, specifically within identified toxic regions. This method not only demonstrates a high degree of effectiveness in reducing toxicity with minimal impact on the LLM’s overall capabilities but also offers significant improvements in efficiency, requiring fewer resources and steps compared to traditional methods. Generally, this paper offers a promising approach for enhancing the safety and reliability of LLMs, marking a significant step forward in the topic of responsible AI.</p> <h3 id="solution">Solution</h3> <p>Their solution is divided into two parts: the <strong>SafeEdit</strong> benchmark and the <strong>Detoxifying with Intraoperative Neural Monitoring (DINM)</strong> method.</p> <h4 id="safeedit">SafeEdit</h4> <p>The SafeEdit framework is designed for evaluating the effectiveness of different detoxification approaches on LLMs. The process of creating such framework is:</p> <ol> <li> <p><strong>Categories of Unsafety</strong>: They defined nine distinct categories of unsafe content (e.g., bias, ethics, political, etc) to ensure a comprehensive coverage of the types of harmful content an LLM might generate. This categorization helps in systematically addressing the wide spectrum of toxicity.</p> </li> <li> <p><strong>Generating Unsafe Questions</strong>: For each of the nine unsafety categories, the LLM (GPT-4, in this case) is tasked to generate 60 unique questions that violate OpenAI’s usage policy. This step is crucial as it leverages the model’s capability to generate diverse instances of potential real-world unsafe queries.</p> </li> <li> <p><strong>Collection of Adversarial Prompts</strong>: The authors collected 48 attack prompts from various sources, including recent papers and handwritten sources. These prompts are designed to trick the LLM into generating responses that might reveal security vulnerabilities or produce harmful content.</p> </li> <li> <p><strong>Reevaluation by Humans</strong>: Human evaluation plays a critical role in ensuring the quality and effectiveness of both the harmful questions and the adversarial prompts. This step ensures that the dataset accurately represents a wide range of potential security threats.</p> </li> <li> <p><strong>Response Generation</strong>: The process of generating responses involves two rounds. In the first round, the LLM is instructed to respond to the harmful question by refusing to fulfill the request (<em>safe</em> response). In the second round, the goal is to elicit an <em>unsafe</em> response from the LLM, without the prior instruction to avoid harm.</p> </li> <li> <p><strong>Dataset Construction</strong>: The concatenation of harmful questions and attack prompts, along with their corresponding safe and unsafe responses, constitutes the dataset referred to as \(D_{edit}\). This dataset forms the basis for evaluating the detoxification effectiveness of various approaches.</p> </li> <li> <p><strong>General Performance Evaluation (D_cons)</strong>: In addition to \(D_{edit}\), another dataset (\(D_{cons}\)) consisting of instruction-following instances is used to assess whether the detoxification process affects the LLM’s performance on general tasks. This is critical for ensuring that the process of detoxifying the model does not degrade its utility for safe requests.</p> </li> <li> <p><strong>Evaluation Goal</strong>: The ultimate objective is to modify the LLM in such a way that it generates safe responses to the potentially harmful queries in \(D_{edit}\), without compromising its ability to respond effectively and accurately to the benign queries in \(D_{cons}\).</p> </li> </ol> <p>This framework aims to provide a comprehensive and systematic means of assessing and improving the safety of LLMs in handling potentially harmful or unsafe queries. The following image represents an overview of the SafeEdit construction process:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/dtoxifying_llm_safeedit.png" alt="The SafeEdit Overview" width="850" height="250"></p> <h4 id="dinm">DINM</h4> <p>The DINM method is as follows:</p> <ol> <li> <p><strong>Identification of Toxic Region</strong>: DINM starts by receiving a safe response and an unsafe response corresponding to a potentially toxic question from \(D_{edit}\) and then compares the hidden states of the LLM in the corresponding layer. Through this comparison, DINM identifies the layer within the model that shows the greatest difference in hidden states between safe and unsafe responses. This layer is named the “toxic region,” and it’s worth mentioning that this identification is instance-specific, meaning it can vary from one instance (input) to another.</p> </li> <li> <p><strong>Tunable and Frozen Parameters</strong>: The model’s parameters are conceptually divided into two groups for the tuning process: the parameters within the identified toxic layer (\(W_{l_{toxic}}^{V}\)) which are subject to tuning, and the rest of the model’s parameters, which are kept frozen. This distinction ensures that the tuning process specifically targets reducing toxicity without broadly altering the model’s overall knowledge or capabilities.</p> </li> <li> <strong>Loss Function for Tuning</strong>: The loss function used to update the tunable parameters in the toxic layer is indeed a weighted sum of two components: <ul> <li>The <strong>negative log likelihood</strong> of generating the safe response given the toxic question. This part of the loss encourages the model to favor safe responses over unsafe ones.</li> <li>The <strong>KL divergence</strong> between the probability distributions generated by both the detoxified LLM (the one being tuned) and the regular LLM (the undetoxified one) for a non-toxic question from \(D_{cons}\). This part ensures that the detoxification process does not compromise the model’s ability to handle general, non-toxic content.</li> </ul> </li> <li> <strong>Objective</strong>: The overall objective of the tuning process is to adjust the toxic layer’s parameters so that the model becomes less likely to generate unsafe responses, while maintaining its performance on unrelated tasks. The dual-component loss function helps balance these goals by pushing the model towards safer outputs without losing its general utility.</li> </ol> <p>In summary, DINM offers a targeted approach to detoxify LLMs by identifying and tuning instance-specific toxic regions within the model, using a loss function that balances the reduction of toxic outputs with the preservation of general performance. The following image represents an overview of the DINM method:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/dtoxifying_llm_dinm.png" alt="The DINM Method" width="400" height="250"></p> <h3 id="experiments">Experiments</h3> <p>The main goal of the experiments is to assess how effectively the DINM method can detoxify LLMs, reducing their propensity to generate unsafe responses, while maintaining or minimizing impact on their general performance.</p> <ul> <li> <strong>Benchmark</strong>: The SafeEdit benchmark, which encompasses various unsafe scenarios categorized into nine types, is used to systematically evaluate the performance of different detoxification approaches.</li> <li> <strong>Metrics</strong>: The evaluation considers several metrics, including detoxification success rate, defense generalization across different unsafe inputs, and the impact on general performance (e.g., fluency, knowledge question answering, content summarization).</li> <li> <strong>Baselines</strong>: Several baseline methods are compared against DINM to benchmark its performance in detoxifying LLMs: <strong>SFT</strong>, <strong>RLHF</strong>, <strong>DPO</strong>, <strong>MEND</strong>, and <strong>Ext-Sub</strong>.</li> </ul> <p>The experiments demonstrate that DINM is particularly effective at detoxifying LLMs. It shows a significant improvement in reducing toxic outputs across various unsafe scenarios without severely impacting the model’s overall capabilities. Compared to traditional approaches like SFT, RLHF, and DPO, DINM offers a more targeted and efficient way to reduce toxicity. It achieves better results in detoxification success rates and defense generalization, indicating its superior ability to generalize across different types of unsafe content. While detoxification methods can potentially degrade a model’s performance on general tasks, DINM manages to maintain a balance, ensuring that the model’s ability to perform unrelated tasks remains largely unaffected.</p> <h3 id="conclusion">Conclusion</h3> <p>This paper has demonstrated the potential of knowledge editing, particularly through the Detoxifying with Intraoperative Neural Monitoring (DINM) method, as a promising avenue for reducing the toxicity of LLMs. By systematically identifying and adjusting the toxic regions within LLMs, DINM offers a promising approach to detoxification that balances safety with the maintaining the general performance of the model.</p> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Shirin Tahmasebi. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>