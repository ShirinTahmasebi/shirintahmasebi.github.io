<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Paper Review - Week 25 | Shirin Tahmasebi </title> <meta name="author" content="Shirin Tahmasebi"> <meta name="description" content="Top NLP Papers Published from June 19 to June 25 "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?ce48ee9bc248ee6b18f3aeefc03ed2f9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shirintahmasebi.github.io/blog/2023/week-25/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Shirin Tahmasebi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/background/">Background </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Paper Review - Week 25</h1> <p class="post-meta"> June 25, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/papers"> <i class="fa-solid fa-tag fa-sm"></i> papers</a>   <a href="/blog/category/weekly-review"> <i class="fa-solid fa-tag fa-sm"></i> weekly-review</a>   <a href="/blog/category/nlp"> <i class="fa-solid fa-tag fa-sm"></i> nlp</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#a-preliminary-study-of-chatgpt-on-news-recommendation-personalization-provider-fairness-fake-news">A Preliminary Study of ChatGPT on News Recommendation: Personalization, Provider Fairness, Fake News</a> <ul> <li class="toc-entry toc-h3"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h3"> <a href="#experiments">Experiments</a> <ul> <li class="toc-entry toc-h4"><a href="#personalization">Personalization</a></li> <li class="toc-entry toc-h4"><a href="#news-provider-fairness">News Provider Fairness</a></li> <li class="toc-entry toc-h4"><a href="#trustfulness-news-recommendation">Trustfulness News Recommendation</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#towards-open-world-recommendation-with-knowledge-augmentation-from-llms">Towards Open-World Recommendation with Knowledge Augmentation from LLMs</a> <ul> <li class="toc-entry toc-h3"><a href="#introduction-1">Introduction</a></li> <li class="toc-entry toc-h3"><a href="#research-gap">Research Gap</a></li> <li class="toc-entry toc-h3"> <a href="#solution-kar">Solution: KAR</a> <ul> <li class="toc-entry toc-h4"><a href="#factorization-prompting">Factorization Prompting</a></li> <li class="toc-entry toc-h4"><a href="#hybrid-expert-adapter">Hybrid-Expert Adapter</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#experiments-1">Experiments</a></li> <li class="toc-entry toc-h3"><a href="#conclusion">Conclusion</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <p>Here is the list of the most interesting papers published in this week:</p> <ul> <li><a href="/blog/2023/week-25/#a-preliminary-study-of-chatgpt-on-news-recommendation-personalization-provider-fairness-fake-news">A Preliminary Study of ChatGPT on News Recommendation: Personalization, Provider Fairness, Fake News</a></li> <li><a href="/blog/2023/week-25/#towards-open-world-recommendation-with-knowledge-augmentation-from-llms">Towards Open-World Recommendation with Knowledge Augmentation from LLMs</a></li> </ul> <hr> <h2 id="a-preliminary-study-of-chatgpt-on-news-recommendation-personalization-provider-fairness-fake-news"><a href="https://arxiv.org/pdf/2306.10702.pdf" rel="external nofollow noopener" target="_blank">A Preliminary Study of ChatGPT on News Recommendation: Personalization, Provider Fairness, Fake News</a></h2> <h3 id="introduction">Introduction</h3> <p>The paper primarily focuses on evaluating the performance of ChatGPT in the context of news recommendation from various perspectives, including personalized news recommendation, news provider fairness, and fake news detection. Additionally, the study examines the impact of different prompt formats on ChatGPT’s response generation and overall performance. While it does not explicitly target a specific research gap, it contributes to the broader understanding of the capabilities and limitations of large language models like ChatGPT in addressing crucial aspects of news recommendation. So, this review post only consists the Experiment section, where their leveraged strategies is briefly discussed.</p> <h3 id="experiments">Experiments</h3> <p>Their experiments are categorized into different sections based on the targeted tasks.</p> <h4 id="personalization">Personalization</h4> <p>They proposed three prompting approaches to evaluate personalisation:</p> <ol> <li> <p>The initial method involved simple textual prompting, where they listed the <strong>watched</strong> movies and the <strong>candidate</strong> items, prompting the recommender to choose one from the candidates. However, they noted instances where ChatGPT occasionally recommended an already-watched movie instead of selecting from the candidate list.</p> </li> <li> <p>In the second approach, the <strong>candidate</strong> movies were embedded into the prompt using the JSON format. In this approach, the <strong>watched</strong> movies are in a list format. The authors noted that there is still the chance of providing an already-watched movie as the recommendation.</p> </li> <li> <p>The third approach incorporated both the history of <strong>watched</strong> movies and the <strong>candidate</strong> movies in JSON format. The results show that in this prompt, the recommendations are always from the candidate list. So, this is the only approach that resulted in getting reasonable recommendations for personalization.</p> </li> </ol> <p style="text-align:center;"><img src="/assets/img/weekly-review/news_prompt_personalization.png" alt="The Architecture" width="750" height="250"></p> <h4 id="news-provider-fairness">News Provider Fairness</h4> <p>The authors aim to mitigate biases related to news provider fairness by explicitly addressing the concept of fairness in the prompts. They divide the news providers into two groups: popular and unpopular, and they use this division as a basis for constructing the prompts. The prompts include instructions regarding the specific numbers of news articles to be included from both popular and unpopular news providers. Additionally, in some of the prompt styles, they explicit mention of the lists of popular and unpopular providers. This approach helps to evaluate ChatGPT’s ability to provide recommendations that give equal opportunities to news articles from both popular and unpopular providers, thereby addressing any inherent biases in the recommendation process. A sample of these 6 prompt styles are depicted in the following figure:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/news_prompt_fairness_1.png" alt="The Architecture" width="750" height="200"></p> <p>The authors also introduced another group of prompting styles that explicitly instruct ChatGPT to consider fairness when generating recommendations. These prompts directly emphasize the concept of fairness, urging ChatGPT to ensure that the recommendations provided maintain an equitable balance between popular and unpopular news providers. The three prompting style proposed in this regard are as follows:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/news_prompt_fairness_2.png" alt="The Architecture" width="750" height="200"></p> <h4 id="trustfulness-news-recommendation">Trustfulness News Recommendation</h4> <p>In this task, the authors focus on how to ensure trustful news recommendation and mitigate the risk of recommending fake news. By including the news ID of the trusted or reliable news articles directly into the prompt, the researchers restrict the recommendation process to solely include articles that have been identified as trustworthy. This method effectively limits the scope of recommendations to only those articles that have met specific criteria for trustworthiness, thereby minimizing the potential for ChatGPT to generate or recommend fake news articles.</p> <h2 id="towards-open-world-recommendation-with-knowledge-augmentation-from-llms"><a href="https://arxiv.org/pdf/2306.10933.pdf" rel="external nofollow noopener" target="_blank">Towards Open-World Recommendation with Knowledge Augmentation from LLMs</a></h2> <h3 id="introduction-1">Introduction</h3> <p>With the emergence of large language models (LLMs) providing access to open-world knowledge, there exists a significant opportunity to revolutionize traditional closed recommendation systems. This paper introduces the Knowledge Augmented Recommendation (KAR) framework, designed to leverage the potential of LLMs in enriching recommendation systems. KAR emphasizes the integration of reasoning and factual knowledge extracted from LLMs to facilitate an in-depth understanding of user preferences and item characteristics. Through a three-stage architecture, KAR aims to bridge the gap between closed and open-world systems, enabling more accurate and personalized recommendations for users.</p> <h3 id="research-gap">Research Gap</h3> <p>This paper tries to address several gaps in existing research related to LLM-based recommendation systems and traditional recommendation systems. Some of the specific gaps include:</p> <ul> <li> <p><strong>Limited Knowledge Extraction:</strong> Traditional recommendation systems often rely on a limited set of features and knowledge, which may not fully capture the diverse preferences of users. The paper seeks to address this limitation by leveraging LLMs to extract a wider range of open-world knowledge beyond the original dataset.</p> </li> <li> <strong>Compositional Gap:</strong> The compositional gap refers to the limitations of LLMs in accurately generating the correct answer for complex and compositional questions. It implies that while LLMs might excel at answering simpler sub-questions, they may fail to correctly answer more complex queries that involve multiple reasoning steps. This gap arises due to the intricate and multifaceted nature of user preferences, which cannot always be accurately captured or represented by the LLMs in a single step.</li> <li> <strong>Performance Limitations due to Information Misalignment:</strong> The paper acknowledges that there can be a mismatch between the knowledge generated by LLMs and the inferred user preferences in recommendation systems. This misalignment can limit the overall performance of recommendation systems. The proposed framework aims to align the reasoning knowledge and the item factual knowledge to ensure that they are effectively utilized in recommendation processes.</li> <li> <strong>Computational- and Resource-intensive Inference:</strong> Given the large size and computational demands of LLMs, the inference process can be time-consuming and resource-intensive, making it challenging to meet the real-time latency requirements of large-scale recommendation systems. The paper introduces a speed-up approach that involves pre-storing the knowledge representation generated by the LLMs, thereby significantly reducing the inference overhead and enhancing the efficiency of the recommendation process.</li> </ul> <h3 id="solution-kar">Solution: KAR</h3> <p>The proposed solution is a recommendation system that leverages LLMs to access open-world knowledge, thus avoiding getting stuck within closed systems. The framework consists of three components, each serving a unique purpose:</p> <ul> <li> <p><strong>Knowledge Reasoning and Generation Stage:</strong> This phase primarily focuses on the extraction of comprehensive information about items and users through the utilization of LLMs. Let’s consider the scenario of movie recommendation as an example. By extracting detailed information about movies and establishing a more precise understanding of user preferences, we can significantly enhance the recommendation process. Specifically, the knowledge extracted about user preferences is referred to as Reasoning Knowledge, whereas the information extracted about the items is termed Factual Knowledge. To extract the reasoning and factual knowledge, a pioneering technique known as <em>factorization prompting</em> is introduced. This technique will be thoroughly explained in the subsequent sections.</p> </li> <li> <p><strong>Knowledge Adaptation Stage:</strong> Once the knowledge about items (reasoning knowledge) and users (factual knowledge) is extracted from LLMs, it must be properly formatted and adapted for integration into the recommendation systems. To facilitate this process, a cutting-edge <em>hybrid-expert</em> approach has been proposed, which will be comprehensively elaborated upon later.</p> </li> <li> <p><strong>Knowledge Utilization:</strong> The main point of this component is to effectively utilize the knowledge augmented vectors in the recommendation system to generate optimal outputs. These knowledge augmented vectors, derived from the reasoning augmented vector and the factual augmented vector, are seamlessly integrated into the recommendation model. They are treated as supplementary fields in the feature interaction layer of the recommendation model, allowing for explicit interaction with other categorical features. By incorporating these augmented vectors into the recommendation model, the system combines both open-world knowledge and recommendation-specific knowledge. This fusion enables the system to provide users with highly informed and deeply personalized recommendations.</p> </li> </ul> <p>An overview of the whole KAR system is depicted in the following figure:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/kar_architecture.png" alt="The Architecture" width="750" height="300"></p> <h4 id="factorization-prompting">Factorization Prompting</h4> <p>The factorization prompting approach is a key technique employed in the <strong>Knowledge Reasoning and Generation Stage</strong>. It is specifically designed to address the challenge of extracting precise and comprehensive knowledge from LLMs about user preferences and item characteristics.</p> <p>The factorization prompting approach is rooted in the concept of decomposing complex reasoning tasks into more manageable subproblems by identifying the major factors that contribute to user preferences and item attributes. By breaking down the decision-making processes into these essential factors, the LLMs can more effectively capture and generate nuanced <strong>Reasoning Knowledge</strong> and <strong>Factual Knowledge</strong>.</p> <p>For instance, in the context of movie recommendation, these factors might encompass various critical dimensions, including genre, actors, directors, theme, mood, and production quality. By explicitly incorporating these factors into the prompting mechanism, the LLMs can more accurately identify the key elements that influence user preferences.</p> <p>This factorization prompting strategy alleviates the compositional gap often observed in LLMs, allowing the system to better capture the diverse and multifaceted nature of user interests and preferences. Consequently, the generated Reasoning Knowledge and Factual Knowledge become more aligned and tailored to the specific requirements of the recommendation process, leading to more effective and personalized recommendations.</p> <p>To better illustrate the factorization prompting approach, let us have a look at the following figure. Also, it is worth mentioning that, in this figure, the term <em>Scenario-specific Factors</em> is used. Different scenarios might correspond to various domains such as movie recommendation, news recommendation, or product recommendation. Each of these domains involves distinct sets of considerations that influence user preferences and item characteristics. For instance, in movie recommendation, scenario-specific factors could include genre, actors, directors, theme, mood, and production quality, as these aspects significantly impact a user’s likelihood of being interested in a particular movie.</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/kar_factorization_prompt.png" alt="The Architecture" width="750" height="400"></p> <h4 id="hybrid-expert-adapter">Hybrid-Expert Adapter</h4> <p>The hybrid-expert adaptor in the proposed framework is based on Multi-Layer Perceptrons (MLPs) and is a crucial component in the <strong>Knowledge Adaptation</strong> stage of the system. It receives the <strong>reasoning knowledge</strong> and <strong>factual knowledge</strong> that have been extracted from the Knowledge Reasoning and Generation step. The adapters function to create refined representations of this knowledge, enhancing its compatibility with the recommendation system.</p> <p>There are three types of adapters (MLPs) utilized in this process. Firstly, there are the <strong>preference experts</strong> that exclusively utilize the reasoning knowledge. Secondly, the <strong>shared experts</strong> leverage both the reasoning knowledge and factual knowledge. Lastly, the <strong>item experts</strong> focus solely on the factual knowledge. By considering these different types of experts, the system can incorporate diverse forms of knowledge to enrich the final representations.</p> <p>To control the impact of each expert representation, learnable weights called <strong>gating networks</strong> are employed. These gating networks are instrumental in regulating the influence of the different expert types, ensuring a balanced and comprehensive integration of knowledge.</p> <p>Through a series of matrix multiplications and additions, the hybrid-expert adaptor generates two vectors: the reasoning augmented vector representing the user and the fact augmented vector representing the item. These vectors are then fed into the recommendation system to predict whether the item should be recommended to the user (i.e., point-wise recommendation). This comprehensive process allows the system to leverage a combination of specialized knowledge and provide more accurate and personalized recommendations.</p> <p>All of these adapters and learnable parameters are trained concurrently with the training of the recommendation system.</p> <h3 id="experiments-1">Experiments</h3> <p>The experiments conducted in the paper aimed to evaluate the performance of the proposed recommendation system, KAR, in sequential recommendation. The primary aspects of the experimental setup are outlined below:</p> <ul> <li> <strong>Datasets:</strong> The experiments were conducted on the MovieLens-1M dataset, which contains 1 million ratings from 6000 users for 4000 movies.</li> <li> <strong>Baseline Methods:</strong> The experiment compared the proposed KAR framework with several baseline methods, including P5, UniSRec, and VQ-Rec.</li> <li> <strong>Evaluation Metrics:</strong> The evaluation metrics used in the experiments were AUC and Binary Cross-Entropy Loss.</li> <li> <strong>Results:</strong> The results demonstrated that the KAR framework significantly improved the performance of the backbone models across various types of baseline models. It outperformed models based on text pretraining or user sequence pretraining, achieving a notable improvement in AUC and BCE. The experiments revealed the effectiveness of incorporating open-world knowledge from large language models (LLMs) into recommendation systems.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>In conclusion, the exploration and evaluation of the Knowledge Augmented Recommendation (KAR) framework underscore its significant contributions to the modern recommendation systems. By effectively harnessing the power of LLMs and integrating reasoning and factual knowledge, KAR has demonstrated substantial improvements in recommendation performance over diverse backbone models and baseline methods.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <p class="mb-2">Here are some more articles relevant to this one:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-27/">Paper Review - Week 27</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-23/">Paper Review - Week 23</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-35/">Paper Review - Week 35</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-46/">Paper Review - Week 46</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-10/">Paper Review - Week 10</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Shirin Tahmasebi. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>