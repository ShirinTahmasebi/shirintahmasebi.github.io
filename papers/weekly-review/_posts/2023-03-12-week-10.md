---
layout: post
title: Paper Review - Week 10
image: 
  path: /assets/img/weekly-review/nlp_cover_week_10.jpg
categories: [papers, weekly-review, nlp]
description: >
    Top NLP Papers Published from March 06 to March 12
sitemap: false
hide_last_modified: true
---


Several interesting papers are published in this week in NLP. Here is a list of them:
* [Large Language Models Are Human-Level Prompt Engineers][humanLevelPromptEngineeringSum]
* [PaLM-E: An Embodied Multimodal Language Model][palmeSum]


## [Large Language Models Are Human-Level Prompt Engineers][humanLevelPromptEngineeringPaper]

Recently, with the developed Large Langugae Models (LLM), it is common to view these LLMs as black-box and focus more on designing better prompts to be able to get a reasonable performance of these LLMs. Several forms of prompt engineering exist in the literature, including tuning differentiable soft prompts and tuning hard (natural language) prompts. The second approach, using hard prompts, is of significant interest due to its being understandable and human-friendly. However, a critical problem with this approach is that, to find a reasonable prompt, humans must repeat the experiments with a wide range of prompts to see which of them is the most compatible one for their languge model. Moreover, these prompts are not transferable to other LLMs, which means that this procedure should be repeated for working with each LLMs. So, the main focus of this paper is to provide an automatic approach for finding the best hard prompt without involving the human effort. Their solution is called __Automatic Prompt Engineer (APE)__.

Let us say that a task is specified by a dataset $$D_{train} = \{ (Q, A) \}$$ which is a set of input/output demonstrations. Considering $$M$$ as a prompted languge model and $$\rho$$ as a single instruction prompt, the input of $$M$$ can be represented as: $$[\rho;Q]$$, the output of which is represented by $$A$$. To measure how well $$M$$ performs, we can use a function named $$f$$ as the score function. This score function receives the prompt $$\rho$$, the question $$Q$$ as well as the answer $$A$$, and then returns the numerical score value. Thus, we can represent the score function as: $$f(\rho, Q, A)$$. The objective of APE is to find the best instruction prompts such that: 

$$
\rho^* = \arg\max_{\rho} f(\rho) = \arg\max_{\rho} E_{(Q,A)} [ f(\rho, Q, A) ]
$$


Now, the question is, how APE achieves this objective? APE is a three-step solution:

1. Generating a set of candidate prompts: In this step, several generative language models (such as T5, GLM, and InsertGPT) are selected and fed with a set of $$(Q,A)$$ as samples. Then, the language model is asked to generate an instruction for the provided examples. These generated instructions are considered as the candidate prompts. To generate these candidate prompts, different templates for input are used: Forward Generation Template and Reverse Generation Template. The example is represented in the following image:
  <p style="text-align:center;"><img src="/assets/img/weekly-review/ape_candidate_generation.png" alt="Candidate Generation Approaches" width="400" height="200"></p>

1. Refining and filtering the candidate prompts: To verify how well can these candidate prompts perform, two metrics are used for scoring them:
  * Execution Accuracy: A 0-1 loss function: $$f(\rho, Q, A) = 1 [M([\rho;A]) = A]$$.
  * Log Probability: $$log P(A|[\rho;Q])$$
  Each of the candidate prompts whose score functions are above a specific threshold will be selected in this steps.

1. Choosing the candidate with the highest score: This step is specifically useful when the filtered prompts are not enough! In this step, the filtered candidate prompts are passed to another LLM for generating a variation of the prompt. This step is like exploring the search space locally around the current best candidates. Among the generated variations, the ones with the highest score are selected as the final instruction prompt. 
  <p style="text-align:center;"><img src="/assets/img/weekly-review/ape_resampling.png" alt="Candidate Generation Approaches" width="400" height="200"></p>

## [PaLM-E: An Embodied Multimodal Language Model][palmePaper]


[humanLevelPromptEngineeringPaper]: https://arxiv.org/abs/2211.01910
[humanLevelPromptEngineeringSum]: /papers/weekly-review/nlp/2023-03-12-week-10/#large-language-models-are-human-level-prompt-engineers
[palmePaper]: https://palm-e.github.io/assets/palm-e.pdf
[palmeSum]: /papers/weekly-review/nlp/2023-03-12-week-10/#palm-e-an-embodied-multimodal-language-model
