---
layout: list
title:  NLP
slug:   nlp
description: >
  Here, you can find a list of all NLP-related posts.
---

 <!--  -->

## List of Recent NLP Papers

In what follows, I provide a list of novel and interesting NLP papers, which are recently published.


* <span style="color:#842C2A;font-size: 15px">__08 Jun 2023__</span> &nbsp;&nbsp;&nbsp;  Controlled Text Generation with Natural Language Instructions &nbsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][controlled2023zhaoPaper]
* <span style="color:#842C2A;font-size: 15px">__31 May 2023__</span> &nbsp;&nbsp;&nbsp;  Pathia: A Suite for Analyzing Large Language Models Across Training and Scaling
 &nbsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][pathia2023bidermanPaper]
* <span style="color:#842C2A;font-size: 15px">__29 May 2023__</span> &nbsp;&nbsp;&nbsp;  Can GPT-4 Perform Neural Architecture Search? &nbsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][can2023zhengPaper]
* <span style="color:#842C2A;font-size: 15px">__29 May 2023__</span> &nbsp;&nbsp;&nbsp; Black Box Adversarial Prompting for Foundation Models &nbsp; [<span style="color:#842C2A;font-size: 12px">__Summary__</span>][blackboxSum] &thinsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][blackboxPaper] 
* <span style="color:#842C2A;font-size: 15px">__20 May 2023__</span> &nbsp;&nbsp;&nbsp;  UP5: Unbiased Foundation Model for Fairness-aware Recommendation &nbsp; [<span style="color:#842C2A;font-size: 12px">__Summary__</span>][up5Sum] &thinsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][up5Paper]
* <span style="color:#842C2A;font-size: 15px">__16 May 2023__</span> &nbsp;&nbsp;&nbsp;  Towards Expert-Level Medical Question Answering with Large Language Models &nbsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][towards2023singhalPaper]
* <span style="color:#842C2A;font-size: 15px">__16 May 2023__</span> &nbsp;&nbsp;&nbsp;  StructGPT: A General Framework for LLM to Reason over Structured Data &nbsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][structgpt2023jiangPaper]
* <span style="color:#842C2A;font-size: 15px">__09 May 2023__</span> &nbsp;&nbsp;&nbsp;  StarCoder: May the Source Be With You! &nbsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][startcoder2023liPaper]
* <span style="color:#842C2A;font-size: 15px">__03 May 2023__</span> &nbsp;&nbsp;&nbsp; Distilling Step-by-Step &nbsp; [<span style="color:#842C2A;font-size: 12px">__Summary__</span>][distilling2023HsiehSum] &thinsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][distilling2023HsiehPaper] 
* <span style="color:#842C2A;font-size: 15px">__02 May 2023__</span> &nbsp;&nbsp;&nbsp;  Unlimiformer: Long-Range Transformers with Unlimited Length Input &nbsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][unlimiformer2023bertschPaper]
* <span style="color:#842C2A;font-size: 15px">__01 May 2023__</span> &nbsp;&nbsp;&nbsp;  Learning to Reason and Memorize with Self-Notes &nbsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][learning2023lanchantinPaper]
* <span style="color:#842C2A;font-size: 15px">__27 Apr 2023__</span> &nbsp;&nbsp;&nbsp; We’re Afraid Language Models Aren’t Modeling Ambiguity &nbsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][we2023liuPaper]
* <span style="color:#842C2A;font-size: 15px">__26 Apr 2023__</span> &nbsp;&nbsp;&nbsp; Exploring the Curious Case of Code Prompts &nbsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][exploring2023zhangPaper]
* <span style="color:#842C2A;font-size: 15px">__25 Apr 2023__</span> &nbsp;&nbsp;&nbsp; AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head &nbsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][audiogpt2023huangPaper]
* <span style="color:#842C2A;font-size: 15px">__20 Mar 2023__</span> &nbsp;&nbsp;&nbsp; Context-faithful Prompting for Large Language Models &nbsp; [<span style="color:#842C2A;font-size: 12px">__Summary__</span>][contextFaithful2023zhouSum] &thinsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][contextFaithful2023zhouPaper] 
* <span style="color:#842C2A;font-size: 15px">__17 Mar 2023__</span> &nbsp;&nbsp;&nbsp; COLT5: Faster Long-Range Transformers with Conditional Computation &nbsp; [<span style="color:#842C2A;font-size: 12px">__Summary__</span>][colt52023ainslieSum] &thinsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][colt52023ainsliePaper]
* <span style="color:#842C2A;font-size: 15px">__15 Mar 2023__</span> &nbsp;&nbsp;&nbsp; Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems &nbsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][pretrain2023liuPaper] 
* <span style="color:#842C2A;font-size: 15px">__10 Mar 2023__</span> &nbsp;&nbsp;&nbsp; Large Language Models Are Human-Level Prompt Engineers &nbsp; [<span style="color:#842C2A;font-size: 12px">__Summary__</span>][large2023zhauSum] &thinsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][large2023zhauPaper]
* <span style="color:#842C2A;font-size: 15px">__06 Mar 2023__</span> &nbsp;&nbsp;&nbsp; PaLM-E: An Embodied Multimodal Language Model &nbsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][palme2023driessPaper] 


* <span style="color:#842C2A;font-size: 15px">__06 Mar 2023__</span> &nbsp;&nbsp;&nbsp; PaLM-E: An Embodied Multimodal Language Model &nbsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][palme2023driessPaper] 
* <span style="color:#842C2A;font-size: 15px">__04 Mar 2023__</span> &nbsp;&nbsp;&nbsp; MathPrompter: Mathematical Reasoning using Large Language Models &nbsp; [<span style="color:#842C2A;font-size: 12px">__Summary__</span>][mathprompter2023imaniSum] &thinsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][mathprompter2023imaniPaper] 
* <span style="color:#842C2A;font-size: 15px">__27 Feb 2023__</span> &nbsp;&nbsp;&nbsp; LLaMA: Open and Efficient Foundation Language Models &nbsp; [<span style="color:#842C2A;font-size: 12px">__Summary__</span>][llama2023touvronSum] &thinsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][llama2023touvronPaper] 
* <span style="color:#842C2A;font-size: 15px">__18 Feb 2023__</span> &nbsp;&nbsp;&nbsp; The Capacity for Moral Self-Correction in Large Language Models &nbsp; [<span style="color:#842C2A;font-size: 12px">__Summary__</span>][capacity2023ganguliSum] &thinsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][capacity2023ganguliPaper] 
* <span style="color:#842C2A;font-size: 15px">__13 Feb 2023__</span> &nbsp;&nbsp;&nbsp; Guiding Pretraining in Reinforcement Learning with Large Language Models &nbsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][guiding2023duPaper] 
* <span style="color:#842C2A;font-size: 15px">__09 Feb 2023__</span> &nbsp;&nbsp;&nbsp; Toolformer: Language Models Can Teach Themselves to Use Tools &nbsp; [<span style="color:#842C2A;font-size: 12px">__Summary__</span>][toolformer2023schickSum] &thinsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][toolformer2023schickPaper] 
* <span style="color:#842C2A;font-size: 15px">__09 Feb 2023__</span> &nbsp;&nbsp;&nbsp; Offsite-Tuning: Transfer Learning without Full Model &nbsp; [<span style="color:#842C2A;font-size: 12px">__Summary__</span>][offsite2023xiaoSum] &thinsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][offsite2023xiaoPaper] 
* <span style="color:#842C2A;font-size: 15px">__06 Dec 2022__</span> &nbsp;&nbsp;&nbsp; Few-Shot Preference Learning for Human-in-the-Loop RL &nbsp; [<span style="color:#842C2A;font-size: 12px">__Paper__</span>][few2023hejnaPaper]



<!-- ## List of Papers by Topic -->




[controlled2023zhaoPaper]: https://arxiv.org/pdf/2304.14293.pdf

[pathia2023bidermanPaper]: https://arxiv.org/pdf/2304.01373.pdf

[can2023zhengPaper]: https://arxiv.org/pdf/2304.10970.pdf

[up5Paper]: https://arxiv.org/abs/2305.12090
[up5Sum]: /papers/weekly-review/nlp/2023-05-21-week-20/#up5-unbiased-foundation-model-for-fairness-aware-recommendation

[blackboxPaper]: https://arxiv.org/pdf/2302.04237.pdf
[blackboxSum]: /papers/weekly-review/nlp/2023-06-04-week-22/#black-box-adversarial-promoting-for-foundation-models

[towards2023singhalPaper]: https://arxiv.org/pdf/2305.09617.pdf
[structgpt2023jiangPaper]: https://arxiv.org/pdf/2305.09645.pdf

[startcoder2023liPaper]: https://arxiv.org/pdf/2305.06161.pdf

[distilling2023HsiehPaper]: https://arxiv.org/pdf/2305.02301v1.pdf
[distilling2023HsiehSum]: /papers/weekly-review/nlp/2023-05-07-week-18/#distilling-step-by-step

[unlimiformer2023bertschPaper]: https://arxiv.org/pdf/2305.01625v1.pdf
[learning2023lanchantinPaper]: https://arxiv.org/pdf/2305.00833.pdf

[exploring2023zhangPaper]: https://arxiv.org/pdf/2304.13250.pdf
[we2023liuPaper]: https://arxiv.org/pdf/2304.14399.pdf

[audiogpt2023huangPaper]: https://arxiv.org/pdf/2304.12995.pdf
[contextFaithful2023zhouPaper]: https://arxiv.org/pdf/2303.11315.pdf
[contextFaithful2023zhouSum]: /papers/weekly-review/nlp/2023-03-26-week-12/#context-faithful-prompting-for-large-language-models

[memorizing2023wuPaper]: https://arxiv.org/abs/2203.08913
[memorizing2023wuSum]: /papers/weekly-review/nlp/2023-03-19-week-11/#memorizing-transformers

[colt52023ainsliePaper]: https://arxiv.org/pdf/2303.09752v1.pdf
[colt52023ainslieSum]: /papers/weekly-review/nlp/2023-03-19-week-11/#colt5-faster-long-range-transformers-with-conditional-computation

[pretrain2023liuPaper]: https://arxiv.org/pdf/2302.03735.pdf

[large2023zhauPaper]: https://arxiv.org/abs/2211.01910
[large2023zhauSum]: /papers/weekly-review/nlp/2023-03-12-week-10/#large-language-models-are-human-level-prompt-engineers

[palme2023driessPaper]: https://palm-e.github.io/assets/palm-e.pdf

[mathprompter2023imaniSum]: /papers/weekly-review/nlp/2023-03-05-week-9/#mathprompter
[mathprompter2023imaniPaper]: https://arxiv.org/pdf/2303.05398.pdf

[llama2023touvronSum]: /papers/weekly-review/nlp/2023-03-05-week-9/#llama-open-and-efficient-foundation-language-models
[llama2023touvronPaper]: https://arxiv.org/pdf/2302.13971.pdf

[toolformer2023schickPaper]: https://arxiv.org/pdf/2302.04761.pdf
[toolformer2023schickSum]: /papers/weekly-review/nlp/2023-02-12-week-6/#toolformer

[offsite2023xiaoPaper]: https://arxiv.org/pdf/2302.04870.pdf
[offsite2023xiaoSum]: /papers/weekly-review/nlp/2023-02-12-week-6/#offsite-tuning

[capacity2023ganguliPaper]: https://arxiv.org/pdf/2302.07459.pdf
[capacity2023ganguliSum]: /papers/weekly-review/nlp/2023-02-19-week-7/#the-capacity-for-moral-self-correction-in-large-language-models

[guiding2023duPaper]: https://arxiv.org/pdf/2302.06692.pdf

[few2023hejnaPaper]: https://arxiv.org/abs/2212.03363