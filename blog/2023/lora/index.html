<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LoRA - Low-Rank Adaptation of LLMs | Shirin Tahmasebi </title> <meta name="author" content="Shirin Tahmasebi"> <meta name="description" content="LoRA Paper Review "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?ce48ee9bc248ee6b18f3aeefc03ed2f9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shirintahmasebi.github.io/blog/2023/lora/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Shirin Tahmasebi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">LoRA - Low-Rank Adaptation of LLMs</h1> <p class="post-meta"> September 25, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/papers"> <i class="fa-solid fa-tag fa-sm"></i> papers</a>   <a href="/blog/category/nlp"> <i class="fa-solid fa-tag fa-sm"></i> nlp</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h2"><a href="#background-matrix-rank">Background: Matrix Rank</a></li> <li class="toc-entry toc-h2"><a href="#low-rank-weight-matrices-in-llms">Low-Rank Weight Matrices in LLMs</a></li> <li class="toc-entry toc-h2"><a href="#research-gap">Research Gap</a></li> <li class="toc-entry toc-h2"> <a href="#solution-lora---a-two-phase-approach">Solution: LoRA - A Two-Phase Approach</a> <ul> <li class="toc-entry toc-h3"><a href="#matrix-updates">Matrix Updates</a></li> <li class="toc-entry toc-h3"><a href="#inference">Inference</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#conclusion">Conclusion</a></li> </ul> </div> <hr> <div id="markdown-content"> <p>In what follows, I am providing a brief summary of the interesting paper named <a href="https://arxiv.org/pdf/2106.09685.pdf" rel="external nofollow noopener" target="_blank">LoRA</a>.</p> <h2 id="introduction">Introduction</h2> <p>The focus of this paper is a groundbreaking approach called Low-Rank Adaptation (LoRA) in the context of large language models (LLMs). In the world of artificial intelligence and natural language processing, LLMs like GPT-3 have gained immense attention for their language generation and understanding capabilities. However, these models are notably large and resource-intensive, posing challenges for efficient fine-tuning and adaptation to specific tasks. LoRA presents an innovative solution to this challenge. At its core, LoRA revolves around the idea that the weight matrices within LLMs often exhibit low-rank properties, indicating significant redundancy in their parameters. By tapping into this redundancy, LoRA introduces a way to drastically reduce the number of parameters involved in fine-tuning, making the process more computationally efficient. In this paper, we delve into the intricacies of LoRA, exploring its principles, benefits, and implications for the world of deep learning.</p> <h2 id="background-matrix-rank">Background: Matrix Rank</h2> <p>The rank of a matrix signifies the maximum number of linearly independent rows or columns it possesses. In simpler terms, it tells us how many rows or columns in the matrix cannot be expressed as combinations of the others. A matrix with full rank means that all its rows and columns are independent, while a lower rank indicates linear dependencies among rows or columns. When the rank is much smaller than the total number of rows and columns, it implies high correlation or redundancy, allowing us to approximate the matrix using a smaller, lower-rank one while preserving its core characteristics.</p> <h2 id="low-rank-weight-matrices-in-llms">Low-Rank Weight Matrices in LLMs</h2> <p>Large language models like GPT-3 often feature weight matrices with low-rank properties. This means that many of the parameters in these matrices are redundant or highly correlated. Surprisingly, we can represent these weight matrices with fewer parameters while maintaining their essential capabilities. Various methods can make these large matrices smaller, including matrix factorization, weight pruning, quantization, and low-rank updates. In the paper, the primary approach employed to reduce large weight matrices is Low-Rank Update, a technique where low-rank updates are learned and applied to pre-trained model weights to adapt them to downstream tasks.</p> <h2 id="research-gap">Research Gap</h2> <p>Existing solutions for adapting pre-trained language models to new tasks can introduce increased inference latency. This happens for several reasons:</p> <ul> <li>Adapter Layers: Some methods introduce adapter layers between the pre-trained model’s layers. These layers add extra computations during inference, causing a cumulative effect when multiple adapters are used.</li> <li>Specialized Architectures: Certain approaches propose specialized model structures or modifications that require additional processing during inference.</li> <li>Parameter Explosion: Fine-tuning the entire pre-trained model can substantially increase the number of parameters, leading to higher computation and memory requirements.</li> <li>Complex Optimization Techniques: Some methods involve complex optimization techniques during fine-tuning, adding computational overhead during inference.</li> </ul> <p>In contrast, Low-Rank Adaptation (LoRA), as presented in the paper, focuses on minimizing additional computations during inference while effectively adapting pre-trained models to new tasks. By reducing the rank of weight matrices and performing low-rank updates, LoRA achieves this with fewer parameters and minimal impact on inference speed.</p> <h2 id="solution-lora---a-two-phase-approach">Solution: LoRA - A Two-Phase Approach</h2> <p>LoRA consists of two distinct phases: <strong>pre-training</strong> and <strong>fine-tuning</strong>. During pre-training, LoRA operates like a regular language model, using only the pre-trained weight matrix (\(W_0\)) for the forward pass and backpropagation. No additional matrices like \(A\) and \(B\) are involved at this stage.</p> <p>In the fine-tuning phase for downstream tasks, LoRA introduces the low-rank update \(\triangle W = AB\), where \(B\) and \(A\) are smaller matrices with trainable parameters specific to the task. During the forward pass, it computes \(h = (W_0 + \triangle W)x\), where W0 remains frozen, and \(\triangle W\) is task-specific and updatable. During backpropagation, gradients for \(A\) and \(B\) are computed separately and updated. This approach reduces gradient computation complexity and minimizes the impact on inference speed.</p> <p>However, it’s important to note that during fine-tuning, LoRA needs to store additional parameters for \(A\) and \(B\), resulting in some extra memory overhead compared to regular models that only store pre-trained weights. This trade-off allows LoRA to efficiently adapt to new tasks while preserving the knowledge gained during pre-training.</p> <h3 id="matrix-updates">Matrix Updates</h3> <p>In a low-rank update scenario, if the size of the original weight matrix \(W\) is \(d \times k\), and the input vector has size \(k\), the matrices \(A\) and \(B\) typically have dimensions:</p> <ul> <li>\(A: r \times k\), where \(r\) is the rank of matrix \(W\).</li> <li>\(B: d \times r\). This arrangement maintains compatibility with the original weight matrix while introducing a low-rank structure to the update \(\triangle W\).</li> </ul> <h3 id="inference">Inference</h3> <p>During inference for downstream tasks, the weight matrix used is \(W_0 + AB\), where \(W_0\) represents the weight matrix from the pre-training phase. This combination leverages the knowledge gained during pre-training and adapts it to the specific downstream task.</p> <h2 id="conclusion">Conclusion</h2> <p>In conclusion, Low-Rank Adaptation (LoRA) emerges as a game-changing technique for the world of deep learning, particularly in the realm of large language models. This paper has unpacked the essence of LoRA, shedding light on how it leverages the inherent low-rank structure of weight matrices to dramatically reduce the computational and memory requirements during fine-tuning. Unlike some existing approaches, LoRA strives to minimize inference latency while maintaining the efficacy of fine-tuning. As we navigate the ever-evolving landscape of artificial intelligence, LoRA’s contribution stands out as an elegant and efficient way to adapt pre-trained models to a plethora of real-world applications. This paper’s exploration of LoRA not only provides valuable insights into the future of model adaptation but also sparks discussions on the ways we can continue to make AI more accessible, resource-friendly, and capable.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <p class="mb-2">Here are some more articles relevant to this one:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-25/">Paper Review - Week 25</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-27/">Paper Review - Week 27</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-36/">Paper Review - Week 36</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/week-10/">Paper Review - Week 10</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/selective-fairness-in-recommenders/">Selective Fairness in Recommendation via Prompts</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Shirin Tahmasebi. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>