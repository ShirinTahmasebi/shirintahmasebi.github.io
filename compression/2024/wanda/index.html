<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Wanda | Shirin Tahmasebi </title> <meta name="author" content="Shirin Tahmasebi"> <meta name="description" content="Pruning by Weights and Activations "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?ce48ee9bc248ee6b18f3aeefc03ed2f9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shirintahmasebi.github.io/compression/2024/wanda/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Shirin Tahmasebi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">CV </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">Publications</a> <a class="dropdown-item " href="/background/">Background</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Notes </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/bias/">Bias</a> <a class="dropdown-item " href="/tokenization/">Tokenization</a> <a class="dropdown-item " href="/llm_judge/">LLM as Evaluator</a> <a class="dropdown-item " href="/prompt/">Prompt Engineering</a> <a class="dropdown-item " href="/compression/">LLM Compression</a> <a class="dropdown-item " href="/agents/">LLM Agents</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/recsys/">Recommendation Systems</a> <a class="dropdown-item " href="/other/">Others</a> </div> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Wanda</h1> <p class="post-meta"> May 06, 2024 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2024   ·   <i class="fa-solid fa-hashtag fa-sm"></i> compression   <i class="fa-solid fa-hashtag fa-sm"></i> pruning   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#a-simple-and-effective-prunig-approach-for-llms">A Simple and Effective Prunig Approach for LLMs</a> <ul> <li class="toc-entry toc-h3"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h3"><a href="#research-gap">Research Gap</a></li> <li class="toc-entry toc-h3"> <a href="#solution-wanda">Solution: Wanda</a> <ul> <li class="toc-entry toc-h4"><a href="#step-1-compute-input-activation-norms-using-calibration-data">Step 1: Compute Input Activation Norms Using Calibration Data</a></li> <li class="toc-entry toc-h4"><a href="#step-2-compute-weight-importance">Step 2: Compute Weight Importance</a></li> <li class="toc-entry toc-h4"><a href="#step-3-prune-the-lowest-scoring-weights">Step 3: Prune the Lowest-Scoring Weights</a></li> <li class="toc-entry toc-h4"><a href="#step-4-use-the-pruned-model-directly">Step 4: Use the Pruned Model Directly</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#conclusion">Conclusion</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="a-simple-and-effective-prunig-approach-for-llms"><a href="https://arxiv.org/pdf/2306.11695" rel="external nofollow noopener" target="_blank">A Simple and Effective Prunig Approach for LLMs</a></h2> <h3 id="introduction">Introduction</h3> <p>Pruning is a critical technique for reducing the size and computational cost of Large Language Models (LLMs), but different pruning methods vary in how they balance efficiency and accuracy retention. Wanda (Pruning by Weights and Activations) is a simple, activation-aware pruning approach that improves upon traditional magnitude pruning while avoiding the computational burden of second-order pruning methods like SparseGPT. To understand its advantages, we can evaluate Wanda along four key aspects:</p> <ol> <li> <strong>Granularity</strong>: Wanda performs layer-wise unstructured pruning, meaning it removes individual weights rather than full neurons or heads. But, it also supports semi-structured pruning (e.g., 2:4, 4:8 sparsity, which is optimized for GPU acceleration).</li> <li> <strong>Pruning Criteria</strong>: Wanda improves traditional magnitude pruning by incorporating activations into the importance metric. This ensures that weights connected to highly active neurons are preserved, preventing the removal of small but functionally important weights.</li> <li> <strong>Pruning Ratio</strong>: Wanda can be applied to various sparsity levels, supporting unstructured sparsity for accuracy retention and N:M sparsity for hardware acceleration.</li> <li> <strong>Need for Retraining</strong>: Wanda is a one-shot pruning method, which applies pruning without retraining or iterative weight reconstruction, making it much faster and easier to implement. This also makes Wanda practical for large-scale LLM deployments where retraining is infeasible.</li> </ol> <h3 id="research-gap">Research Gap</h3> <p>So, why the traditional magnitude pruning fails for LLMs? Why do we need another solution?</p> <p>LLMs are computationally expensive, making model compression techniques (quantization, pruning) essential. Pruning is a widely used technique that removes unimportant weights, reducing model size while maintaining performance. However, most existing pruning methods either:</p> <ol> <li>Require <strong>retraining</strong>, which is impractical for billion-scale models.</li> <li>Use <strong>expensive second-order weight updates</strong>, making pruning computationally costly. (A second-order weight update refers to a pruning method that uses second-order (Hessian) information to determine which weights to remove. This approach takes into account not only the magnitude of the weights but also their impact on the loss function, making it more precise than simple magnitude-based pruning.)</li> </ol> <p>But the magnitude pruning in LLMs does not work. since it removes weights with the smallest absolute values but does not consider their role in the network. While effective for smaller models, it fails dramatically for LLMs due to the following reasons:</p> <ul> <li>Once LLMs reach a certain size (~6B+ parameters), some hidden state features have exceptionally large magnitudes. These <strong>outlier features</strong> are: <ul> <li>100× larger than typical activations.</li> <li>Sparse but highly influential for the model’s predictive performance.</li> <li>Removing them severely degrades LLM performance.</li> </ul> <p>Since magnitude pruning ignores activations, it may remove small weights that are actually <strong>connected to these important high-magnitude features</strong>, causing major performance loss.</p> </li> <li>Traditional deep networks (CNNs, small transformers) contain many redundant parameters. LLMs, however, are highly optimized, meaning their weights are <strong>more interdependent</strong>. Magnitude pruning disrupts critical pathways, making LLMs harder to prune effectively.</li> <li>The importance of a weight depends not only on its magnitude but also on the <strong>input activations</strong>. For example, a small-magnitude weight connected to a highly active neuron may be crucial, but magnitude pruning removes it anyway. LLMs exhibit high activation imbalance, making activation-aware pruning necessary.</li> <li>In OPT, LLaMA, and Pythia models, magnitude pruning fails catastrophically. Even at 10–20% sparsity, performance drops drastically. In some cases, pruned models completely fail (perplexity skyrockets).</li> </ul> <h3 id="solution-wanda">Solution: Wanda</h3> <p>Wanda (Pruning by <strong>W</strong>eights <strong>and</strong> <strong>A</strong>ctivations) is a simple and effective method for pruning pretrained LLMs without retraining or weight updates. Instead of relying only on weight magnitude, it also considers <strong>input activations</strong>, leading to better sparse sub-networks. Here is how it works:</p> <p style="text-align:center;"><img src="/assets/img/weekly-review/wanda_architecture.png" alt="Wanda" width="800" height="180"></p> <h4 id="step-1-compute-input-activation-norms-using-calibration-data">Step 1: Compute Input Activation Norms Using Calibration Data</h4> <ul> <li>Since activations are <strong>input-dependent</strong>, Wanda estimates them using a small calibration set (e.g., 128 sequences from C4).</li> <li>A single forward pass is run through the model to obtain input activations (\(X\)).</li> <li>Compute the <strong>L2 norm</strong> of activations for each feature (\(j\)) across all input tokens. (Wanda operates at the layer level, meaning that \(j\) corresponds to the neurons (features) within a specific layer, and \(x_j\) represents the activations of those neurons across all tokens in the calibration dataset.) \(\|X_j\|_2 = \sqrt{\sum_{\text{tokens } i} X_{ij}^2}\)</li> <li>This norm represents <strong>the average activation strength</strong> for each input feature (neurons in each layer considering the input of that layer).</li> </ul> <h4 id="step-2-compute-weight-importance">Step 2: Compute Weight Importance</h4> <p>Instead of pruning based on <strong>weight magnitude</strong>, Wanda computes an <strong>importance score</strong> for each weight:</p> \[S_{ij} = |W_{ij}| \cdot \|X_j\|_2\] <p>where:</p> <ul> <li>\(\vert W_{ij} \vert\) is the absolute weight value.</li> <li>\(\|X_j\|_2\) is the L2 norm of the input activation (per layer).</li> </ul> <p>The key idea here is that if a weight \(W_{ij}\) is <strong>small</strong> but is connected to a <strong>high-activation feature</strong> (\(X_j\)), it gets <strong>higher importance</strong>. This prevents the model from pruning weights connected to critical neurons.</p> <h4 id="step-3-prune-the-lowest-scoring-weights">Step 3: Prune the Lowest-Scoring Weights</h4> <p>Instead of pruning across the entire layer, Wanda <strong>prunes per output neuron</strong>. For each output neuron (\(i\)), Wanda:</p> <ul> <li>Sorts its weights by importance score (\(S_{ij}\)).</li> <li>Prunes the lowest-scoring weights based on the target sparsity level (\(s\%\)).</li> </ul> <p>This preserves structural balance, preventing the collapse of important neurons.</p> <h4 id="step-4-use-the-pruned-model-directly">Step 4: Use the Pruned Model Directly</h4> <p>Unlike SparseGPT, Wanda does not require weight updates or retraining. The pruned model is ready to use immediately.</p> <hr> <p>Why Wanda works better than magnitude pruning:</p> <ul> <li> <strong>Considers Activations:</strong> Preserves weights linked to <strong>important features</strong> instead of just using magnitude.</li> <li> <strong>More Balanced Pruning:</strong> Prunes <strong>per output neuron</strong>, preventing <strong>over-pruning of critical layers</strong>.</li> <li> <strong>No Retraining or Weight Updates:</strong> Unlike SparseGPT, Wanda <strong>directly prunes weights</strong> with minimal computation.</li> <li> <strong>Fast and Scalable:</strong> Works with a <strong>single forward pass</strong> and is <strong>300× faster</strong> than SparseGPT in pruning.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>Wanda provides an efficient and scalable pruning approach for LLMs by introducing an activation-aware importance metric that significantly improves upon traditional magnitude pruning. By computing weight importance as the product of weight magnitude and activation norms, Wanda ensures that critical weights connected to active neurons are preserved, mitigating the primary failure points of existing pruning techniques. The layer-wise pruning process further enhances stability by ensuring that pruning decisions are applied locally within each layer, preventing excessive degradation of specific components of the network. Additionally, Wanda’s ability to be used directly after pruning, without the need for retraining or weight updates, makes it an attractive alternative to second-order pruning methods like SparseGPT. Its flexibility across different pruning paradigms—including unstructured, structured, and ratio-based pruning—demonstrates its adaptability to real-world deployment constraints. With its fast, one-shot pruning mechanism and competitive performance, Wanda represents an important step toward practical and scalable sparsity in LLMs, opening up new possibilities for efficient inference and deployment in resource-constrained environments.</p> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Shirin Tahmasebi. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>