---
layout: post
title: Paper Review - Week 6
image: 
  path: /assets/img/weekly-review/nlp_cover_week_06.jpg
categories: [papers, weekly-review, nlp]
description: >
    Top NLP Papers Published from February 6 to February 12
sitemap: false
hide_last_modified: true
---

Several interesting papers are published in this week in NLP. Here is a list of them:

* [Toolformer: Language Models Can Teach Themselves to Use Tools][ToolformerLink]
* 

## Toolformer

This paper is really interesting, because it shows that language models, such as GPT-3, can be trained to perform arithmetic operations, such as addition, subtraction, multiplication, and division, with a high level of accuracy. While probabilistic methods are often used to generate answers to questions, some questions have a definite answer that should be calculated rather than generated randomly. In such cases, the use of language models that can generate exact answers can be beneficial. 

The idea in [Toolformer][ToolformerLink] is to leverage some external tools (e.g., a calculator, Q&A systems, search engines, translators, a calendar) to enhance the model's ability for doing some basic tasks, such as arithmetic operations and factual lookups. For training a language model for this purpose, a self-supervised approach is followed. First, given a corpus, it is necessary to create the train dataset for training the model. For doing so, each sentence of the corpus is modified and augmented by injecting the API calls. Among all the available positions of a sentence, the API calls are added where the probabiltiy of having the API calls is the most, based on the predictions of a langugae model such as GPT-J. Then, the response of the API call is added right after that. But, how to make sure that this sentence is good enough to be added to the train dataset? To ensure that the sentence is good enough, they calculate two loss functions for the augmented sentence. But, what are the two loss functions? 

1. The augmented sentence is first passed to GPT-J to generate the output. Then, the cross-entropy of the generated sentence (by GPT-J) and the original sentence is calcuated. This cross-entropy shows how much the generated sentence is close to its original version. The more similar the better. We use $$loss_1$$ to refer to this loss value.

1. The original sentence (without any API calls) and the augmented sentence without the response of the API calls are passed to the GPT-J model. The cross-entropy of the both generated sentences againt the original sentence is calculated. Then, the minumum of the two calculated cross-entropy is considered as the seond loss value.  We use $$loss_2$$ to refer to this loss value.

After calculating $$loss_1$$ and $$loss_2$$, if ($$loss_2$$ - $$loss_1$$) is more than a specific threshold, then it means that adding the API call can help the model to generate a sentence similar to the original one. So, in this case, the augmented sentence is added to the train dataset. 



[ToolformerLink]: https://arxiv.org/pdf/2302.04761.pdf