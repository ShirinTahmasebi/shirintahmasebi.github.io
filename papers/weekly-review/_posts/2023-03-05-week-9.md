---
layout: post
title: Paper Review - Week 9
image: 
  path: /assets/img/weekly-review/nlp_cover_week_09.jpg
categories: [papers, weekly-review, nlp]
description: >
    Top NLP Papers Published from February 27 to March 05
sitemap: false
hide_last_modified: true
---

To me, the most interesting papers among those published in this week are:
* [MathPrompter: Mathematical Reasoning Using Large Langugage Models][mathPrompterSum]
* [LLaMA: Open and Efficient Foundation Language Models][LLaMaSum]


## [MathPrompter][mathPrompterPaper]

This paper focuses on using LLMs for mathematical reasoning. Some of the challenges in this work are as follows:

* Mathematical operations usually have a single correct answer.
* Not only the final output is important, the procedure and the intermidate steps that resulted into that final output should also be correct.
* It is difficult to ensure that the generated output and the intermediate steps are all correct. In other words, it is better to have a metric to show that how much the model is confident about the generated output!

Accordingly, MathPrompter tries to propose a solution to increase the validity and reliability of mathematical reasoning using LLMs. Their solution is to simulate the usual approach that studnets usually take for solving a math problem. So, the steps in MathPrompter is as follows:

1. Receiving the definition of a mathematical problem as text. For example:
````
At a restaurant, each adult meal costs $5 and kids eat free. If a group of 15 people came in and 8 were kids, how much would it cost for the group to eat? 
````
1. Generating a template for the input question and creating a list of values for solving the template:
````
Template = At a restaurant, each adult meal costs A and kids eat free. if a group of B people came in and C were kids, how much would it cost for the group to eat?
Mapping = {A:5, B:15, C:8}
````
1. Creaing proper prompts for creating different solutions. For example, one solution can be a python function and the other can be a algebraic equation. So, by providing these prompts to the LLM:
````
Algebraic prompt: Write a mathematical equation and generate the answer format starting with ‘Answer =’
Python prompt: Write a Python function that returns the answer.
````
then, the LLM is supposed to generated the two following outputs:
````python
# Algebraic expression output
Answer = A * (B - C)
# Python expression output
def total_price(A, B, C):
  return A * (B - C)
````
These outputs can help the user to have an insight about how the LLM is reasoning.
1. Then, MathPrompter uses different randomized values for $$A$$, $$B$$, and $$C$$. Then, it passes all the randomized values to the both outpus. If, for more than 5 times, the result of the two outputs (algebraic and python expression) are the same, then both of them are accepted as the correct solutions for the mathematical problem.

This is definitely a very interesting and novel idea. However, I personally have some concerns about it:
1. First of all, MathPrompter is not making it easier for LLMs to find a solution. The only thing that it does is to ask LLMs to generate multiple outputs, such as algebraic and python expression. 
1. MathPrompter uses several randomized values (at least 5 values) to validate if the LLM outputs result in the same number or not. However, it is not clear that how these randomized values are generated. If users are supposed to provide these values, then I think it somehow makes the LLM useless here! Because users need to solve the problem themselves to be able to provide such values. So, in this case, they don't need LLMs anymore! :)
1. If for the specific values in the input mathematical problem, algebraic and python expressions result in different values, then what can be reported to the user?


## [LLaMA: Open and Efficient Foundation Language Models][LLaMaPaper]

Recently, the trend of novel language models is that they are becomming larger and larger. The main idea behind this trend is that, probabely, by increasing the number of parameters of a language model, it can have a better performance. In other words, the prevalent idea is that, the larger the model, the better the performance.

However, a paper, published in 2022, tried to challange this idea. They hypothesised that it is not only the size of the language models which make their quality better. There is another very important aspect: **the size of the training data**. This hypothesis, which is also known as the __Hoffmann's Scaling Law__, has been the foundation of one of the recenly-proposed language models, named __Chinchilla__. Chinchilla has only 70B parameter (compared to GPT-3 with about 175B parameter) and can still get comparable performance to GPT-3.  

LLaMa is another novel language model which uses the Hoffmann's scaling law as its foundation. It has several variants, each of which with different number of parameter--ranging from 7B to 65B parameter. Although the size of the model is significantly smaller than GPT-3, it has comparable results to GPT-3 for most downstream task; and even for some of the downstream tasks, it can outperform GPT-3. 

Now, to investigate the specific charactristics of LLaMa, we look at its three aspects:

1. Training Datasets
1. Architecture
1. Performance

### Training Datasets

As mentioned previously, LLaMa is supposed to have between 7B and 65B parameters. To this end, according to the Hoffmann's scaling law, the training dataset consist of 14 Trillion tokens. To collect this 14 Trillion tokens, the following datasets are used:
* English CommonCrawl [67%]: Public crawled web pages from 2017 to 2020
* C4  [15%]: Preprocessed CommonCrawl!
* Github [4.5%]
* Wikipedia [4.5%]: Covering 20 languages from June 2022 to August 2022
* Gutenberg and Books3 [4.5%]: Two book corpora from public domain
* ArXiv [2.5%]
* Stack Exchange [2%]: From 28 topics


### Architecture

LLaMa is a transformer-based model. However, instead of using the main transformer building block as it is, they apply several minor changes to it. These changes are mainly for reducing the training time. The changes are as follows:
* Pre-normalization instead of post-normalization
* Activation function from ReLU to SwiGLU
* Positional encoding from absolute to Rotary

The following image represents how the architecture looks like after applying the above changes. The image on the left shows the usual transformer building block and the one on the right illustrates the building block after applying the changes.

<p style="text-align:center;"><img src="/assets/img/weekly-review/llama_architecture_building_block.png" alt="Candidate Generation Approaches" width="800" height="400"></p>

### Performance

They have evaluated LLaMa on different tasks and compared it with the two of the recent extremely large language models. The results are as the following:

* Common Sense Reading
  * Evaluated in zero-shot setting.
  * **LLaMa (65B)** _outperforms_ **Chinchilla (70B)**.
  * **LLaMa (65B)** _outperforms_ **PaLM (540B)** and **GPT-3 (175B)** on most datasets.

* Closed-book Question Answering
  * Evaluated in zero-shot and few-shot settings.
  * **LLaMa (13B)** is _competitive_ with **GPT-3 (175B)** and **Chinchilla (70B)**.
  * **LLaMa (65B)** _outperforms_ **PaLM (540B)**, **GPT-3 (175B)**, and **Chinchilla (70B)**.

* Reading Comprehension
  * Evaluated in zero-shot setting.
  * **LLaMa (65B)** is _competitive_ with **PaLM (540B)**.
  * **LLaMa (13B)** _outperforms_ **GPT-3 (175B)**.

* Mathematical Reasoning
  * **LLaMa (65B)** _outperforms_ **PaLM (540B)**.


[mathPrompterPaper]: https://arxiv.org/pdf/2303.05398.pdf
[mathPrompterSum]: /papers/weekly-review/nlp/2023-03-05-week-9/#mathprompter
[LLaMaSum]: /papers/weekly-review/nlp/2023-03-05-week-9/#llama-open-and-efficient-foundation-language-models
[LLaMaPaper]: https://arxiv.org/pdf/2302.13971.pdf